{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1792,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0011160714285714285,
      "grad_norm": 8.714164174460775,
      "learning_rate": 2e-05,
      "loss": 3.2969,
      "step": 1
    },
    {
      "epoch": 0.002232142857142857,
      "grad_norm": 8.128486322384092,
      "learning_rate": 2e-05,
      "loss": 3.3594,
      "step": 2
    },
    {
      "epoch": 0.0033482142857142855,
      "grad_norm": 11.915371164647565,
      "learning_rate": 2e-05,
      "loss": 3.1719,
      "step": 3
    },
    {
      "epoch": 0.004464285714285714,
      "grad_norm": 11.408181156121259,
      "learning_rate": 2e-05,
      "loss": 3.1719,
      "step": 4
    },
    {
      "epoch": 0.005580357142857143,
      "grad_norm": 7.410388430792192,
      "learning_rate": 2e-05,
      "loss": 3.2656,
      "step": 5
    },
    {
      "epoch": 0.006696428571428571,
      "grad_norm": 10.210658826761904,
      "learning_rate": 2e-05,
      "loss": 3.4844,
      "step": 6
    },
    {
      "epoch": 0.0078125,
      "grad_norm": 6.2256575973149095,
      "learning_rate": 2e-05,
      "loss": 3.5312,
      "step": 7
    },
    {
      "epoch": 0.008928571428571428,
      "grad_norm": 8.226034935319733,
      "learning_rate": 2e-05,
      "loss": 3.0781,
      "step": 8
    },
    {
      "epoch": 0.010044642857142858,
      "grad_norm": 8.017552136568476,
      "learning_rate": 2e-05,
      "loss": 3.4219,
      "step": 9
    },
    {
      "epoch": 0.011160714285714286,
      "grad_norm": 9.580740289235301,
      "learning_rate": 2e-05,
      "loss": 3.6406,
      "step": 10
    },
    {
      "epoch": 0.012276785714285714,
      "grad_norm": 6.580472320930182,
      "learning_rate": 2e-05,
      "loss": 3.7969,
      "step": 11
    },
    {
      "epoch": 0.013392857142857142,
      "grad_norm": 8.950205081738448,
      "learning_rate": 2e-05,
      "loss": 3.7188,
      "step": 12
    },
    {
      "epoch": 0.014508928571428572,
      "grad_norm": 9.249130772894745,
      "learning_rate": 2e-05,
      "loss": 2.9531,
      "step": 13
    },
    {
      "epoch": 0.015625,
      "grad_norm": 12.141656431544279,
      "learning_rate": 2e-05,
      "loss": 3.0781,
      "step": 14
    },
    {
      "epoch": 0.016741071428571428,
      "grad_norm": 33.93048922623467,
      "learning_rate": 2e-05,
      "loss": 3.3125,
      "step": 15
    },
    {
      "epoch": 0.017857142857142856,
      "grad_norm": 9.337238368722058,
      "learning_rate": 2e-05,
      "loss": 3.1562,
      "step": 16
    },
    {
      "epoch": 0.018973214285714284,
      "grad_norm": 11.029510825327677,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 17
    },
    {
      "epoch": 0.020089285714285716,
      "grad_norm": 11.70375218990609,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 18
    },
    {
      "epoch": 0.021205357142857144,
      "grad_norm": 7.214853201256005,
      "learning_rate": 2e-05,
      "loss": 2.9531,
      "step": 19
    },
    {
      "epoch": 0.022321428571428572,
      "grad_norm": 6.79025276963439,
      "learning_rate": 2e-05,
      "loss": 3.0312,
      "step": 20
    },
    {
      "epoch": 0.0234375,
      "grad_norm": 6.24740592895989,
      "learning_rate": 2e-05,
      "loss": 3.1094,
      "step": 21
    },
    {
      "epoch": 0.024553571428571428,
      "grad_norm": 10.375773543428478,
      "learning_rate": 2e-05,
      "loss": 3.3906,
      "step": 22
    },
    {
      "epoch": 0.025669642857142856,
      "grad_norm": 9.753379612530864,
      "learning_rate": 2e-05,
      "loss": 3.3125,
      "step": 23
    },
    {
      "epoch": 0.026785714285714284,
      "grad_norm": 13.487060172979637,
      "learning_rate": 2e-05,
      "loss": 3.125,
      "step": 24
    },
    {
      "epoch": 0.027901785714285716,
      "grad_norm": 10.482803374502716,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 25
    },
    {
      "epoch": 0.029017857142857144,
      "grad_norm": 12.323622600485368,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 26
    },
    {
      "epoch": 0.030133928571428572,
      "grad_norm": 7.732238438481504,
      "learning_rate": 2e-05,
      "loss": 3.6875,
      "step": 27
    },
    {
      "epoch": 0.03125,
      "grad_norm": 14.585271907120244,
      "learning_rate": 2e-05,
      "loss": 3.2656,
      "step": 28
    },
    {
      "epoch": 0.03236607142857143,
      "grad_norm": 10.060856015814013,
      "learning_rate": 2e-05,
      "loss": 3.0156,
      "step": 29
    },
    {
      "epoch": 0.033482142857142856,
      "grad_norm": 10.638996642140716,
      "learning_rate": 2e-05,
      "loss": 2.9062,
      "step": 30
    },
    {
      "epoch": 0.03459821428571429,
      "grad_norm": 5.682474307743042,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 31
    },
    {
      "epoch": 0.03571428571428571,
      "grad_norm": 8.462105780228269,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 32
    },
    {
      "epoch": 0.036830357142857144,
      "grad_norm": 12.9593568887898,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 33
    },
    {
      "epoch": 0.03794642857142857,
      "grad_norm": 9.766670242434621,
      "learning_rate": 2e-05,
      "loss": 3.4375,
      "step": 34
    },
    {
      "epoch": 0.0390625,
      "grad_norm": 8.68065181492996,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 35
    },
    {
      "epoch": 0.04017857142857143,
      "grad_norm": 6.641247307035858,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 36
    },
    {
      "epoch": 0.041294642857142856,
      "grad_norm": 6.861348755022539,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 37
    },
    {
      "epoch": 0.04241071428571429,
      "grad_norm": 5.900988240530282,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 38
    },
    {
      "epoch": 0.04352678571428571,
      "grad_norm": 9.62879180130691,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 39
    },
    {
      "epoch": 0.044642857142857144,
      "grad_norm": 10.069632475943388,
      "learning_rate": 2e-05,
      "loss": 3.0156,
      "step": 40
    },
    {
      "epoch": 0.04575892857142857,
      "grad_norm": 6.399836160670785,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 41
    },
    {
      "epoch": 0.046875,
      "grad_norm": 8.154120603779646,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 42
    },
    {
      "epoch": 0.04799107142857143,
      "grad_norm": 6.540864545099026,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 43
    },
    {
      "epoch": 0.049107142857142856,
      "grad_norm": 6.179297670838394,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 44
    },
    {
      "epoch": 0.05022321428571429,
      "grad_norm": 5.9164193745863045,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 45
    },
    {
      "epoch": 0.05133928571428571,
      "grad_norm": 5.213136646124924,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 46
    },
    {
      "epoch": 0.052455357142857144,
      "grad_norm": 7.581984144528733,
      "learning_rate": 2e-05,
      "loss": 2.8438,
      "step": 47
    },
    {
      "epoch": 0.05357142857142857,
      "grad_norm": 6.2324928560410315,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 48
    },
    {
      "epoch": 0.0546875,
      "grad_norm": 4.661068382685757,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 49
    },
    {
      "epoch": 0.05580357142857143,
      "grad_norm": 5.733918148085847,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 50
    },
    {
      "epoch": 0.056919642857142856,
      "grad_norm": 6.1210780790092345,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 51
    },
    {
      "epoch": 0.05803571428571429,
      "grad_norm": 6.129837412297985,
      "learning_rate": 2e-05,
      "loss": 2.9844,
      "step": 52
    },
    {
      "epoch": 0.05915178571428571,
      "grad_norm": 6.93503430350704,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 53
    },
    {
      "epoch": 0.060267857142857144,
      "grad_norm": 12.691960155294792,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 54
    },
    {
      "epoch": 0.06138392857142857,
      "grad_norm": 5.005629787320814,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 55
    },
    {
      "epoch": 0.0625,
      "grad_norm": 5.651187530245846,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 56
    },
    {
      "epoch": 0.06361607142857142,
      "grad_norm": 4.038033895067932,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 57
    },
    {
      "epoch": 0.06473214285714286,
      "grad_norm": 6.438456401260694,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 58
    },
    {
      "epoch": 0.06584821428571429,
      "grad_norm": 5.745731504927555,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 59
    },
    {
      "epoch": 0.06696428571428571,
      "grad_norm": 6.841252472633887,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 60
    },
    {
      "epoch": 0.06808035714285714,
      "grad_norm": 5.997203136648066,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 61
    },
    {
      "epoch": 0.06919642857142858,
      "grad_norm": 5.587411571079513,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 62
    },
    {
      "epoch": 0.0703125,
      "grad_norm": 6.187783477094426,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 63
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 5.848047770729405,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 64
    },
    {
      "epoch": 0.07254464285714286,
      "grad_norm": 4.364303256188071,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 65
    },
    {
      "epoch": 0.07366071428571429,
      "grad_norm": 5.894046035424198,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 66
    },
    {
      "epoch": 0.07477678571428571,
      "grad_norm": 4.188722537348376,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 67
    },
    {
      "epoch": 0.07589285714285714,
      "grad_norm": 5.338077657329879,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 68
    },
    {
      "epoch": 0.07700892857142858,
      "grad_norm": 6.432186512205561,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 69
    },
    {
      "epoch": 0.078125,
      "grad_norm": 5.355334501508085,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 70
    },
    {
      "epoch": 0.07924107142857142,
      "grad_norm": 6.336103241273201,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 71
    },
    {
      "epoch": 0.08035714285714286,
      "grad_norm": 5.459706147832072,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 72
    },
    {
      "epoch": 0.08147321428571429,
      "grad_norm": 4.1566987094408585,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 73
    },
    {
      "epoch": 0.08258928571428571,
      "grad_norm": 5.42232549814063,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 74
    },
    {
      "epoch": 0.08370535714285714,
      "grad_norm": 4.616704349550249,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 75
    },
    {
      "epoch": 0.08482142857142858,
      "grad_norm": 5.170182091793704,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 76
    },
    {
      "epoch": 0.0859375,
      "grad_norm": 7.5025892903524865,
      "learning_rate": 2e-05,
      "loss": 2.8281,
      "step": 77
    },
    {
      "epoch": 0.08705357142857142,
      "grad_norm": 5.8911760382637555,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 78
    },
    {
      "epoch": 0.08816964285714286,
      "grad_norm": 4.211172336958261,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 79
    },
    {
      "epoch": 0.08928571428571429,
      "grad_norm": 4.876182123666695,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 80
    },
    {
      "epoch": 0.09040178571428571,
      "grad_norm": 4.880607340204337,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 81
    },
    {
      "epoch": 0.09151785714285714,
      "grad_norm": 5.675387226031149,
      "learning_rate": 2e-05,
      "loss": 2.9219,
      "step": 82
    },
    {
      "epoch": 0.09263392857142858,
      "grad_norm": 4.822244926498086,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 83
    },
    {
      "epoch": 0.09375,
      "grad_norm": 5.246993994638488,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 84
    },
    {
      "epoch": 0.09486607142857142,
      "grad_norm": 5.365437669016616,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 85
    },
    {
      "epoch": 0.09598214285714286,
      "grad_norm": 4.693012149991979,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 86
    },
    {
      "epoch": 0.09709821428571429,
      "grad_norm": 5.449327693418202,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 87
    },
    {
      "epoch": 0.09821428571428571,
      "grad_norm": 4.885869174488196,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 88
    },
    {
      "epoch": 0.09933035714285714,
      "grad_norm": 4.919588518513742,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 89
    },
    {
      "epoch": 0.10044642857142858,
      "grad_norm": 5.011515684358079,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 90
    },
    {
      "epoch": 0.1015625,
      "grad_norm": 5.192746392384502,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 91
    },
    {
      "epoch": 0.10267857142857142,
      "grad_norm": 5.744993884964599,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 92
    },
    {
      "epoch": 0.10379464285714286,
      "grad_norm": 3.800407046105842,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 93
    },
    {
      "epoch": 0.10491071428571429,
      "grad_norm": 4.238044802514492,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 94
    },
    {
      "epoch": 0.10602678571428571,
      "grad_norm": 5.07951714597911,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 95
    },
    {
      "epoch": 0.10714285714285714,
      "grad_norm": 4.780198280740559,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 96
    },
    {
      "epoch": 0.10825892857142858,
      "grad_norm": 4.272564684212395,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 97
    },
    {
      "epoch": 0.109375,
      "grad_norm": 4.726894705469157,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 98
    },
    {
      "epoch": 0.11049107142857142,
      "grad_norm": 4.96539974104562,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 99
    },
    {
      "epoch": 0.11160714285714286,
      "grad_norm": 5.043302341718162,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 100
    },
    {
      "epoch": 0.11272321428571429,
      "grad_norm": 4.547819817378021,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 101
    },
    {
      "epoch": 0.11383928571428571,
      "grad_norm": 5.327671400186427,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 102
    },
    {
      "epoch": 0.11495535714285714,
      "grad_norm": 5.012937991854735,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 103
    },
    {
      "epoch": 0.11607142857142858,
      "grad_norm": 4.340149108887595,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 104
    },
    {
      "epoch": 0.1171875,
      "grad_norm": 4.956152466926903,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 105
    },
    {
      "epoch": 0.11830357142857142,
      "grad_norm": 4.785804944126185,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 106
    },
    {
      "epoch": 0.11941964285714286,
      "grad_norm": 4.431269369599007,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 107
    },
    {
      "epoch": 0.12053571428571429,
      "grad_norm": 4.7913980866313395,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 108
    },
    {
      "epoch": 0.12165178571428571,
      "grad_norm": 5.924198309153987,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 109
    },
    {
      "epoch": 0.12276785714285714,
      "grad_norm": 5.377041684494544,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 110
    },
    {
      "epoch": 0.12388392857142858,
      "grad_norm": 5.358904182996805,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 111
    },
    {
      "epoch": 0.125,
      "grad_norm": 4.979890590026488,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 112
    },
    {
      "epoch": 0.12611607142857142,
      "grad_norm": 5.186079196422482,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 113
    },
    {
      "epoch": 0.12723214285714285,
      "grad_norm": 5.129254376263673,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 114
    },
    {
      "epoch": 0.12834821428571427,
      "grad_norm": 4.767729206516721,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 115
    },
    {
      "epoch": 0.12946428571428573,
      "grad_norm": 5.573094659249564,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 116
    },
    {
      "epoch": 0.13058035714285715,
      "grad_norm": 5.347652091324301,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 117
    },
    {
      "epoch": 0.13169642857142858,
      "grad_norm": 5.421429622860902,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 118
    },
    {
      "epoch": 0.1328125,
      "grad_norm": 4.752026631762671,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 119
    },
    {
      "epoch": 0.13392857142857142,
      "grad_norm": 5.974699339998891,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 120
    },
    {
      "epoch": 0.13504464285714285,
      "grad_norm": 5.381149135432819,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 121
    },
    {
      "epoch": 0.13616071428571427,
      "grad_norm": 5.067710492577204,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 122
    },
    {
      "epoch": 0.13727678571428573,
      "grad_norm": 4.729334265765399,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 123
    },
    {
      "epoch": 0.13839285714285715,
      "grad_norm": 4.630528978500444,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 124
    },
    {
      "epoch": 0.13950892857142858,
      "grad_norm": 4.720231090368908,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 125
    },
    {
      "epoch": 0.140625,
      "grad_norm": 5.864592231775595,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 126
    },
    {
      "epoch": 0.14174107142857142,
      "grad_norm": 5.55009570482248,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 127
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 4.77610964730505,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 128
    },
    {
      "epoch": 0.14397321428571427,
      "grad_norm": 5.353534214215445,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 129
    },
    {
      "epoch": 0.14508928571428573,
      "grad_norm": 6.0802820928348424,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 130
    },
    {
      "epoch": 0.14620535714285715,
      "grad_norm": 6.287590694294681,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 131
    },
    {
      "epoch": 0.14732142857142858,
      "grad_norm": 4.910417236773405,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 132
    },
    {
      "epoch": 0.1484375,
      "grad_norm": 4.464446508360941,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 133
    },
    {
      "epoch": 0.14955357142857142,
      "grad_norm": 5.040095053261613,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 134
    },
    {
      "epoch": 0.15066964285714285,
      "grad_norm": 4.561919707363219,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 135
    },
    {
      "epoch": 0.15178571428571427,
      "grad_norm": 4.733170255165138,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 136
    },
    {
      "epoch": 0.15290178571428573,
      "grad_norm": 6.65344704806108,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 137
    },
    {
      "epoch": 0.15401785714285715,
      "grad_norm": 5.356209447932266,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 138
    },
    {
      "epoch": 0.15513392857142858,
      "grad_norm": 5.3580387456693055,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 139
    },
    {
      "epoch": 0.15625,
      "grad_norm": 5.569688087950964,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 140
    },
    {
      "epoch": 0.15736607142857142,
      "grad_norm": 5.824046549502556,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 141
    },
    {
      "epoch": 0.15848214285714285,
      "grad_norm": 4.977314491783535,
      "learning_rate": 2e-05,
      "loss": 2.8906,
      "step": 142
    },
    {
      "epoch": 0.15959821428571427,
      "grad_norm": 5.063110574174531,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 143
    },
    {
      "epoch": 0.16071428571428573,
      "grad_norm": 6.339391661897655,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 144
    },
    {
      "epoch": 0.16183035714285715,
      "grad_norm": 4.582333281181453,
      "learning_rate": 2e-05,
      "loss": 2.9688,
      "step": 145
    },
    {
      "epoch": 0.16294642857142858,
      "grad_norm": 5.367377203244943,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 146
    },
    {
      "epoch": 0.1640625,
      "grad_norm": 6.680092903686295,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 147
    },
    {
      "epoch": 0.16517857142857142,
      "grad_norm": 4.472201453539309,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 148
    },
    {
      "epoch": 0.16629464285714285,
      "grad_norm": 4.642459438092517,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 149
    },
    {
      "epoch": 0.16741071428571427,
      "grad_norm": 4.61954425833527,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 150
    },
    {
      "epoch": 0.16852678571428573,
      "grad_norm": 5.9386776620981365,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 151
    },
    {
      "epoch": 0.16964285714285715,
      "grad_norm": 6.084532124228088,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 152
    },
    {
      "epoch": 0.17075892857142858,
      "grad_norm": 4.087681681776893,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 153
    },
    {
      "epoch": 0.171875,
      "grad_norm": 6.224213591288618,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 154
    },
    {
      "epoch": 0.17299107142857142,
      "grad_norm": 4.368275464215769,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 155
    },
    {
      "epoch": 0.17410714285714285,
      "grad_norm": 4.596235125350103,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 156
    },
    {
      "epoch": 0.17522321428571427,
      "grad_norm": 4.104998762834547,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 157
    },
    {
      "epoch": 0.17633928571428573,
      "grad_norm": 4.911848926113566,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 158
    },
    {
      "epoch": 0.17745535714285715,
      "grad_norm": 4.9981036728800925,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 159
    },
    {
      "epoch": 0.17857142857142858,
      "grad_norm": 5.510851467998093,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 160
    },
    {
      "epoch": 0.1796875,
      "grad_norm": 5.139795591294709,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 161
    },
    {
      "epoch": 0.18080357142857142,
      "grad_norm": 4.437112357067863,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 162
    },
    {
      "epoch": 0.18191964285714285,
      "grad_norm": 5.690554318954808,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 163
    },
    {
      "epoch": 0.18303571428571427,
      "grad_norm": 5.34529343117522,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 164
    },
    {
      "epoch": 0.18415178571428573,
      "grad_norm": 4.872261742232438,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 165
    },
    {
      "epoch": 0.18526785714285715,
      "grad_norm": 4.157230479263477,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 166
    },
    {
      "epoch": 0.18638392857142858,
      "grad_norm": 4.349506969868449,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 167
    },
    {
      "epoch": 0.1875,
      "grad_norm": 4.497870585174852,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 168
    },
    {
      "epoch": 0.18861607142857142,
      "grad_norm": 5.917971395374744,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 169
    },
    {
      "epoch": 0.18973214285714285,
      "grad_norm": 5.802441588850042,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 170
    },
    {
      "epoch": 0.19084821428571427,
      "grad_norm": 5.316338204099572,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 171
    },
    {
      "epoch": 0.19196428571428573,
      "grad_norm": 4.6337257817105915,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 172
    },
    {
      "epoch": 0.19308035714285715,
      "grad_norm": 5.544854809139442,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 173
    },
    {
      "epoch": 0.19419642857142858,
      "grad_norm": 5.0812840280191285,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 174
    },
    {
      "epoch": 0.1953125,
      "grad_norm": 5.007017664080652,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 175
    },
    {
      "epoch": 0.19642857142857142,
      "grad_norm": 5.987366589781244,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 176
    },
    {
      "epoch": 0.19754464285714285,
      "grad_norm": 4.721525528452891,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 177
    },
    {
      "epoch": 0.19866071428571427,
      "grad_norm": 5.019483872417742,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 178
    },
    {
      "epoch": 0.19977678571428573,
      "grad_norm": 4.697501312780749,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 179
    },
    {
      "epoch": 0.20089285714285715,
      "grad_norm": 6.046201115248955,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 180
    },
    {
      "epoch": 0.20200892857142858,
      "grad_norm": 5.910415472171007,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 181
    },
    {
      "epoch": 0.203125,
      "grad_norm": 5.473306389087776,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 182
    },
    {
      "epoch": 0.20424107142857142,
      "grad_norm": 4.565682727749405,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 183
    },
    {
      "epoch": 0.20535714285714285,
      "grad_norm": 6.2328227238187806,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 184
    },
    {
      "epoch": 0.20647321428571427,
      "grad_norm": 5.93364014209044,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 185
    },
    {
      "epoch": 0.20758928571428573,
      "grad_norm": 5.318288597782362,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 186
    },
    {
      "epoch": 0.20870535714285715,
      "grad_norm": 9.244094239583756,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 187
    },
    {
      "epoch": 0.20982142857142858,
      "grad_norm": 5.2493517713917885,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 188
    },
    {
      "epoch": 0.2109375,
      "grad_norm": 4.718023847907817,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 189
    },
    {
      "epoch": 0.21205357142857142,
      "grad_norm": 5.1117706451353495,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 190
    },
    {
      "epoch": 0.21316964285714285,
      "grad_norm": 6.5400312346390415,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 191
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 4.622534073536627,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 192
    },
    {
      "epoch": 0.21540178571428573,
      "grad_norm": 4.9784670189788365,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 193
    },
    {
      "epoch": 0.21651785714285715,
      "grad_norm": 5.326543381816517,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 194
    },
    {
      "epoch": 0.21763392857142858,
      "grad_norm": 4.283091419725382,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 195
    },
    {
      "epoch": 0.21875,
      "grad_norm": 4.648454954980838,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 196
    },
    {
      "epoch": 0.21986607142857142,
      "grad_norm": 4.725599795986592,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 197
    },
    {
      "epoch": 0.22098214285714285,
      "grad_norm": 4.9415366236178055,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 198
    },
    {
      "epoch": 0.22209821428571427,
      "grad_norm": 5.543879086202631,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 199
    },
    {
      "epoch": 0.22321428571428573,
      "grad_norm": 4.6114693631699915,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 200
    },
    {
      "epoch": 0.22433035714285715,
      "grad_norm": 4.455777071656654,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 201
    },
    {
      "epoch": 0.22544642857142858,
      "grad_norm": 5.426585916659562,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 202
    },
    {
      "epoch": 0.2265625,
      "grad_norm": 5.37508598796026,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 203
    },
    {
      "epoch": 0.22767857142857142,
      "grad_norm": 4.504463553703966,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 204
    },
    {
      "epoch": 0.22879464285714285,
      "grad_norm": 5.632933312255413,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 205
    },
    {
      "epoch": 0.22991071428571427,
      "grad_norm": 4.820290487959167,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 206
    },
    {
      "epoch": 0.23102678571428573,
      "grad_norm": 5.0591413578384525,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 207
    },
    {
      "epoch": 0.23214285714285715,
      "grad_norm": 4.90720053919676,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 208
    },
    {
      "epoch": 0.23325892857142858,
      "grad_norm": 5.584408987152432,
      "learning_rate": 2e-05,
      "loss": 2.9531,
      "step": 209
    },
    {
      "epoch": 0.234375,
      "grad_norm": 6.483344982186383,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 210
    },
    {
      "epoch": 0.23549107142857142,
      "grad_norm": 4.891662989812903,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 211
    },
    {
      "epoch": 0.23660714285714285,
      "grad_norm": 4.715300941994764,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 212
    },
    {
      "epoch": 0.23772321428571427,
      "grad_norm": 4.917011966533966,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 213
    },
    {
      "epoch": 0.23883928571428573,
      "grad_norm": 5.344035610548399,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 214
    },
    {
      "epoch": 0.23995535714285715,
      "grad_norm": 5.841296825708515,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 215
    },
    {
      "epoch": 0.24107142857142858,
      "grad_norm": 5.567403122420545,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 216
    },
    {
      "epoch": 0.2421875,
      "grad_norm": 7.110422885257681,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 217
    },
    {
      "epoch": 0.24330357142857142,
      "grad_norm": 5.8911103376815355,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 218
    },
    {
      "epoch": 0.24441964285714285,
      "grad_norm": 5.159853234572748,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 219
    },
    {
      "epoch": 0.24553571428571427,
      "grad_norm": 4.25010334015838,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 220
    },
    {
      "epoch": 0.24665178571428573,
      "grad_norm": 5.926599948903188,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 221
    },
    {
      "epoch": 0.24776785714285715,
      "grad_norm": 5.114772930689475,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 222
    },
    {
      "epoch": 0.24888392857142858,
      "grad_norm": 5.616855086548894,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 223
    },
    {
      "epoch": 0.25,
      "grad_norm": 4.884625570370895,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 224
    },
    {
      "epoch": 0.25111607142857145,
      "grad_norm": 6.505135334760109,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 225
    },
    {
      "epoch": 0.25223214285714285,
      "grad_norm": 7.250049962995516,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 226
    },
    {
      "epoch": 0.2533482142857143,
      "grad_norm": 5.640007119784958,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 227
    },
    {
      "epoch": 0.2544642857142857,
      "grad_norm": 4.8684104062266265,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 228
    },
    {
      "epoch": 0.25558035714285715,
      "grad_norm": 5.246299659371866,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 229
    },
    {
      "epoch": 0.25669642857142855,
      "grad_norm": 6.197864698442289,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 230
    },
    {
      "epoch": 0.2578125,
      "grad_norm": 5.156051058484374,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 231
    },
    {
      "epoch": 0.25892857142857145,
      "grad_norm": 5.42501117012951,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 232
    },
    {
      "epoch": 0.26004464285714285,
      "grad_norm": 6.101554902984205,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 233
    },
    {
      "epoch": 0.2611607142857143,
      "grad_norm": 4.673240223255446,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 234
    },
    {
      "epoch": 0.2622767857142857,
      "grad_norm": 5.682365722933665,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 235
    },
    {
      "epoch": 0.26339285714285715,
      "grad_norm": 5.92508714921797,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 236
    },
    {
      "epoch": 0.26450892857142855,
      "grad_norm": 5.527002715594196,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 237
    },
    {
      "epoch": 0.265625,
      "grad_norm": 4.861116207642927,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 238
    },
    {
      "epoch": 0.26674107142857145,
      "grad_norm": 4.9088171632362085,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 239
    },
    {
      "epoch": 0.26785714285714285,
      "grad_norm": 5.855620912498212,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 240
    },
    {
      "epoch": 0.2689732142857143,
      "grad_norm": 5.0733099057221684,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 241
    },
    {
      "epoch": 0.2700892857142857,
      "grad_norm": 5.989621966709335,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 242
    },
    {
      "epoch": 0.27120535714285715,
      "grad_norm": 4.982177302652287,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 243
    },
    {
      "epoch": 0.27232142857142855,
      "grad_norm": 5.222638222315441,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 244
    },
    {
      "epoch": 0.2734375,
      "grad_norm": 5.154905593698026,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 245
    },
    {
      "epoch": 0.27455357142857145,
      "grad_norm": 6.035031319020624,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 246
    },
    {
      "epoch": 0.27566964285714285,
      "grad_norm": 4.8919104357200816,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 247
    },
    {
      "epoch": 0.2767857142857143,
      "grad_norm": 3.995860097179417,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 248
    },
    {
      "epoch": 0.2779017857142857,
      "grad_norm": 4.607390122853896,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 249
    },
    {
      "epoch": 0.27901785714285715,
      "grad_norm": 4.394756002697058,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 250
    },
    {
      "epoch": 0.28013392857142855,
      "grad_norm": 4.581849334008666,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 251
    },
    {
      "epoch": 0.28125,
      "grad_norm": 4.570974643313947,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 252
    },
    {
      "epoch": 0.28236607142857145,
      "grad_norm": 5.2103735682870616,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 253
    },
    {
      "epoch": 0.28348214285714285,
      "grad_norm": 6.02447386620731,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 254
    },
    {
      "epoch": 0.2845982142857143,
      "grad_norm": 4.995303310974033,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 255
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 7.167986441114688,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 256
    },
    {
      "epoch": 0.28683035714285715,
      "grad_norm": 6.524993932244898,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 257
    },
    {
      "epoch": 0.28794642857142855,
      "grad_norm": 4.578586452874299,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 258
    },
    {
      "epoch": 0.2890625,
      "grad_norm": 5.738987692650044,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 259
    },
    {
      "epoch": 0.29017857142857145,
      "grad_norm": 5.721515767690259,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 260
    },
    {
      "epoch": 0.29129464285714285,
      "grad_norm": 5.5721865096830046,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 261
    },
    {
      "epoch": 0.2924107142857143,
      "grad_norm": 5.65983154115369,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 262
    },
    {
      "epoch": 0.2935267857142857,
      "grad_norm": 5.837739419844417,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 263
    },
    {
      "epoch": 0.29464285714285715,
      "grad_norm": 6.330631551983623,
      "learning_rate": 2e-05,
      "loss": 3.0312,
      "step": 264
    },
    {
      "epoch": 0.29575892857142855,
      "grad_norm": 5.489079083264709,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 265
    },
    {
      "epoch": 0.296875,
      "grad_norm": 5.480703314702249,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 266
    },
    {
      "epoch": 0.29799107142857145,
      "grad_norm": 5.41364619266832,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 267
    },
    {
      "epoch": 0.29910714285714285,
      "grad_norm": 5.152914532654801,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 268
    },
    {
      "epoch": 0.3002232142857143,
      "grad_norm": 5.470606194602912,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 269
    },
    {
      "epoch": 0.3013392857142857,
      "grad_norm": 5.889479608238374,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 270
    },
    {
      "epoch": 0.30245535714285715,
      "grad_norm": 6.245747382885006,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 271
    },
    {
      "epoch": 0.30357142857142855,
      "grad_norm": 5.623342948736502,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 272
    },
    {
      "epoch": 0.3046875,
      "grad_norm": 5.90200300337235,
      "learning_rate": 2e-05,
      "loss": 2.8594,
      "step": 273
    },
    {
      "epoch": 0.30580357142857145,
      "grad_norm": 5.194870702425177,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 274
    },
    {
      "epoch": 0.30691964285714285,
      "grad_norm": 5.794914889446456,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 275
    },
    {
      "epoch": 0.3080357142857143,
      "grad_norm": 6.381987639823284,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 276
    },
    {
      "epoch": 0.3091517857142857,
      "grad_norm": 6.1834017563839225,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 277
    },
    {
      "epoch": 0.31026785714285715,
      "grad_norm": 5.170698168348872,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 278
    },
    {
      "epoch": 0.31138392857142855,
      "grad_norm": 6.17602319354166,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 279
    },
    {
      "epoch": 0.3125,
      "grad_norm": 5.725733454018046,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 280
    },
    {
      "epoch": 0.31361607142857145,
      "grad_norm": 4.930488614452633,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 281
    },
    {
      "epoch": 0.31473214285714285,
      "grad_norm": 5.356077894774006,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 282
    },
    {
      "epoch": 0.3158482142857143,
      "grad_norm": 7.393537275045752,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 283
    },
    {
      "epoch": 0.3169642857142857,
      "grad_norm": 6.313266564480613,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 284
    },
    {
      "epoch": 0.31808035714285715,
      "grad_norm": 6.256324757762068,
      "learning_rate": 2e-05,
      "loss": 2.8906,
      "step": 285
    },
    {
      "epoch": 0.31919642857142855,
      "grad_norm": 6.937892129876937,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 286
    },
    {
      "epoch": 0.3203125,
      "grad_norm": 5.939261007145628,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 287
    },
    {
      "epoch": 0.32142857142857145,
      "grad_norm": 4.970244513880297,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 288
    },
    {
      "epoch": 0.32254464285714285,
      "grad_norm": 6.100740073085076,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 289
    },
    {
      "epoch": 0.3236607142857143,
      "grad_norm": 7.817134691450799,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 290
    },
    {
      "epoch": 0.3247767857142857,
      "grad_norm": 5.9876404696188645,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 291
    },
    {
      "epoch": 0.32589285714285715,
      "grad_norm": 6.7560981222037455,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 292
    },
    {
      "epoch": 0.32700892857142855,
      "grad_norm": 4.8528760131213575,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 293
    },
    {
      "epoch": 0.328125,
      "grad_norm": 6.479218529036824,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 294
    },
    {
      "epoch": 0.32924107142857145,
      "grad_norm": 5.970693461617088,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 295
    },
    {
      "epoch": 0.33035714285714285,
      "grad_norm": 5.985771874748177,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 296
    },
    {
      "epoch": 0.3314732142857143,
      "grad_norm": 7.120535499634246,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 297
    },
    {
      "epoch": 0.3325892857142857,
      "grad_norm": 5.5017586636818585,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 298
    },
    {
      "epoch": 0.33370535714285715,
      "grad_norm": 8.306176640131177,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 299
    },
    {
      "epoch": 0.33482142857142855,
      "grad_norm": 4.681558910307139,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 300
    },
    {
      "epoch": 0.3359375,
      "grad_norm": 6.403087530825947,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 301
    },
    {
      "epoch": 0.33705357142857145,
      "grad_norm": 5.738328659066505,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 302
    },
    {
      "epoch": 0.33816964285714285,
      "grad_norm": 5.202189004026315,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 303
    },
    {
      "epoch": 0.3392857142857143,
      "grad_norm": 5.012230752872492,
      "learning_rate": 2e-05,
      "loss": 1.0859,
      "step": 304
    },
    {
      "epoch": 0.3404017857142857,
      "grad_norm": 5.595198360432458,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 305
    },
    {
      "epoch": 0.34151785714285715,
      "grad_norm": 4.108237430988085,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 306
    },
    {
      "epoch": 0.34263392857142855,
      "grad_norm": 7.445207832035633,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 307
    },
    {
      "epoch": 0.34375,
      "grad_norm": 5.102228760086666,
      "learning_rate": 2e-05,
      "loss": 1.2734,
      "step": 308
    },
    {
      "epoch": 0.34486607142857145,
      "grad_norm": 5.918815693531516,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 309
    },
    {
      "epoch": 0.34598214285714285,
      "grad_norm": 6.017954914081264,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 310
    },
    {
      "epoch": 0.3470982142857143,
      "grad_norm": 6.405339743119702,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 311
    },
    {
      "epoch": 0.3482142857142857,
      "grad_norm": 5.943174245522186,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 312
    },
    {
      "epoch": 0.34933035714285715,
      "grad_norm": 6.14619126533703,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 313
    },
    {
      "epoch": 0.35044642857142855,
      "grad_norm": 4.043394727049005,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 314
    },
    {
      "epoch": 0.3515625,
      "grad_norm": 6.491212784856782,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 315
    },
    {
      "epoch": 0.35267857142857145,
      "grad_norm": 6.122075727934848,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 316
    },
    {
      "epoch": 0.35379464285714285,
      "grad_norm": 6.7001201964117225,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 317
    },
    {
      "epoch": 0.3549107142857143,
      "grad_norm": 5.246304713916152,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 318
    },
    {
      "epoch": 0.3560267857142857,
      "grad_norm": 5.38420903793464,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 319
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 5.468480386425721,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 320
    },
    {
      "epoch": 0.35825892857142855,
      "grad_norm": 6.521058901771744,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 321
    },
    {
      "epoch": 0.359375,
      "grad_norm": 5.998039052021094,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 322
    },
    {
      "epoch": 0.36049107142857145,
      "grad_norm": 5.6877215069394245,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 323
    },
    {
      "epoch": 0.36160714285714285,
      "grad_norm": 5.796934576746362,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 324
    },
    {
      "epoch": 0.3627232142857143,
      "grad_norm": 5.75865595517348,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 325
    },
    {
      "epoch": 0.3638392857142857,
      "grad_norm": 6.259569392547488,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 326
    },
    {
      "epoch": 0.36495535714285715,
      "grad_norm": 6.219423948128885,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 327
    },
    {
      "epoch": 0.36607142857142855,
      "grad_norm": 5.7428639267416335,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 328
    },
    {
      "epoch": 0.3671875,
      "grad_norm": 5.829919905472288,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 329
    },
    {
      "epoch": 0.36830357142857145,
      "grad_norm": 5.836898358141586,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 330
    },
    {
      "epoch": 0.36941964285714285,
      "grad_norm": 6.685005677280061,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 331
    },
    {
      "epoch": 0.3705357142857143,
      "grad_norm": 6.981256958896409,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 332
    },
    {
      "epoch": 0.3716517857142857,
      "grad_norm": 5.260740722332293,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 333
    },
    {
      "epoch": 0.37276785714285715,
      "grad_norm": 6.946577895491614,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 334
    },
    {
      "epoch": 0.37388392857142855,
      "grad_norm": 5.898512311305371,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 335
    },
    {
      "epoch": 0.375,
      "grad_norm": 6.6867196651542,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 336
    },
    {
      "epoch": 0.37611607142857145,
      "grad_norm": 6.2167047098151205,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 337
    },
    {
      "epoch": 0.37723214285714285,
      "grad_norm": 6.481859342806182,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 338
    },
    {
      "epoch": 0.3783482142857143,
      "grad_norm": 6.672480363971137,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 339
    },
    {
      "epoch": 0.3794642857142857,
      "grad_norm": 6.474222763479225,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 340
    },
    {
      "epoch": 0.38058035714285715,
      "grad_norm": 6.865369480979002,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 341
    },
    {
      "epoch": 0.38169642857142855,
      "grad_norm": 5.700214233178537,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 342
    },
    {
      "epoch": 0.3828125,
      "grad_norm": 4.330382195168137,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 343
    },
    {
      "epoch": 0.38392857142857145,
      "grad_norm": 5.990920890336209,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 344
    },
    {
      "epoch": 0.38504464285714285,
      "grad_norm": 7.716361785749903,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 345
    },
    {
      "epoch": 0.3861607142857143,
      "grad_norm": 5.641672105532815,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 346
    },
    {
      "epoch": 0.3872767857142857,
      "grad_norm": 6.247139794342922,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 347
    },
    {
      "epoch": 0.38839285714285715,
      "grad_norm": 4.526405318878593,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 348
    },
    {
      "epoch": 0.38950892857142855,
      "grad_norm": 5.84802183091192,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 349
    },
    {
      "epoch": 0.390625,
      "grad_norm": 6.3075716987046135,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 350
    },
    {
      "epoch": 0.39174107142857145,
      "grad_norm": 5.3962683021827855,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 351
    },
    {
      "epoch": 0.39285714285714285,
      "grad_norm": 6.859895369254169,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 352
    },
    {
      "epoch": 0.3939732142857143,
      "grad_norm": 5.985377035158306,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 353
    },
    {
      "epoch": 0.3950892857142857,
      "grad_norm": 5.410371489631165,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 354
    },
    {
      "epoch": 0.39620535714285715,
      "grad_norm": 6.665188916366339,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 355
    },
    {
      "epoch": 0.39732142857142855,
      "grad_norm": 5.391209519947215,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 356
    },
    {
      "epoch": 0.3984375,
      "grad_norm": 6.001163232692434,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 357
    },
    {
      "epoch": 0.39955357142857145,
      "grad_norm": 6.603579911502081,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 358
    },
    {
      "epoch": 0.40066964285714285,
      "grad_norm": 6.579369133187988,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 359
    },
    {
      "epoch": 0.4017857142857143,
      "grad_norm": 5.545349699122646,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 360
    },
    {
      "epoch": 0.4029017857142857,
      "grad_norm": 6.105307562591906,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 361
    },
    {
      "epoch": 0.40401785714285715,
      "grad_norm": 6.7813003407687695,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 362
    },
    {
      "epoch": 0.40513392857142855,
      "grad_norm": 7.046860244609957,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 363
    },
    {
      "epoch": 0.40625,
      "grad_norm": 5.426861969461977,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 364
    },
    {
      "epoch": 0.40736607142857145,
      "grad_norm": 6.195293264878027,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 365
    },
    {
      "epoch": 0.40848214285714285,
      "grad_norm": 6.200447050861281,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 366
    },
    {
      "epoch": 0.4095982142857143,
      "grad_norm": 5.595228344398224,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 367
    },
    {
      "epoch": 0.4107142857142857,
      "grad_norm": 5.8432601289481125,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 368
    },
    {
      "epoch": 0.41183035714285715,
      "grad_norm": 6.079638208423657,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 369
    },
    {
      "epoch": 0.41294642857142855,
      "grad_norm": 5.402080857317698,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 370
    },
    {
      "epoch": 0.4140625,
      "grad_norm": 5.373155401106747,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 371
    },
    {
      "epoch": 0.41517857142857145,
      "grad_norm": 5.212789404143644,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 372
    },
    {
      "epoch": 0.41629464285714285,
      "grad_norm": 6.592401396032539,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 373
    },
    {
      "epoch": 0.4174107142857143,
      "grad_norm": 5.444279805970692,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 374
    },
    {
      "epoch": 0.4185267857142857,
      "grad_norm": 6.129844170249731,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 375
    },
    {
      "epoch": 0.41964285714285715,
      "grad_norm": 6.34720215493731,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 376
    },
    {
      "epoch": 0.42075892857142855,
      "grad_norm": 5.276766947606829,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 377
    },
    {
      "epoch": 0.421875,
      "grad_norm": 6.5468063152206115,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 378
    },
    {
      "epoch": 0.42299107142857145,
      "grad_norm": 6.590926383115588,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 379
    },
    {
      "epoch": 0.42410714285714285,
      "grad_norm": 6.499396319898452,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 380
    },
    {
      "epoch": 0.4252232142857143,
      "grad_norm": 6.024348317348521,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 381
    },
    {
      "epoch": 0.4263392857142857,
      "grad_norm": 4.818488782305947,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 382
    },
    {
      "epoch": 0.42745535714285715,
      "grad_norm": 6.097826451283656,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 383
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 5.6921555500568575,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 384
    },
    {
      "epoch": 0.4296875,
      "grad_norm": 7.504548885171974,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 385
    },
    {
      "epoch": 0.43080357142857145,
      "grad_norm": 6.243656641408445,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 386
    },
    {
      "epoch": 0.43191964285714285,
      "grad_norm": 7.082077410520427,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 387
    },
    {
      "epoch": 0.4330357142857143,
      "grad_norm": 6.263167564970838,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 388
    },
    {
      "epoch": 0.4341517857142857,
      "grad_norm": 5.9703564545701795,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 389
    },
    {
      "epoch": 0.43526785714285715,
      "grad_norm": 6.70410265204994,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 390
    },
    {
      "epoch": 0.43638392857142855,
      "grad_norm": 5.855712967355312,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 391
    },
    {
      "epoch": 0.4375,
      "grad_norm": 4.492800181110915,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 392
    },
    {
      "epoch": 0.43861607142857145,
      "grad_norm": 6.5544548419871544,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 393
    },
    {
      "epoch": 0.43973214285714285,
      "grad_norm": 7.247362772010449,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 394
    },
    {
      "epoch": 0.4408482142857143,
      "grad_norm": 5.884066207580705,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 395
    },
    {
      "epoch": 0.4419642857142857,
      "grad_norm": 6.8943334875672315,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 396
    },
    {
      "epoch": 0.44308035714285715,
      "grad_norm": 6.864360297879285,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 397
    },
    {
      "epoch": 0.44419642857142855,
      "grad_norm": 6.140238793237744,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 398
    },
    {
      "epoch": 0.4453125,
      "grad_norm": 5.3291157336995285,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 399
    },
    {
      "epoch": 0.44642857142857145,
      "grad_norm": 5.598887102514742,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 400
    },
    {
      "epoch": 0.44754464285714285,
      "grad_norm": 8.286684196049563,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 401
    },
    {
      "epoch": 0.4486607142857143,
      "grad_norm": 5.788371395199885,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 402
    },
    {
      "epoch": 0.4497767857142857,
      "grad_norm": 5.244714959085075,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 403
    },
    {
      "epoch": 0.45089285714285715,
      "grad_norm": 7.9648950652684505,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 404
    },
    {
      "epoch": 0.45200892857142855,
      "grad_norm": 5.840505536000766,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 405
    },
    {
      "epoch": 0.453125,
      "grad_norm": 5.325114943786698,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 406
    },
    {
      "epoch": 0.45424107142857145,
      "grad_norm": 6.860372358841779,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 407
    },
    {
      "epoch": 0.45535714285714285,
      "grad_norm": 5.283241798012223,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 408
    },
    {
      "epoch": 0.4564732142857143,
      "grad_norm": 6.14959717966234,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 409
    },
    {
      "epoch": 0.4575892857142857,
      "grad_norm": 6.778686519742661,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 410
    },
    {
      "epoch": 0.45870535714285715,
      "grad_norm": 7.232801883524477,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 411
    },
    {
      "epoch": 0.45982142857142855,
      "grad_norm": 6.145644586883981,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 412
    },
    {
      "epoch": 0.4609375,
      "grad_norm": 5.898979644439567,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 413
    },
    {
      "epoch": 0.46205357142857145,
      "grad_norm": 5.569789972706816,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 414
    },
    {
      "epoch": 0.46316964285714285,
      "grad_norm": 5.045999108828418,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 415
    },
    {
      "epoch": 0.4642857142857143,
      "grad_norm": 4.894099241373748,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 416
    },
    {
      "epoch": 0.4654017857142857,
      "grad_norm": 6.449555308330442,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 417
    },
    {
      "epoch": 0.46651785714285715,
      "grad_norm": 6.093921261346725,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 418
    },
    {
      "epoch": 0.46763392857142855,
      "grad_norm": 5.105227635925224,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 419
    },
    {
      "epoch": 0.46875,
      "grad_norm": 4.195421234821704,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 420
    },
    {
      "epoch": 0.46986607142857145,
      "grad_norm": 5.84584469759069,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 421
    },
    {
      "epoch": 0.47098214285714285,
      "grad_norm": 7.367747184371759,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 422
    },
    {
      "epoch": 0.4720982142857143,
      "grad_norm": 6.056601658643672,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 423
    },
    {
      "epoch": 0.4732142857142857,
      "grad_norm": 6.8024343216931245,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 424
    },
    {
      "epoch": 0.47433035714285715,
      "grad_norm": 7.195987718691981,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 425
    },
    {
      "epoch": 0.47544642857142855,
      "grad_norm": 7.293579492177266,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 426
    },
    {
      "epoch": 0.4765625,
      "grad_norm": 6.210461453096692,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 427
    },
    {
      "epoch": 0.47767857142857145,
      "grad_norm": 7.465061823243178,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 428
    },
    {
      "epoch": 0.47879464285714285,
      "grad_norm": 6.780365154528446,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 429
    },
    {
      "epoch": 0.4799107142857143,
      "grad_norm": 5.973983523759283,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 430
    },
    {
      "epoch": 0.4810267857142857,
      "grad_norm": 7.040120640539666,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 431
    },
    {
      "epoch": 0.48214285714285715,
      "grad_norm": 5.181417951339893,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 432
    },
    {
      "epoch": 0.48325892857142855,
      "grad_norm": 9.015996162493627,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 433
    },
    {
      "epoch": 0.484375,
      "grad_norm": 6.086607805504361,
      "learning_rate": 2e-05,
      "loss": 1.4688,
      "step": 434
    },
    {
      "epoch": 0.48549107142857145,
      "grad_norm": 7.675171980234482,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 435
    },
    {
      "epoch": 0.48660714285714285,
      "grad_norm": 7.196074650717554,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 436
    },
    {
      "epoch": 0.4877232142857143,
      "grad_norm": 7.247741196750318,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 437
    },
    {
      "epoch": 0.4888392857142857,
      "grad_norm": 7.079544525807087,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 438
    },
    {
      "epoch": 0.48995535714285715,
      "grad_norm": 5.821062052032175,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 439
    },
    {
      "epoch": 0.49107142857142855,
      "grad_norm": 7.621067196834796,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 440
    },
    {
      "epoch": 0.4921875,
      "grad_norm": 6.191078994075534,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 441
    },
    {
      "epoch": 0.49330357142857145,
      "grad_norm": 7.656098911630931,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 442
    },
    {
      "epoch": 0.49441964285714285,
      "grad_norm": 6.580625077145156,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 443
    },
    {
      "epoch": 0.4955357142857143,
      "grad_norm": 5.615320082987715,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 444
    },
    {
      "epoch": 0.4966517857142857,
      "grad_norm": 5.540945424055651,
      "learning_rate": 2e-05,
      "loss": 2.8594,
      "step": 445
    },
    {
      "epoch": 0.49776785714285715,
      "grad_norm": 7.179715882567501,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 446
    },
    {
      "epoch": 0.49888392857142855,
      "grad_norm": 6.506292836312854,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 447
    },
    {
      "epoch": 0.5,
      "grad_norm": 10.789402229886935,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 448
    },
    {
      "epoch": 0.5011160714285714,
      "grad_norm": 6.1447604043094195,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 449
    },
    {
      "epoch": 0.5022321428571429,
      "grad_norm": 8.116584067649637,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 450
    },
    {
      "epoch": 0.5033482142857143,
      "grad_norm": 5.982471564908289,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 451
    },
    {
      "epoch": 0.5044642857142857,
      "grad_norm": 7.097930942452871,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 452
    },
    {
      "epoch": 0.5055803571428571,
      "grad_norm": 5.4296084797052,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 453
    },
    {
      "epoch": 0.5066964285714286,
      "grad_norm": 6.182447680161965,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 454
    },
    {
      "epoch": 0.5078125,
      "grad_norm": 6.603200965927261,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 455
    },
    {
      "epoch": 0.5089285714285714,
      "grad_norm": 7.816722907344761,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 456
    },
    {
      "epoch": 0.5100446428571429,
      "grad_norm": 7.048079859801893,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 457
    },
    {
      "epoch": 0.5111607142857143,
      "grad_norm": 8.243049639406017,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 458
    },
    {
      "epoch": 0.5122767857142857,
      "grad_norm": 6.659956671556887,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 459
    },
    {
      "epoch": 0.5133928571428571,
      "grad_norm": 6.126250657764653,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 460
    },
    {
      "epoch": 0.5145089285714286,
      "grad_norm": 6.124736043291105,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 461
    },
    {
      "epoch": 0.515625,
      "grad_norm": 6.371795427964414,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 462
    },
    {
      "epoch": 0.5167410714285714,
      "grad_norm": 6.476056901849557,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 463
    },
    {
      "epoch": 0.5178571428571429,
      "grad_norm": 7.232511118933011,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 464
    },
    {
      "epoch": 0.5189732142857143,
      "grad_norm": 17.891802830408736,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 465
    },
    {
      "epoch": 0.5200892857142857,
      "grad_norm": 6.473444054020356,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 466
    },
    {
      "epoch": 0.5212053571428571,
      "grad_norm": 7.4946040906703475,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 467
    },
    {
      "epoch": 0.5223214285714286,
      "grad_norm": 7.517101274756477,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 468
    },
    {
      "epoch": 0.5234375,
      "grad_norm": 5.484951120052039,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 469
    },
    {
      "epoch": 0.5245535714285714,
      "grad_norm": 5.403223141452,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 470
    },
    {
      "epoch": 0.5256696428571429,
      "grad_norm": 6.032150457320694,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 471
    },
    {
      "epoch": 0.5267857142857143,
      "grad_norm": 6.908303312738406,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 472
    },
    {
      "epoch": 0.5279017857142857,
      "grad_norm": 6.01720104010625,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 473
    },
    {
      "epoch": 0.5290178571428571,
      "grad_norm": 5.289694474262655,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 474
    },
    {
      "epoch": 0.5301339285714286,
      "grad_norm": 6.054021005714556,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 475
    },
    {
      "epoch": 0.53125,
      "grad_norm": 7.315500183203906,
      "learning_rate": 2e-05,
      "loss": 2.8906,
      "step": 476
    },
    {
      "epoch": 0.5323660714285714,
      "grad_norm": 5.959600683071562,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 477
    },
    {
      "epoch": 0.5334821428571429,
      "grad_norm": 7.471727914337743,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 478
    },
    {
      "epoch": 0.5345982142857143,
      "grad_norm": 5.418746173238664,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 479
    },
    {
      "epoch": 0.5357142857142857,
      "grad_norm": 6.576982561255422,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 480
    },
    {
      "epoch": 0.5368303571428571,
      "grad_norm": 7.05482827603986,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 481
    },
    {
      "epoch": 0.5379464285714286,
      "grad_norm": 6.775847449417505,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 482
    },
    {
      "epoch": 0.5390625,
      "grad_norm": 6.810206150635688,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 483
    },
    {
      "epoch": 0.5401785714285714,
      "grad_norm": 5.231155553741888,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 484
    },
    {
      "epoch": 0.5412946428571429,
      "grad_norm": 5.5019347897277235,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 485
    },
    {
      "epoch": 0.5424107142857143,
      "grad_norm": 7.681471626862553,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 486
    },
    {
      "epoch": 0.5435267857142857,
      "grad_norm": 7.045405885975734,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 487
    },
    {
      "epoch": 0.5446428571428571,
      "grad_norm": 7.22107619181274,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 488
    },
    {
      "epoch": 0.5457589285714286,
      "grad_norm": 6.758795494377998,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 489
    },
    {
      "epoch": 0.546875,
      "grad_norm": 7.046430042982425,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 490
    },
    {
      "epoch": 0.5479910714285714,
      "grad_norm": 7.555019298129235,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 491
    },
    {
      "epoch": 0.5491071428571429,
      "grad_norm": 21.320133622636387,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 492
    },
    {
      "epoch": 0.5502232142857143,
      "grad_norm": 5.647300531352911,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 493
    },
    {
      "epoch": 0.5513392857142857,
      "grad_norm": 7.457379278094384,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 494
    },
    {
      "epoch": 0.5524553571428571,
      "grad_norm": 6.462934028920638,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 495
    },
    {
      "epoch": 0.5535714285714286,
      "grad_norm": 6.832618514429236,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 496
    },
    {
      "epoch": 0.5546875,
      "grad_norm": 6.924903099903694,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 497
    },
    {
      "epoch": 0.5558035714285714,
      "grad_norm": 7.745826770596821,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 498
    },
    {
      "epoch": 0.5569196428571429,
      "grad_norm": 6.476621917596814,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 499
    },
    {
      "epoch": 0.5580357142857143,
      "grad_norm": 6.37764145462918,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 500
    },
    {
      "epoch": 0.5591517857142857,
      "grad_norm": 7.287321350753652,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 501
    },
    {
      "epoch": 0.5602678571428571,
      "grad_norm": 7.544470055865645,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 502
    },
    {
      "epoch": 0.5613839285714286,
      "grad_norm": 5.433592106725318,
      "learning_rate": 2e-05,
      "loss": 2.9219,
      "step": 503
    },
    {
      "epoch": 0.5625,
      "grad_norm": 7.33444143982492,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 504
    },
    {
      "epoch": 0.5636160714285714,
      "grad_norm": 5.713758572542107,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 505
    },
    {
      "epoch": 0.5647321428571429,
      "grad_norm": 6.707458073683937,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 506
    },
    {
      "epoch": 0.5658482142857143,
      "grad_norm": 6.803334840536561,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 507
    },
    {
      "epoch": 0.5669642857142857,
      "grad_norm": 6.693282613415061,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 508
    },
    {
      "epoch": 0.5680803571428571,
      "grad_norm": 7.0015250940395095,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 509
    },
    {
      "epoch": 0.5691964285714286,
      "grad_norm": 7.398563117347831,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 510
    },
    {
      "epoch": 0.5703125,
      "grad_norm": 5.926340025577637,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 511
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 7.858769337353693,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 512
    },
    {
      "epoch": 0.5725446428571429,
      "grad_norm": 9.15600071934991,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 513
    },
    {
      "epoch": 0.5736607142857143,
      "grad_norm": 8.089059350747743,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 514
    },
    {
      "epoch": 0.5747767857142857,
      "grad_norm": 6.603976613525273,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 515
    },
    {
      "epoch": 0.5758928571428571,
      "grad_norm": 6.785829823358593,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 516
    },
    {
      "epoch": 0.5770089285714286,
      "grad_norm": 6.787069524523533,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 517
    },
    {
      "epoch": 0.578125,
      "grad_norm": 7.404594138145085,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 518
    },
    {
      "epoch": 0.5792410714285714,
      "grad_norm": 5.197338671742264,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 519
    },
    {
      "epoch": 0.5803571428571429,
      "grad_norm": 6.503093819456712,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 520
    },
    {
      "epoch": 0.5814732142857143,
      "grad_norm": 7.234193494894187,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 521
    },
    {
      "epoch": 0.5825892857142857,
      "grad_norm": 6.382905598785433,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 522
    },
    {
      "epoch": 0.5837053571428571,
      "grad_norm": 6.259005172333159,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 523
    },
    {
      "epoch": 0.5848214285714286,
      "grad_norm": 7.500613290315144,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 524
    },
    {
      "epoch": 0.5859375,
      "grad_norm": 6.752121141840796,
      "learning_rate": 2e-05,
      "loss": 2.8438,
      "step": 525
    },
    {
      "epoch": 0.5870535714285714,
      "grad_norm": 8.674907180311699,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 526
    },
    {
      "epoch": 0.5881696428571429,
      "grad_norm": 6.230415798326659,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 527
    },
    {
      "epoch": 0.5892857142857143,
      "grad_norm": 7.473396496556749,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 528
    },
    {
      "epoch": 0.5904017857142857,
      "grad_norm": 7.0905655610625,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 529
    },
    {
      "epoch": 0.5915178571428571,
      "grad_norm": 7.547132878917807,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 530
    },
    {
      "epoch": 0.5926339285714286,
      "grad_norm": 7.368677873502134,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 531
    },
    {
      "epoch": 0.59375,
      "grad_norm": 7.629273979369373,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 532
    },
    {
      "epoch": 0.5948660714285714,
      "grad_norm": 6.343202035930694,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 533
    },
    {
      "epoch": 0.5959821428571429,
      "grad_norm": 7.113872410593122,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 534
    },
    {
      "epoch": 0.5970982142857143,
      "grad_norm": 6.861135827815423,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 535
    },
    {
      "epoch": 0.5982142857142857,
      "grad_norm": 8.750357965847202,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 536
    },
    {
      "epoch": 0.5993303571428571,
      "grad_norm": 6.641071597773124,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 537
    },
    {
      "epoch": 0.6004464285714286,
      "grad_norm": 6.5215686581069745,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 538
    },
    {
      "epoch": 0.6015625,
      "grad_norm": 7.165276449631838,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 539
    },
    {
      "epoch": 0.6026785714285714,
      "grad_norm": 7.737082160252734,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 540
    },
    {
      "epoch": 0.6037946428571429,
      "grad_norm": 6.885279340195057,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 541
    },
    {
      "epoch": 0.6049107142857143,
      "grad_norm": 7.257508557159585,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 542
    },
    {
      "epoch": 0.6060267857142857,
      "grad_norm": 7.391386065709816,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 543
    },
    {
      "epoch": 0.6071428571428571,
      "grad_norm": 7.137488746959001,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 544
    },
    {
      "epoch": 0.6082589285714286,
      "grad_norm": 7.748661890383331,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 545
    },
    {
      "epoch": 0.609375,
      "grad_norm": 6.582130996381901,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 546
    },
    {
      "epoch": 0.6104910714285714,
      "grad_norm": 7.0629110368915695,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 547
    },
    {
      "epoch": 0.6116071428571429,
      "grad_norm": 6.734276089021543,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 548
    },
    {
      "epoch": 0.6127232142857143,
      "grad_norm": 5.535353287322365,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 549
    },
    {
      "epoch": 0.6138392857142857,
      "grad_norm": 6.178656547762734,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 550
    },
    {
      "epoch": 0.6149553571428571,
      "grad_norm": 7.039181863661586,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 551
    },
    {
      "epoch": 0.6160714285714286,
      "grad_norm": 7.165958925195559,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 552
    },
    {
      "epoch": 0.6171875,
      "grad_norm": 7.368164764534582,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 553
    },
    {
      "epoch": 0.6183035714285714,
      "grad_norm": 8.243520899076342,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 554
    },
    {
      "epoch": 0.6194196428571429,
      "grad_norm": 7.486382789499762,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 555
    },
    {
      "epoch": 0.6205357142857143,
      "grad_norm": 6.640119412702982,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 556
    },
    {
      "epoch": 0.6216517857142857,
      "grad_norm": 7.786718397435241,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 557
    },
    {
      "epoch": 0.6227678571428571,
      "grad_norm": 8.481859862736677,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 558
    },
    {
      "epoch": 0.6238839285714286,
      "grad_norm": 7.27861097670762,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 559
    },
    {
      "epoch": 0.625,
      "grad_norm": 7.574828353868911,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 560
    },
    {
      "epoch": 0.6261160714285714,
      "grad_norm": 6.6340484097224355,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 561
    },
    {
      "epoch": 0.6272321428571429,
      "grad_norm": 6.500167685041858,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 562
    },
    {
      "epoch": 0.6283482142857143,
      "grad_norm": 6.1237783123238545,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 563
    },
    {
      "epoch": 0.6294642857142857,
      "grad_norm": 5.12880085763775,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 564
    },
    {
      "epoch": 0.6305803571428571,
      "grad_norm": 8.077846784437718,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 565
    },
    {
      "epoch": 0.6316964285714286,
      "grad_norm": 7.850332157185584,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 566
    },
    {
      "epoch": 0.6328125,
      "grad_norm": 6.525401122005646,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 567
    },
    {
      "epoch": 0.6339285714285714,
      "grad_norm": 6.828137853245754,
      "learning_rate": 2e-05,
      "loss": 3.1094,
      "step": 568
    },
    {
      "epoch": 0.6350446428571429,
      "grad_norm": 6.933946469483155,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 569
    },
    {
      "epoch": 0.6361607142857143,
      "grad_norm": 6.4225118553788745,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 570
    },
    {
      "epoch": 0.6372767857142857,
      "grad_norm": 9.055079622552078,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 571
    },
    {
      "epoch": 0.6383928571428571,
      "grad_norm": 6.9392919932494745,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 572
    },
    {
      "epoch": 0.6395089285714286,
      "grad_norm": 6.714547797727804,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 573
    },
    {
      "epoch": 0.640625,
      "grad_norm": 7.462917953744773,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 574
    },
    {
      "epoch": 0.6417410714285714,
      "grad_norm": 6.019678968935781,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 575
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 5.319139837355859,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 576
    },
    {
      "epoch": 0.6439732142857143,
      "grad_norm": 6.890259971829131,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 577
    },
    {
      "epoch": 0.6450892857142857,
      "grad_norm": 6.343981202862711,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 578
    },
    {
      "epoch": 0.6462053571428571,
      "grad_norm": 6.687299925702872,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 579
    },
    {
      "epoch": 0.6473214285714286,
      "grad_norm": 6.654496776350769,
      "learning_rate": 2e-05,
      "loss": 3.0938,
      "step": 580
    },
    {
      "epoch": 0.6484375,
      "grad_norm": 7.05216497002948,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 581
    },
    {
      "epoch": 0.6495535714285714,
      "grad_norm": 7.185968454340757,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 582
    },
    {
      "epoch": 0.6506696428571429,
      "grad_norm": 6.925185750341311,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 583
    },
    {
      "epoch": 0.6517857142857143,
      "grad_norm": 7.231674884097017,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 584
    },
    {
      "epoch": 0.6529017857142857,
      "grad_norm": 6.345907813514099,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 585
    },
    {
      "epoch": 0.6540178571428571,
      "grad_norm": 7.886918775010117,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 586
    },
    {
      "epoch": 0.6551339285714286,
      "grad_norm": 6.24959111035519,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 587
    },
    {
      "epoch": 0.65625,
      "grad_norm": 6.796481894674375,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 588
    },
    {
      "epoch": 0.6573660714285714,
      "grad_norm": 7.819064229184331,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 589
    },
    {
      "epoch": 0.6584821428571429,
      "grad_norm": 5.8413055916330165,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 590
    },
    {
      "epoch": 0.6595982142857143,
      "grad_norm": 8.268594819742033,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 591
    },
    {
      "epoch": 0.6607142857142857,
      "grad_norm": 7.8517082476443445,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 592
    },
    {
      "epoch": 0.6618303571428571,
      "grad_norm": 7.3243181307788925,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 593
    },
    {
      "epoch": 0.6629464285714286,
      "grad_norm": 7.396669674394314,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 594
    },
    {
      "epoch": 0.6640625,
      "grad_norm": 6.486828848442188,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 595
    },
    {
      "epoch": 0.6651785714285714,
      "grad_norm": 9.034632567352785,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 596
    },
    {
      "epoch": 0.6662946428571429,
      "grad_norm": 8.297827583814112,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 597
    },
    {
      "epoch": 0.6674107142857143,
      "grad_norm": 8.69103226112985,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 598
    },
    {
      "epoch": 0.6685267857142857,
      "grad_norm": 7.847905503198117,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 599
    },
    {
      "epoch": 0.6696428571428571,
      "grad_norm": 11.909023856182442,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 600
    },
    {
      "epoch": 0.6707589285714286,
      "grad_norm": 8.049127314411349,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 601
    },
    {
      "epoch": 0.671875,
      "grad_norm": 6.093627224198067,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 602
    },
    {
      "epoch": 0.6729910714285714,
      "grad_norm": 7.460176050458266,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 603
    },
    {
      "epoch": 0.6741071428571429,
      "grad_norm": 8.680047266521463,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 604
    },
    {
      "epoch": 0.6752232142857143,
      "grad_norm": 6.449522781940608,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 605
    },
    {
      "epoch": 0.6763392857142857,
      "grad_norm": 6.6233250634896566,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 606
    },
    {
      "epoch": 0.6774553571428571,
      "grad_norm": 7.827692141312697,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 607
    },
    {
      "epoch": 0.6785714285714286,
      "grad_norm": 13.597371567260781,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 608
    },
    {
      "epoch": 0.6796875,
      "grad_norm": 7.578447552180426,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 609
    },
    {
      "epoch": 0.6808035714285714,
      "grad_norm": 8.338432520821854,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 610
    },
    {
      "epoch": 0.6819196428571429,
      "grad_norm": 7.317481846904897,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 611
    },
    {
      "epoch": 0.6830357142857143,
      "grad_norm": 7.362421322426254,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 612
    },
    {
      "epoch": 0.6841517857142857,
      "grad_norm": 5.206645099891276,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 613
    },
    {
      "epoch": 0.6852678571428571,
      "grad_norm": 6.530204837885705,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 614
    },
    {
      "epoch": 0.6863839285714286,
      "grad_norm": 8.563800334057776,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 615
    },
    {
      "epoch": 0.6875,
      "grad_norm": 6.337579807856006,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 616
    },
    {
      "epoch": 0.6886160714285714,
      "grad_norm": 7.741881110306949,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 617
    },
    {
      "epoch": 0.6897321428571429,
      "grad_norm": 7.215246397096221,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 618
    },
    {
      "epoch": 0.6908482142857143,
      "grad_norm": 5.124279706126531,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 619
    },
    {
      "epoch": 0.6919642857142857,
      "grad_norm": 8.702251162301915,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 620
    },
    {
      "epoch": 0.6930803571428571,
      "grad_norm": 8.414264255254512,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 621
    },
    {
      "epoch": 0.6941964285714286,
      "grad_norm": 6.884341711234197,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 622
    },
    {
      "epoch": 0.6953125,
      "grad_norm": 7.441513791789359,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 623
    },
    {
      "epoch": 0.6964285714285714,
      "grad_norm": 7.0293900375822265,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 624
    },
    {
      "epoch": 0.6975446428571429,
      "grad_norm": 7.239744578628313,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 625
    },
    {
      "epoch": 0.6986607142857143,
      "grad_norm": 8.200515413058048,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 626
    },
    {
      "epoch": 0.6997767857142857,
      "grad_norm": 7.54549419799597,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 627
    },
    {
      "epoch": 0.7008928571428571,
      "grad_norm": 7.369938171985584,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 628
    },
    {
      "epoch": 0.7020089285714286,
      "grad_norm": 7.692149336453549,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 629
    },
    {
      "epoch": 0.703125,
      "grad_norm": 8.114366300501242,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 630
    },
    {
      "epoch": 0.7042410714285714,
      "grad_norm": 6.909344860456727,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 631
    },
    {
      "epoch": 0.7053571428571429,
      "grad_norm": 6.626275043118414,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 632
    },
    {
      "epoch": 0.7064732142857143,
      "grad_norm": 9.22769722379324,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 633
    },
    {
      "epoch": 0.7075892857142857,
      "grad_norm": 7.802051115433907,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 634
    },
    {
      "epoch": 0.7087053571428571,
      "grad_norm": 8.944644090925104,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 635
    },
    {
      "epoch": 0.7098214285714286,
      "grad_norm": 6.92652515600993,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 636
    },
    {
      "epoch": 0.7109375,
      "grad_norm": 6.812532015268174,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 637
    },
    {
      "epoch": 0.7120535714285714,
      "grad_norm": 6.894567142747653,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 638
    },
    {
      "epoch": 0.7131696428571429,
      "grad_norm": 8.09618501611102,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 639
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 8.48950473722746,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 640
    },
    {
      "epoch": 0.7154017857142857,
      "grad_norm": 6.859688482929468,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 641
    },
    {
      "epoch": 0.7165178571428571,
      "grad_norm": 8.190078940248425,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 642
    },
    {
      "epoch": 0.7176339285714286,
      "grad_norm": 6.43415608795316,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 643
    },
    {
      "epoch": 0.71875,
      "grad_norm": 7.264083390180525,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 644
    },
    {
      "epoch": 0.7198660714285714,
      "grad_norm": 6.197882300103818,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 645
    },
    {
      "epoch": 0.7209821428571429,
      "grad_norm": 8.66468277840732,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 646
    },
    {
      "epoch": 0.7220982142857143,
      "grad_norm": 8.189051036002342,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 647
    },
    {
      "epoch": 0.7232142857142857,
      "grad_norm": 5.863270120115275,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 648
    },
    {
      "epoch": 0.7243303571428571,
      "grad_norm": 7.5682111696073076,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 649
    },
    {
      "epoch": 0.7254464285714286,
      "grad_norm": 6.75854755331268,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 650
    },
    {
      "epoch": 0.7265625,
      "grad_norm": 6.97848641724375,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 651
    },
    {
      "epoch": 0.7276785714285714,
      "grad_norm": 6.518540836545243,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 652
    },
    {
      "epoch": 0.7287946428571429,
      "grad_norm": 8.39757695297506,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 653
    },
    {
      "epoch": 0.7299107142857143,
      "grad_norm": 7.002154417262316,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 654
    },
    {
      "epoch": 0.7310267857142857,
      "grad_norm": 7.652785148280761,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 655
    },
    {
      "epoch": 0.7321428571428571,
      "grad_norm": 7.58861480616225,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 656
    },
    {
      "epoch": 0.7332589285714286,
      "grad_norm": 7.014376724110922,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 657
    },
    {
      "epoch": 0.734375,
      "grad_norm": 6.139948883690198,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 658
    },
    {
      "epoch": 0.7354910714285714,
      "grad_norm": 7.439759946247817,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 659
    },
    {
      "epoch": 0.7366071428571429,
      "grad_norm": 6.8249113237600145,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 660
    },
    {
      "epoch": 0.7377232142857143,
      "grad_norm": 6.914146794957794,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 661
    },
    {
      "epoch": 0.7388392857142857,
      "grad_norm": 6.837275664608894,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 662
    },
    {
      "epoch": 0.7399553571428571,
      "grad_norm": 7.598402800495714,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 663
    },
    {
      "epoch": 0.7410714285714286,
      "grad_norm": 6.403469630272328,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 664
    },
    {
      "epoch": 0.7421875,
      "grad_norm": 6.00450945960009,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 665
    },
    {
      "epoch": 0.7433035714285714,
      "grad_norm": 8.037821345312222,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 666
    },
    {
      "epoch": 0.7444196428571429,
      "grad_norm": 6.6797290359445745,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 667
    },
    {
      "epoch": 0.7455357142857143,
      "grad_norm": 7.009173429277118,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 668
    },
    {
      "epoch": 0.7466517857142857,
      "grad_norm": 6.524823752456167,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 669
    },
    {
      "epoch": 0.7477678571428571,
      "grad_norm": 7.637059834023824,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 670
    },
    {
      "epoch": 0.7488839285714286,
      "grad_norm": 8.49148476677469,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 671
    },
    {
      "epoch": 0.75,
      "grad_norm": 8.338162586372338,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 672
    },
    {
      "epoch": 0.7511160714285714,
      "grad_norm": 5.69856314965629,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 673
    },
    {
      "epoch": 0.7522321428571429,
      "grad_norm": 7.631287716869639,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 674
    },
    {
      "epoch": 0.7533482142857143,
      "grad_norm": 6.987960668144088,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 675
    },
    {
      "epoch": 0.7544642857142857,
      "grad_norm": 6.487843927051443,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 676
    },
    {
      "epoch": 0.7555803571428571,
      "grad_norm": 5.618709956810665,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 677
    },
    {
      "epoch": 0.7566964285714286,
      "grad_norm": 7.430354842673078,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 678
    },
    {
      "epoch": 0.7578125,
      "grad_norm": 6.805536374070492,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 679
    },
    {
      "epoch": 0.7589285714285714,
      "grad_norm": 7.402628875469795,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 680
    },
    {
      "epoch": 0.7600446428571429,
      "grad_norm": 6.777933446889452,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 681
    },
    {
      "epoch": 0.7611607142857143,
      "grad_norm": 8.987087718675813,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 682
    },
    {
      "epoch": 0.7622767857142857,
      "grad_norm": 6.735449008398507,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 683
    },
    {
      "epoch": 0.7633928571428571,
      "grad_norm": 6.117891642385611,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 684
    },
    {
      "epoch": 0.7645089285714286,
      "grad_norm": 7.577422617049957,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 685
    },
    {
      "epoch": 0.765625,
      "grad_norm": 7.131983366478902,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 686
    },
    {
      "epoch": 0.7667410714285714,
      "grad_norm": 7.173704669490769,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 687
    },
    {
      "epoch": 0.7678571428571429,
      "grad_norm": 6.833493701191782,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 688
    },
    {
      "epoch": 0.7689732142857143,
      "grad_norm": 6.862652266855852,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 689
    },
    {
      "epoch": 0.7700892857142857,
      "grad_norm": 7.009071164719957,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 690
    },
    {
      "epoch": 0.7712053571428571,
      "grad_norm": 7.4444652975568255,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 691
    },
    {
      "epoch": 0.7723214285714286,
      "grad_norm": 7.071156172563195,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 692
    },
    {
      "epoch": 0.7734375,
      "grad_norm": 9.076918207109802,
      "learning_rate": 2e-05,
      "loss": 1.5312,
      "step": 693
    },
    {
      "epoch": 0.7745535714285714,
      "grad_norm": 9.049819640386074,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 694
    },
    {
      "epoch": 0.7756696428571429,
      "grad_norm": 7.700182006709742,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 695
    },
    {
      "epoch": 0.7767857142857143,
      "grad_norm": 8.244202926480916,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 696
    },
    {
      "epoch": 0.7779017857142857,
      "grad_norm": 8.011975509597269,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 697
    },
    {
      "epoch": 0.7790178571428571,
      "grad_norm": 8.235422546185953,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 698
    },
    {
      "epoch": 0.7801339285714286,
      "grad_norm": 8.237198966702296,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 699
    },
    {
      "epoch": 0.78125,
      "grad_norm": 6.958088283859897,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 700
    },
    {
      "epoch": 0.7823660714285714,
      "grad_norm": 8.080095292496676,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 701
    },
    {
      "epoch": 0.7834821428571429,
      "grad_norm": 8.613320075329357,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 702
    },
    {
      "epoch": 0.7845982142857143,
      "grad_norm": 7.267569669648207,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 703
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 7.163436934525759,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 704
    },
    {
      "epoch": 0.7868303571428571,
      "grad_norm": 7.5081256418136455,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 705
    },
    {
      "epoch": 0.7879464285714286,
      "grad_norm": 6.398911623566062,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 706
    },
    {
      "epoch": 0.7890625,
      "grad_norm": 7.357886595221823,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 707
    },
    {
      "epoch": 0.7901785714285714,
      "grad_norm": 8.742769070315433,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 708
    },
    {
      "epoch": 0.7912946428571429,
      "grad_norm": 7.223230135475636,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 709
    },
    {
      "epoch": 0.7924107142857143,
      "grad_norm": 7.385531215376964,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 710
    },
    {
      "epoch": 0.7935267857142857,
      "grad_norm": 8.494435773668522,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 711
    },
    {
      "epoch": 0.7946428571428571,
      "grad_norm": 7.555709835403667,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 712
    },
    {
      "epoch": 0.7957589285714286,
      "grad_norm": 7.115783696917038,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 713
    },
    {
      "epoch": 0.796875,
      "grad_norm": 8.179043577779913,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 714
    },
    {
      "epoch": 0.7979910714285714,
      "grad_norm": 7.9204184658056755,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 715
    },
    {
      "epoch": 0.7991071428571429,
      "grad_norm": 6.376362290456513,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 716
    },
    {
      "epoch": 0.8002232142857143,
      "grad_norm": 8.444289579939406,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 717
    },
    {
      "epoch": 0.8013392857142857,
      "grad_norm": 6.625989986426366,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 718
    },
    {
      "epoch": 0.8024553571428571,
      "grad_norm": 8.236021997988285,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 719
    },
    {
      "epoch": 0.8035714285714286,
      "grad_norm": 8.302409271271902,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 720
    },
    {
      "epoch": 0.8046875,
      "grad_norm": 9.737315346326259,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 721
    },
    {
      "epoch": 0.8058035714285714,
      "grad_norm": 8.886967551231395,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 722
    },
    {
      "epoch": 0.8069196428571429,
      "grad_norm": 7.21556747069122,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 723
    },
    {
      "epoch": 0.8080357142857143,
      "grad_norm": 7.942171871566063,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 724
    },
    {
      "epoch": 0.8091517857142857,
      "grad_norm": 7.627586842265554,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 725
    },
    {
      "epoch": 0.8102678571428571,
      "grad_norm": 7.349261648822988,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 726
    },
    {
      "epoch": 0.8113839285714286,
      "grad_norm": 8.407068878246223,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 727
    },
    {
      "epoch": 0.8125,
      "grad_norm": 7.318734018745202,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 728
    },
    {
      "epoch": 0.8136160714285714,
      "grad_norm": 6.396774982844105,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 729
    },
    {
      "epoch": 0.8147321428571429,
      "grad_norm": 7.660688635890924,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 730
    },
    {
      "epoch": 0.8158482142857143,
      "grad_norm": 7.187627556889666,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 731
    },
    {
      "epoch": 0.8169642857142857,
      "grad_norm": 5.9968303588111675,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 732
    },
    {
      "epoch": 0.8180803571428571,
      "grad_norm": 8.132129445268596,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 733
    },
    {
      "epoch": 0.8191964285714286,
      "grad_norm": 7.133863199586631,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 734
    },
    {
      "epoch": 0.8203125,
      "grad_norm": 7.109822789443284,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 735
    },
    {
      "epoch": 0.8214285714285714,
      "grad_norm": 8.63670134240111,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 736
    },
    {
      "epoch": 0.8225446428571429,
      "grad_norm": 7.3113072525929725,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 737
    },
    {
      "epoch": 0.8236607142857143,
      "grad_norm": 9.300369055243136,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 738
    },
    {
      "epoch": 0.8247767857142857,
      "grad_norm": 7.085335281545922,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 739
    },
    {
      "epoch": 0.8258928571428571,
      "grad_norm": 10.069946620389745,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 740
    },
    {
      "epoch": 0.8270089285714286,
      "grad_norm": 6.852805442574333,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 741
    },
    {
      "epoch": 0.828125,
      "grad_norm": 7.302169996038232,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 742
    },
    {
      "epoch": 0.8292410714285714,
      "grad_norm": 9.255867150465374,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 743
    },
    {
      "epoch": 0.8303571428571429,
      "grad_norm": 7.4413950850050306,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 744
    },
    {
      "epoch": 0.8314732142857143,
      "grad_norm": 7.7385358273103595,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 745
    },
    {
      "epoch": 0.8325892857142857,
      "grad_norm": 7.340417180177864,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 746
    },
    {
      "epoch": 0.8337053571428571,
      "grad_norm": 5.679604687666759,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 747
    },
    {
      "epoch": 0.8348214285714286,
      "grad_norm": 7.481594455169163,
      "learning_rate": 2e-05,
      "loss": 1.4453,
      "step": 748
    },
    {
      "epoch": 0.8359375,
      "grad_norm": 9.303365562523432,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 749
    },
    {
      "epoch": 0.8370535714285714,
      "grad_norm": 6.869555334216046,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 750
    },
    {
      "epoch": 0.8381696428571429,
      "grad_norm": 7.425644984434222,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 751
    },
    {
      "epoch": 0.8392857142857143,
      "grad_norm": 8.649820336507238,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 752
    },
    {
      "epoch": 0.8404017857142857,
      "grad_norm": 5.289604832401025,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 753
    },
    {
      "epoch": 0.8415178571428571,
      "grad_norm": 7.277697761309936,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 754
    },
    {
      "epoch": 0.8426339285714286,
      "grad_norm": 10.090745597411415,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 755
    },
    {
      "epoch": 0.84375,
      "grad_norm": 6.468005829214493,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 756
    },
    {
      "epoch": 0.8448660714285714,
      "grad_norm": 6.035282974460319,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 757
    },
    {
      "epoch": 0.8459821428571429,
      "grad_norm": 5.581858398047519,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 758
    },
    {
      "epoch": 0.8470982142857143,
      "grad_norm": 7.046060932972032,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 759
    },
    {
      "epoch": 0.8482142857142857,
      "grad_norm": 6.707705784166958,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 760
    },
    {
      "epoch": 0.8493303571428571,
      "grad_norm": 7.1766893492439525,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 761
    },
    {
      "epoch": 0.8504464285714286,
      "grad_norm": 7.933267883547185,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 762
    },
    {
      "epoch": 0.8515625,
      "grad_norm": 7.499401054942811,
      "learning_rate": 2e-05,
      "loss": 3.0,
      "step": 763
    },
    {
      "epoch": 0.8526785714285714,
      "grad_norm": 8.124846669315502,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 764
    },
    {
      "epoch": 0.8537946428571429,
      "grad_norm": 7.454956785124179,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 765
    },
    {
      "epoch": 0.8549107142857143,
      "grad_norm": 7.7574843361412835,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 766
    },
    {
      "epoch": 0.8560267857142857,
      "grad_norm": 6.671722876057147,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 767
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 9.982878960914976,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 768
    },
    {
      "epoch": 0.8582589285714286,
      "grad_norm": 7.906663708530348,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 769
    },
    {
      "epoch": 0.859375,
      "grad_norm": 5.388841252551708,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 770
    },
    {
      "epoch": 0.8604910714285714,
      "grad_norm": 5.778472453045861,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 771
    },
    {
      "epoch": 0.8616071428571429,
      "grad_norm": 7.582251002867786,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 772
    },
    {
      "epoch": 0.8627232142857143,
      "grad_norm": 8.006312658964319,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 773
    },
    {
      "epoch": 0.8638392857142857,
      "grad_norm": 7.895503372255256,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 774
    },
    {
      "epoch": 0.8649553571428571,
      "grad_norm": 7.679708314806106,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 775
    },
    {
      "epoch": 0.8660714285714286,
      "grad_norm": 7.623966888936106,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 776
    },
    {
      "epoch": 0.8671875,
      "grad_norm": 8.569040070861268,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 777
    },
    {
      "epoch": 0.8683035714285714,
      "grad_norm": 7.790891892802453,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 778
    },
    {
      "epoch": 0.8694196428571429,
      "grad_norm": 7.6613463134530235,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 779
    },
    {
      "epoch": 0.8705357142857143,
      "grad_norm": 7.0592210182544415,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 780
    },
    {
      "epoch": 0.8716517857142857,
      "grad_norm": 7.529115065034707,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 781
    },
    {
      "epoch": 0.8727678571428571,
      "grad_norm": 7.424860194958381,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 782
    },
    {
      "epoch": 0.8738839285714286,
      "grad_norm": 7.626131493352132,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 783
    },
    {
      "epoch": 0.875,
      "grad_norm": 7.876400909117194,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 784
    },
    {
      "epoch": 0.8761160714285714,
      "grad_norm": 7.870317858347874,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 785
    },
    {
      "epoch": 0.8772321428571429,
      "grad_norm": 6.9428170879681455,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 786
    },
    {
      "epoch": 0.8783482142857143,
      "grad_norm": 6.926305000659284,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 787
    },
    {
      "epoch": 0.8794642857142857,
      "grad_norm": 7.90865231714586,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 788
    },
    {
      "epoch": 0.8805803571428571,
      "grad_norm": 8.089814304439924,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 789
    },
    {
      "epoch": 0.8816964285714286,
      "grad_norm": 6.521415643620119,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 790
    },
    {
      "epoch": 0.8828125,
      "grad_norm": 9.17658570222275,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 791
    },
    {
      "epoch": 0.8839285714285714,
      "grad_norm": 7.129909145799193,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 792
    },
    {
      "epoch": 0.8850446428571429,
      "grad_norm": 6.6225561686866365,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 793
    },
    {
      "epoch": 0.8861607142857143,
      "grad_norm": 10.621948008079693,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 794
    },
    {
      "epoch": 0.8872767857142857,
      "grad_norm": 7.709827609705132,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 795
    },
    {
      "epoch": 0.8883928571428571,
      "grad_norm": 4.4416294940118295,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 796
    },
    {
      "epoch": 0.8895089285714286,
      "grad_norm": 8.362173413105783,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 797
    },
    {
      "epoch": 0.890625,
      "grad_norm": 5.68851529600558,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 798
    },
    {
      "epoch": 0.8917410714285714,
      "grad_norm": 8.356346198763275,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 799
    },
    {
      "epoch": 0.8928571428571429,
      "grad_norm": 7.762687151391418,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 800
    },
    {
      "epoch": 0.8939732142857143,
      "grad_norm": 6.5805818197757375,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 801
    },
    {
      "epoch": 0.8950892857142857,
      "grad_norm": 8.25122791610741,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 802
    },
    {
      "epoch": 0.8962053571428571,
      "grad_norm": 7.466197672511711,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 803
    },
    {
      "epoch": 0.8973214285714286,
      "grad_norm": 6.966245302351483,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 804
    },
    {
      "epoch": 0.8984375,
      "grad_norm": 6.7751623023385426,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 805
    },
    {
      "epoch": 0.8995535714285714,
      "grad_norm": 11.1491256155419,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 806
    },
    {
      "epoch": 0.9006696428571429,
      "grad_norm": 9.214656191446174,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 807
    },
    {
      "epoch": 0.9017857142857143,
      "grad_norm": 7.3660538769824395,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 808
    },
    {
      "epoch": 0.9029017857142857,
      "grad_norm": 7.429777127164266,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 809
    },
    {
      "epoch": 0.9040178571428571,
      "grad_norm": 6.883428898836841,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 810
    },
    {
      "epoch": 0.9051339285714286,
      "grad_norm": 6.320497591879356,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 811
    },
    {
      "epoch": 0.90625,
      "grad_norm": 8.43694707550417,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 812
    },
    {
      "epoch": 0.9073660714285714,
      "grad_norm": 7.132273889261657,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 813
    },
    {
      "epoch": 0.9084821428571429,
      "grad_norm": 7.385123139681288,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 814
    },
    {
      "epoch": 0.9095982142857143,
      "grad_norm": 8.061602711766193,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 815
    },
    {
      "epoch": 0.9107142857142857,
      "grad_norm": 6.572520918758388,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 816
    },
    {
      "epoch": 0.9118303571428571,
      "grad_norm": 6.915982889844661,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 817
    },
    {
      "epoch": 0.9129464285714286,
      "grad_norm": 7.724134960053868,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 818
    },
    {
      "epoch": 0.9140625,
      "grad_norm": 7.907718137289556,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 819
    },
    {
      "epoch": 0.9151785714285714,
      "grad_norm": 7.770451769764148,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 820
    },
    {
      "epoch": 0.9162946428571429,
      "grad_norm": 7.269102966777141,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 821
    },
    {
      "epoch": 0.9174107142857143,
      "grad_norm": 9.66235770318758,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 822
    },
    {
      "epoch": 0.9185267857142857,
      "grad_norm": 7.629217227520317,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 823
    },
    {
      "epoch": 0.9196428571428571,
      "grad_norm": 8.452368469098834,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 824
    },
    {
      "epoch": 0.9207589285714286,
      "grad_norm": 7.57519682426313,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 825
    },
    {
      "epoch": 0.921875,
      "grad_norm": 6.764097362158105,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 826
    },
    {
      "epoch": 0.9229910714285714,
      "grad_norm": 7.62189770198356,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 827
    },
    {
      "epoch": 0.9241071428571429,
      "grad_norm": 6.9485382094004615,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 828
    },
    {
      "epoch": 0.9252232142857143,
      "grad_norm": 7.708413330777987,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 829
    },
    {
      "epoch": 0.9263392857142857,
      "grad_norm": 8.255494452092904,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 830
    },
    {
      "epoch": 0.9274553571428571,
      "grad_norm": 6.691665341841153,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 831
    },
    {
      "epoch": 0.9285714285714286,
      "grad_norm": 8.439901138952038,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 832
    },
    {
      "epoch": 0.9296875,
      "grad_norm": 7.505067871747109,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 833
    },
    {
      "epoch": 0.9308035714285714,
      "grad_norm": 7.957964788608105,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 834
    },
    {
      "epoch": 0.9319196428571429,
      "grad_norm": 7.097183084473005,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 835
    },
    {
      "epoch": 0.9330357142857143,
      "grad_norm": 8.09717670978944,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 836
    },
    {
      "epoch": 0.9341517857142857,
      "grad_norm": 8.369563410119511,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 837
    },
    {
      "epoch": 0.9352678571428571,
      "grad_norm": 6.79453391453038,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 838
    },
    {
      "epoch": 0.9363839285714286,
      "grad_norm": 7.179151748633432,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 839
    },
    {
      "epoch": 0.9375,
      "grad_norm": 11.49101770775062,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 840
    },
    {
      "epoch": 0.9386160714285714,
      "grad_norm": 7.6322646452696254,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 841
    },
    {
      "epoch": 0.9397321428571429,
      "grad_norm": 8.569679600866866,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 842
    },
    {
      "epoch": 0.9408482142857143,
      "grad_norm": 7.924316767148923,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 843
    },
    {
      "epoch": 0.9419642857142857,
      "grad_norm": 8.190955496226874,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 844
    },
    {
      "epoch": 0.9430803571428571,
      "grad_norm": 9.056915547089309,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 845
    },
    {
      "epoch": 0.9441964285714286,
      "grad_norm": 6.240819633154437,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 846
    },
    {
      "epoch": 0.9453125,
      "grad_norm": 9.2630281755415,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 847
    },
    {
      "epoch": 0.9464285714285714,
      "grad_norm": 7.93785259584136,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 848
    },
    {
      "epoch": 0.9475446428571429,
      "grad_norm": 7.703943005271166,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 849
    },
    {
      "epoch": 0.9486607142857143,
      "grad_norm": 7.709231143205931,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 850
    },
    {
      "epoch": 0.9497767857142857,
      "grad_norm": 5.991929236206334,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 851
    },
    {
      "epoch": 0.9508928571428571,
      "grad_norm": 7.300431800124602,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 852
    },
    {
      "epoch": 0.9520089285714286,
      "grad_norm": 7.359055059295672,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 853
    },
    {
      "epoch": 0.953125,
      "grad_norm": 9.401226808903882,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 854
    },
    {
      "epoch": 0.9542410714285714,
      "grad_norm": 8.234120994876932,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 855
    },
    {
      "epoch": 0.9553571428571429,
      "grad_norm": 7.652783252392185,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 856
    },
    {
      "epoch": 0.9564732142857143,
      "grad_norm": 8.387724213253467,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 857
    },
    {
      "epoch": 0.9575892857142857,
      "grad_norm": 8.80450130650201,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 858
    },
    {
      "epoch": 0.9587053571428571,
      "grad_norm": 7.2414223383416765,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 859
    },
    {
      "epoch": 0.9598214285714286,
      "grad_norm": 6.277292458914491,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 860
    },
    {
      "epoch": 0.9609375,
      "grad_norm": 8.425193085914424,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 861
    },
    {
      "epoch": 0.9620535714285714,
      "grad_norm": 8.828768431233788,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 862
    },
    {
      "epoch": 0.9631696428571429,
      "grad_norm": 5.664319232822921,
      "learning_rate": 2e-05,
      "loss": 2.8281,
      "step": 863
    },
    {
      "epoch": 0.9642857142857143,
      "grad_norm": 7.196473451673784,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 864
    },
    {
      "epoch": 0.9654017857142857,
      "grad_norm": 9.109002661960586,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 865
    },
    {
      "epoch": 0.9665178571428571,
      "grad_norm": 7.3153509671100325,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 866
    },
    {
      "epoch": 0.9676339285714286,
      "grad_norm": 7.746957519809649,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 867
    },
    {
      "epoch": 0.96875,
      "grad_norm": 8.71259281907862,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 868
    },
    {
      "epoch": 0.9698660714285714,
      "grad_norm": 8.406368992644454,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 869
    },
    {
      "epoch": 0.9709821428571429,
      "grad_norm": 7.8730859516977505,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 870
    },
    {
      "epoch": 0.9720982142857143,
      "grad_norm": 7.363588014310592,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 871
    },
    {
      "epoch": 0.9732142857142857,
      "grad_norm": 7.384399565939538,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 872
    },
    {
      "epoch": 0.9743303571428571,
      "grad_norm": 6.48682564127364,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 873
    },
    {
      "epoch": 0.9754464285714286,
      "grad_norm": 8.175047863768238,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 874
    },
    {
      "epoch": 0.9765625,
      "grad_norm": 9.490286332023473,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 875
    },
    {
      "epoch": 0.9776785714285714,
      "grad_norm": 8.296203084604528,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 876
    },
    {
      "epoch": 0.9787946428571429,
      "grad_norm": 7.562503388027406,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 877
    },
    {
      "epoch": 0.9799107142857143,
      "grad_norm": 7.363696290211345,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 878
    },
    {
      "epoch": 0.9810267857142857,
      "grad_norm": 8.548006655887228,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 879
    },
    {
      "epoch": 0.9821428571428571,
      "grad_norm": 7.3071845684102605,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 880
    },
    {
      "epoch": 0.9832589285714286,
      "grad_norm": 7.967861274278176,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 881
    },
    {
      "epoch": 0.984375,
      "grad_norm": 8.45353461545602,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 882
    },
    {
      "epoch": 0.9854910714285714,
      "grad_norm": 7.576104875232474,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 883
    },
    {
      "epoch": 0.9866071428571429,
      "grad_norm": 8.840917981292552,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 884
    },
    {
      "epoch": 0.9877232142857143,
      "grad_norm": 7.028927444049435,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 885
    },
    {
      "epoch": 0.9888392857142857,
      "grad_norm": 8.935505644650485,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 886
    },
    {
      "epoch": 0.9899553571428571,
      "grad_norm": 6.885889557990625,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 887
    },
    {
      "epoch": 0.9910714285714286,
      "grad_norm": 7.473054535197915,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 888
    },
    {
      "epoch": 0.9921875,
      "grad_norm": 7.659368125704597,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 889
    },
    {
      "epoch": 0.9933035714285714,
      "grad_norm": 7.765536772815751,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 890
    },
    {
      "epoch": 0.9944196428571429,
      "grad_norm": 6.4654037182918564,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 891
    },
    {
      "epoch": 0.9955357142857143,
      "grad_norm": 8.559616449987061,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 892
    },
    {
      "epoch": 0.9966517857142857,
      "grad_norm": 6.7653584756777745,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 893
    },
    {
      "epoch": 0.9977678571428571,
      "grad_norm": 9.255095539737054,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 894
    },
    {
      "epoch": 0.9988839285714286,
      "grad_norm": 9.094258407653669,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 895
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.574840564248845,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 896
    },
    {
      "epoch": 1.0011160714285714,
      "grad_norm": 7.836884517044679,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 897
    },
    {
      "epoch": 1.0022321428571428,
      "grad_norm": 10.36284303856991,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 898
    },
    {
      "epoch": 1.0033482142857142,
      "grad_norm": 7.736698729434541,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 899
    },
    {
      "epoch": 1.0044642857142858,
      "grad_norm": 7.897631851874295,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 900
    },
    {
      "epoch": 1.0055803571428572,
      "grad_norm": 8.791893446111258,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 901
    },
    {
      "epoch": 1.0066964285714286,
      "grad_norm": 5.350713395040623,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 902
    },
    {
      "epoch": 1.0078125,
      "grad_norm": 7.75551036622426,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 903
    },
    {
      "epoch": 1.0089285714285714,
      "grad_norm": 6.11607642089473,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 904
    },
    {
      "epoch": 1.0100446428571428,
      "grad_norm": 5.801975086385737,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 905
    },
    {
      "epoch": 1.0111607142857142,
      "grad_norm": 4.970167692719291,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 906
    },
    {
      "epoch": 1.0122767857142858,
      "grad_norm": 6.951520488932862,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 907
    },
    {
      "epoch": 1.0133928571428572,
      "grad_norm": 6.674878701797229,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 908
    },
    {
      "epoch": 1.0145089285714286,
      "grad_norm": 6.5995679214515235,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 909
    },
    {
      "epoch": 1.015625,
      "grad_norm": 7.122932071524123,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 910
    },
    {
      "epoch": 1.0167410714285714,
      "grad_norm": 7.287356044674242,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 911
    },
    {
      "epoch": 1.0178571428571428,
      "grad_norm": 8.164035525736338,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 912
    },
    {
      "epoch": 1.0189732142857142,
      "grad_norm": 8.062432584109649,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 913
    },
    {
      "epoch": 1.0200892857142858,
      "grad_norm": 6.797912039392409,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 914
    },
    {
      "epoch": 1.0212053571428572,
      "grad_norm": 7.111123497908928,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 915
    },
    {
      "epoch": 1.0223214285714286,
      "grad_norm": 7.581576086747828,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 916
    },
    {
      "epoch": 1.0234375,
      "grad_norm": 7.424266571047113,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 917
    },
    {
      "epoch": 1.0245535714285714,
      "grad_norm": 6.528291360655316,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 918
    },
    {
      "epoch": 1.0256696428571428,
      "grad_norm": 8.246820967136108,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 919
    },
    {
      "epoch": 1.0267857142857142,
      "grad_norm": 8.128742999479359,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 920
    },
    {
      "epoch": 1.0279017857142858,
      "grad_norm": 5.899088100130955,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 921
    },
    {
      "epoch": 1.0290178571428572,
      "grad_norm": 5.9160859383423805,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 922
    },
    {
      "epoch": 1.0301339285714286,
      "grad_norm": 7.5461543233467765,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 923
    },
    {
      "epoch": 1.03125,
      "grad_norm": 7.544074649362211,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 924
    },
    {
      "epoch": 1.0323660714285714,
      "grad_norm": 7.76883865725328,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 925
    },
    {
      "epoch": 1.0334821428571428,
      "grad_norm": 8.583870587598536,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 926
    },
    {
      "epoch": 1.0345982142857142,
      "grad_norm": 7.487283711104325,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 927
    },
    {
      "epoch": 1.0357142857142858,
      "grad_norm": 9.107828902718827,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 928
    },
    {
      "epoch": 1.0368303571428572,
      "grad_norm": 8.895168222602944,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 929
    },
    {
      "epoch": 1.0379464285714286,
      "grad_norm": 7.863377387728573,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 930
    },
    {
      "epoch": 1.0390625,
      "grad_norm": 9.202896610015648,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 931
    },
    {
      "epoch": 1.0401785714285714,
      "grad_norm": 9.087757657779905,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 932
    },
    {
      "epoch": 1.0412946428571428,
      "grad_norm": 11.691979485750268,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 933
    },
    {
      "epoch": 1.0424107142857142,
      "grad_norm": 9.715944800997388,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 934
    },
    {
      "epoch": 1.0435267857142858,
      "grad_norm": 7.672755440691134,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 935
    },
    {
      "epoch": 1.0446428571428572,
      "grad_norm": 9.925779036540169,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 936
    },
    {
      "epoch": 1.0457589285714286,
      "grad_norm": 9.401621647923983,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 937
    },
    {
      "epoch": 1.046875,
      "grad_norm": 7.650264470374535,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 938
    },
    {
      "epoch": 1.0479910714285714,
      "grad_norm": 8.727995572648483,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 939
    },
    {
      "epoch": 1.0491071428571428,
      "grad_norm": 7.893928273255643,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 940
    },
    {
      "epoch": 1.0502232142857142,
      "grad_norm": 7.672924768594406,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 941
    },
    {
      "epoch": 1.0513392857142858,
      "grad_norm": 8.082149472744147,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 942
    },
    {
      "epoch": 1.0524553571428572,
      "grad_norm": 8.218914072374437,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 943
    },
    {
      "epoch": 1.0535714285714286,
      "grad_norm": 8.237424693387517,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 944
    },
    {
      "epoch": 1.0546875,
      "grad_norm": 8.797575395356088,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 945
    },
    {
      "epoch": 1.0558035714285714,
      "grad_norm": 7.594985207615466,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 946
    },
    {
      "epoch": 1.0569196428571428,
      "grad_norm": 7.744604566593887,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 947
    },
    {
      "epoch": 1.0580357142857142,
      "grad_norm": 8.48223230680141,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 948
    },
    {
      "epoch": 1.0591517857142858,
      "grad_norm": 8.902059912207811,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 949
    },
    {
      "epoch": 1.0602678571428572,
      "grad_norm": 7.019526192448254,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 950
    },
    {
      "epoch": 1.0613839285714286,
      "grad_norm": 8.716108099803701,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 951
    },
    {
      "epoch": 1.0625,
      "grad_norm": 9.668806835097852,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 952
    },
    {
      "epoch": 1.0636160714285714,
      "grad_norm": 9.103669555002366,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 953
    },
    {
      "epoch": 1.0647321428571428,
      "grad_norm": 7.803717006126733,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 954
    },
    {
      "epoch": 1.0658482142857142,
      "grad_norm": 9.779885447179158,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 955
    },
    {
      "epoch": 1.0669642857142858,
      "grad_norm": 8.960286891772821,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 956
    },
    {
      "epoch": 1.0680803571428572,
      "grad_norm": 6.929278173212102,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 957
    },
    {
      "epoch": 1.0691964285714286,
      "grad_norm": 7.989872824532442,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 958
    },
    {
      "epoch": 1.0703125,
      "grad_norm": 9.681681845923386,
      "learning_rate": 2e-05,
      "loss": 1.3516,
      "step": 959
    },
    {
      "epoch": 1.0714285714285714,
      "grad_norm": 8.473422556227794,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 960
    },
    {
      "epoch": 1.0725446428571428,
      "grad_norm": 8.116099409396508,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 961
    },
    {
      "epoch": 1.0736607142857142,
      "grad_norm": 8.935491306839863,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 962
    },
    {
      "epoch": 1.0747767857142858,
      "grad_norm": 7.938910962515295,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 963
    },
    {
      "epoch": 1.0758928571428572,
      "grad_norm": 7.76380593369636,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 964
    },
    {
      "epoch": 1.0770089285714286,
      "grad_norm": 8.5324099808491,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 965
    },
    {
      "epoch": 1.078125,
      "grad_norm": 7.069390423849135,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 966
    },
    {
      "epoch": 1.0792410714285714,
      "grad_norm": 6.732082234817774,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 967
    },
    {
      "epoch": 1.0803571428571428,
      "grad_norm": 7.473161761558064,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 968
    },
    {
      "epoch": 1.0814732142857142,
      "grad_norm": 8.90118774959711,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 969
    },
    {
      "epoch": 1.0825892857142858,
      "grad_norm": 9.210608618649541,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 970
    },
    {
      "epoch": 1.0837053571428572,
      "grad_norm": 8.325153440443083,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 971
    },
    {
      "epoch": 1.0848214285714286,
      "grad_norm": 7.81906056600338,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 972
    },
    {
      "epoch": 1.0859375,
      "grad_norm": 8.917449349581728,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 973
    },
    {
      "epoch": 1.0870535714285714,
      "grad_norm": 7.981455137513596,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 974
    },
    {
      "epoch": 1.0881696428571428,
      "grad_norm": 8.451815574715173,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 975
    },
    {
      "epoch": 1.0892857142857142,
      "grad_norm": 7.268212391740938,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 976
    },
    {
      "epoch": 1.0904017857142858,
      "grad_norm": 7.658409230270069,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 977
    },
    {
      "epoch": 1.0915178571428572,
      "grad_norm": 15.15205551152532,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 978
    },
    {
      "epoch": 1.0926339285714286,
      "grad_norm": 7.086359731314495,
      "learning_rate": 2e-05,
      "loss": 1.5312,
      "step": 979
    },
    {
      "epoch": 1.09375,
      "grad_norm": 7.619627171623271,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 980
    },
    {
      "epoch": 1.0948660714285714,
      "grad_norm": 8.707069423125219,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 981
    },
    {
      "epoch": 1.0959821428571428,
      "grad_norm": 9.100573958393236,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 982
    },
    {
      "epoch": 1.0970982142857142,
      "grad_norm": 9.26675235352146,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 983
    },
    {
      "epoch": 1.0982142857142858,
      "grad_norm": 8.44603573421521,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 984
    },
    {
      "epoch": 1.0993303571428572,
      "grad_norm": 9.204729369822285,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 985
    },
    {
      "epoch": 1.1004464285714286,
      "grad_norm": 7.983650234016037,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 986
    },
    {
      "epoch": 1.1015625,
      "grad_norm": 8.473150206530828,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 987
    },
    {
      "epoch": 1.1026785714285714,
      "grad_norm": 10.137037697407544,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 988
    },
    {
      "epoch": 1.1037946428571428,
      "grad_norm": 8.533561469453012,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 989
    },
    {
      "epoch": 1.1049107142857142,
      "grad_norm": 7.763177692201899,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 990
    },
    {
      "epoch": 1.1060267857142858,
      "grad_norm": 8.838175342696703,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 991
    },
    {
      "epoch": 1.1071428571428572,
      "grad_norm": 8.077163091266563,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 992
    },
    {
      "epoch": 1.1082589285714286,
      "grad_norm": 10.46047352204952,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 993
    },
    {
      "epoch": 1.109375,
      "grad_norm": 8.202858808069486,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 994
    },
    {
      "epoch": 1.1104910714285714,
      "grad_norm": 8.10209202916333,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 995
    },
    {
      "epoch": 1.1116071428571428,
      "grad_norm": 9.88314368222718,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 996
    },
    {
      "epoch": 1.1127232142857142,
      "grad_norm": 8.811686493964896,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 997
    },
    {
      "epoch": 1.1138392857142858,
      "grad_norm": 8.454040115631631,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 998
    },
    {
      "epoch": 1.1149553571428572,
      "grad_norm": 8.627732956717143,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 999
    },
    {
      "epoch": 1.1160714285714286,
      "grad_norm": 10.865512946521067,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1000
    },
    {
      "epoch": 1.1171875,
      "grad_norm": 10.72607964625994,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1001
    },
    {
      "epoch": 1.1183035714285714,
      "grad_norm": 8.044263677177966,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1002
    },
    {
      "epoch": 1.1194196428571428,
      "grad_norm": 8.549829168232444,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1003
    },
    {
      "epoch": 1.1205357142857142,
      "grad_norm": 9.930080761861092,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1004
    },
    {
      "epoch": 1.1216517857142858,
      "grad_norm": 7.3156586635194785,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 1005
    },
    {
      "epoch": 1.1227678571428572,
      "grad_norm": 7.796179714088146,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 1006
    },
    {
      "epoch": 1.1238839285714286,
      "grad_norm": 8.59099460751913,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1007
    },
    {
      "epoch": 1.125,
      "grad_norm": 7.416425829454912,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1008
    },
    {
      "epoch": 1.1261160714285714,
      "grad_norm": 8.734741268066015,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1009
    },
    {
      "epoch": 1.1272321428571428,
      "grad_norm": 8.515601806655788,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1010
    },
    {
      "epoch": 1.1283482142857142,
      "grad_norm": 8.579207310722206,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1011
    },
    {
      "epoch": 1.1294642857142858,
      "grad_norm": 8.992077433676771,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1012
    },
    {
      "epoch": 1.1305803571428572,
      "grad_norm": 8.02603237095462,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1013
    },
    {
      "epoch": 1.1316964285714286,
      "grad_norm": 8.129177794054137,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 1014
    },
    {
      "epoch": 1.1328125,
      "grad_norm": 6.4617671760269575,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 1015
    },
    {
      "epoch": 1.1339285714285714,
      "grad_norm": 7.842347044503524,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1016
    },
    {
      "epoch": 1.1350446428571428,
      "grad_norm": 8.359204361163812,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 1017
    },
    {
      "epoch": 1.1361607142857142,
      "grad_norm": 7.221328082768517,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 1018
    },
    {
      "epoch": 1.1372767857142858,
      "grad_norm": 8.987926647712927,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 1019
    },
    {
      "epoch": 1.1383928571428572,
      "grad_norm": 8.287388645727265,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 1020
    },
    {
      "epoch": 1.1395089285714286,
      "grad_norm": 8.970566659578541,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1021
    },
    {
      "epoch": 1.140625,
      "grad_norm": 7.602371479050258,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1022
    },
    {
      "epoch": 1.1417410714285714,
      "grad_norm": 7.0423087066433805,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1023
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 8.26616829713952,
      "learning_rate": 2e-05,
      "loss": 1.4141,
      "step": 1024
    },
    {
      "epoch": 1.1439732142857142,
      "grad_norm": 9.308414100971007,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1025
    },
    {
      "epoch": 1.1450892857142858,
      "grad_norm": 7.7435514160514725,
      "learning_rate": 2e-05,
      "loss": 1.5312,
      "step": 1026
    },
    {
      "epoch": 1.1462053571428572,
      "grad_norm": 9.942964338737516,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1027
    },
    {
      "epoch": 1.1473214285714286,
      "grad_norm": 6.118435737508498,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1028
    },
    {
      "epoch": 1.1484375,
      "grad_norm": 7.575108680752182,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1029
    },
    {
      "epoch": 1.1495535714285714,
      "grad_norm": 9.651488649970341,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1030
    },
    {
      "epoch": 1.1506696428571428,
      "grad_norm": 10.604687692639441,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1031
    },
    {
      "epoch": 1.1517857142857142,
      "grad_norm": 7.6076302546350165,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1032
    },
    {
      "epoch": 1.1529017857142858,
      "grad_norm": 8.062805893512195,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 1033
    },
    {
      "epoch": 1.1540178571428572,
      "grad_norm": 9.385699086030966,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 1034
    },
    {
      "epoch": 1.1551339285714286,
      "grad_norm": 8.19347330001589,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1035
    },
    {
      "epoch": 1.15625,
      "grad_norm": 7.635984178539151,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1036
    },
    {
      "epoch": 1.1573660714285714,
      "grad_norm": 8.512759836915512,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1037
    },
    {
      "epoch": 1.1584821428571428,
      "grad_norm": 8.005028976718982,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 1038
    },
    {
      "epoch": 1.1595982142857142,
      "grad_norm": 8.779959280610704,
      "learning_rate": 2e-05,
      "loss": 1.3438,
      "step": 1039
    },
    {
      "epoch": 1.1607142857142858,
      "grad_norm": 8.268418754102159,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1040
    },
    {
      "epoch": 1.1618303571428572,
      "grad_norm": 9.639217626370298,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1041
    },
    {
      "epoch": 1.1629464285714286,
      "grad_norm": 8.953470040388979,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1042
    },
    {
      "epoch": 1.1640625,
      "grad_norm": 8.579779110025848,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 1043
    },
    {
      "epoch": 1.1651785714285714,
      "grad_norm": 10.551453791998298,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1044
    },
    {
      "epoch": 1.1662946428571428,
      "grad_norm": 7.4168160983293445,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 1045
    },
    {
      "epoch": 1.1674107142857142,
      "grad_norm": 8.604528361185546,
      "learning_rate": 2e-05,
      "loss": 3.0312,
      "step": 1046
    },
    {
      "epoch": 1.1685267857142858,
      "grad_norm": 9.67384357023731,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1047
    },
    {
      "epoch": 1.1696428571428572,
      "grad_norm": 8.529282039894515,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1048
    },
    {
      "epoch": 1.1707589285714286,
      "grad_norm": 9.724380017630958,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1049
    },
    {
      "epoch": 1.171875,
      "grad_norm": 9.283640777875414,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1050
    },
    {
      "epoch": 1.1729910714285714,
      "grad_norm": 7.358900030024502,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1051
    },
    {
      "epoch": 1.1741071428571428,
      "grad_norm": 8.591281730702661,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1052
    },
    {
      "epoch": 1.1752232142857142,
      "grad_norm": 8.51461462420764,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1053
    },
    {
      "epoch": 1.1763392857142858,
      "grad_norm": 7.686485750424307,
      "learning_rate": 2e-05,
      "loss": 3.1094,
      "step": 1054
    },
    {
      "epoch": 1.1774553571428572,
      "grad_norm": 8.334625728165836,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1055
    },
    {
      "epoch": 1.1785714285714286,
      "grad_norm": 7.923350342359338,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1056
    },
    {
      "epoch": 1.1796875,
      "grad_norm": 9.293639327481415,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1057
    },
    {
      "epoch": 1.1808035714285714,
      "grad_norm": 7.737544348368465,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1058
    },
    {
      "epoch": 1.1819196428571428,
      "grad_norm": 7.231315275476082,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1059
    },
    {
      "epoch": 1.1830357142857142,
      "grad_norm": 8.457553593564558,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1060
    },
    {
      "epoch": 1.1841517857142858,
      "grad_norm": 9.273286248433536,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1061
    },
    {
      "epoch": 1.1852678571428572,
      "grad_norm": 8.640210076257679,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1062
    },
    {
      "epoch": 1.1863839285714286,
      "grad_norm": 8.3649142904507,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1063
    },
    {
      "epoch": 1.1875,
      "grad_norm": 10.601583963696328,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 1064
    },
    {
      "epoch": 1.1886160714285714,
      "grad_norm": 8.054787520446672,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 1065
    },
    {
      "epoch": 1.1897321428571428,
      "grad_norm": 8.762124703681765,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1066
    },
    {
      "epoch": 1.1908482142857142,
      "grad_norm": 9.698669373532327,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1067
    },
    {
      "epoch": 1.1919642857142858,
      "grad_norm": 8.757127486565853,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1068
    },
    {
      "epoch": 1.1930803571428572,
      "grad_norm": 8.32796346881753,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 1069
    },
    {
      "epoch": 1.1941964285714286,
      "grad_norm": 12.131552225431005,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 1070
    },
    {
      "epoch": 1.1953125,
      "grad_norm": 8.711621821851143,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1071
    },
    {
      "epoch": 1.1964285714285714,
      "grad_norm": 11.268444245472358,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1072
    },
    {
      "epoch": 1.1975446428571428,
      "grad_norm": 7.587655533704155,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 1073
    },
    {
      "epoch": 1.1986607142857142,
      "grad_norm": 7.560455332911672,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1074
    },
    {
      "epoch": 1.1997767857142858,
      "grad_norm": 9.461187203308292,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1075
    },
    {
      "epoch": 1.2008928571428572,
      "grad_norm": 8.556946347027845,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1076
    },
    {
      "epoch": 1.2020089285714286,
      "grad_norm": 8.608771441498698,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1077
    },
    {
      "epoch": 1.203125,
      "grad_norm": 9.717470201256367,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1078
    },
    {
      "epoch": 1.2042410714285714,
      "grad_norm": 7.7875110255487785,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1079
    },
    {
      "epoch": 1.2053571428571428,
      "grad_norm": 7.92812078374669,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 1080
    },
    {
      "epoch": 1.2064732142857142,
      "grad_norm": 9.795563262556863,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1081
    },
    {
      "epoch": 1.2075892857142858,
      "grad_norm": 7.890773681612872,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1082
    },
    {
      "epoch": 1.2087053571428572,
      "grad_norm": 9.258822404318401,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1083
    },
    {
      "epoch": 1.2098214285714286,
      "grad_norm": 9.200110660402915,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 1084
    },
    {
      "epoch": 1.2109375,
      "grad_norm": 7.193216243449202,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1085
    },
    {
      "epoch": 1.2120535714285714,
      "grad_norm": 7.695065753558855,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1086
    },
    {
      "epoch": 1.2131696428571428,
      "grad_norm": 9.483700741830932,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1087
    },
    {
      "epoch": 1.2142857142857142,
      "grad_norm": 8.18977507935526,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1088
    },
    {
      "epoch": 1.2154017857142858,
      "grad_norm": 7.927273611657882,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1089
    },
    {
      "epoch": 1.2165178571428572,
      "grad_norm": 7.345141593857942,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1090
    },
    {
      "epoch": 1.2176339285714286,
      "grad_norm": 8.793419555524935,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1091
    },
    {
      "epoch": 1.21875,
      "grad_norm": 8.954950863624344,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1092
    },
    {
      "epoch": 1.2198660714285714,
      "grad_norm": 7.051559454817158,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1093
    },
    {
      "epoch": 1.2209821428571428,
      "grad_norm": 7.396807715600818,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1094
    },
    {
      "epoch": 1.2220982142857142,
      "grad_norm": 7.273320516778204,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1095
    },
    {
      "epoch": 1.2232142857142858,
      "grad_norm": 7.332871683657693,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1096
    },
    {
      "epoch": 1.2243303571428572,
      "grad_norm": 7.180584955556742,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1097
    },
    {
      "epoch": 1.2254464285714286,
      "grad_norm": 9.073520396693636,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1098
    },
    {
      "epoch": 1.2265625,
      "grad_norm": 8.94497967541895,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1099
    },
    {
      "epoch": 1.2276785714285714,
      "grad_norm": 7.433490879587775,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1100
    },
    {
      "epoch": 1.2287946428571428,
      "grad_norm": 7.088334888936316,
      "learning_rate": 2e-05,
      "loss": 2.9062,
      "step": 1101
    },
    {
      "epoch": 1.2299107142857142,
      "grad_norm": 9.365330016231676,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1102
    },
    {
      "epoch": 1.2310267857142858,
      "grad_norm": 7.501848525083746,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1103
    },
    {
      "epoch": 1.2321428571428572,
      "grad_norm": 9.800509940163135,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1104
    },
    {
      "epoch": 1.2332589285714286,
      "grad_norm": 5.433651708390468,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1105
    },
    {
      "epoch": 1.234375,
      "grad_norm": 6.9717640879185945,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 1106
    },
    {
      "epoch": 1.2354910714285714,
      "grad_norm": 7.416075213593145,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1107
    },
    {
      "epoch": 1.2366071428571428,
      "grad_norm": 7.002940289490218,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1108
    },
    {
      "epoch": 1.2377232142857142,
      "grad_norm": 8.323599551129078,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1109
    },
    {
      "epoch": 1.2388392857142858,
      "grad_norm": 8.801550925084038,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1110
    },
    {
      "epoch": 1.2399553571428572,
      "grad_norm": 7.47376197990806,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1111
    },
    {
      "epoch": 1.2410714285714286,
      "grad_norm": 9.477262121320086,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1112
    },
    {
      "epoch": 1.2421875,
      "grad_norm": 9.750027813048904,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1113
    },
    {
      "epoch": 1.2433035714285714,
      "grad_norm": 7.6224898842338,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1114
    },
    {
      "epoch": 1.2444196428571428,
      "grad_norm": 8.579818516436614,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1115
    },
    {
      "epoch": 1.2455357142857142,
      "grad_norm": 8.386939561711719,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1116
    },
    {
      "epoch": 1.2466517857142858,
      "grad_norm": 9.272759645162939,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1117
    },
    {
      "epoch": 1.2477678571428572,
      "grad_norm": 9.781895226037618,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1118
    },
    {
      "epoch": 1.2488839285714286,
      "grad_norm": 9.594489196191308,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1119
    },
    {
      "epoch": 1.25,
      "grad_norm": 7.779636137947398,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1120
    },
    {
      "epoch": 1.2511160714285714,
      "grad_norm": 10.6243789007046,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1121
    },
    {
      "epoch": 1.2522321428571428,
      "grad_norm": 9.123386302121386,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1122
    },
    {
      "epoch": 1.2533482142857144,
      "grad_norm": 6.258053392685313,
      "learning_rate": 2e-05,
      "loss": 2.7656,
      "step": 1123
    },
    {
      "epoch": 1.2544642857142856,
      "grad_norm": 9.308791084917267,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1124
    },
    {
      "epoch": 1.2555803571428572,
      "grad_norm": 7.973828682349559,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1125
    },
    {
      "epoch": 1.2566964285714286,
      "grad_norm": 7.783847235239663,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1126
    },
    {
      "epoch": 1.2578125,
      "grad_norm": 8.328531522778615,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1127
    },
    {
      "epoch": 1.2589285714285714,
      "grad_norm": 9.591850186939295,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1128
    },
    {
      "epoch": 1.2600446428571428,
      "grad_norm": 9.313474808351392,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1129
    },
    {
      "epoch": 1.2611607142857144,
      "grad_norm": 9.17328491362963,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1130
    },
    {
      "epoch": 1.2622767857142856,
      "grad_norm": 9.278073709501776,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1131
    },
    {
      "epoch": 1.2633928571428572,
      "grad_norm": 8.966381934071176,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1132
    },
    {
      "epoch": 1.2645089285714286,
      "grad_norm": 10.849479065898334,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1133
    },
    {
      "epoch": 1.265625,
      "grad_norm": 7.6207994293623535,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1134
    },
    {
      "epoch": 1.2667410714285714,
      "grad_norm": 8.975259777546169,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1135
    },
    {
      "epoch": 1.2678571428571428,
      "grad_norm": 10.172079233832857,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1136
    },
    {
      "epoch": 1.2689732142857144,
      "grad_norm": 9.582853005773998,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1137
    },
    {
      "epoch": 1.2700892857142856,
      "grad_norm": 9.370396225075398,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1138
    },
    {
      "epoch": 1.2712053571428572,
      "grad_norm": 8.711094177569574,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1139
    },
    {
      "epoch": 1.2723214285714286,
      "grad_norm": 8.137463629134196,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 1140
    },
    {
      "epoch": 1.2734375,
      "grad_norm": 8.797478627555426,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 1141
    },
    {
      "epoch": 1.2745535714285714,
      "grad_norm": 9.300867451972676,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1142
    },
    {
      "epoch": 1.2756696428571428,
      "grad_norm": 6.922886751208065,
      "learning_rate": 2e-05,
      "loss": 3.0938,
      "step": 1143
    },
    {
      "epoch": 1.2767857142857144,
      "grad_norm": 8.021524858804636,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1144
    },
    {
      "epoch": 1.2779017857142856,
      "grad_norm": 8.209953593456431,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 1145
    },
    {
      "epoch": 1.2790178571428572,
      "grad_norm": 9.789382928443686,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1146
    },
    {
      "epoch": 1.2801339285714286,
      "grad_norm": 11.997242478403107,
      "learning_rate": 2e-05,
      "loss": 1.4062,
      "step": 1147
    },
    {
      "epoch": 1.28125,
      "grad_norm": 7.894543550784331,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1148
    },
    {
      "epoch": 1.2823660714285714,
      "grad_norm": 8.330450785563997,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1149
    },
    {
      "epoch": 1.2834821428571428,
      "grad_norm": 14.764561944864528,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1150
    },
    {
      "epoch": 1.2845982142857144,
      "grad_norm": 9.593684153020178,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1151
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 8.816782996412055,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1152
    },
    {
      "epoch": 1.2868303571428572,
      "grad_norm": 10.508542361930159,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1153
    },
    {
      "epoch": 1.2879464285714286,
      "grad_norm": 8.570274601179115,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1154
    },
    {
      "epoch": 1.2890625,
      "grad_norm": 8.369056932164737,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1155
    },
    {
      "epoch": 1.2901785714285714,
      "grad_norm": 6.596977175743173,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1156
    },
    {
      "epoch": 1.2912946428571428,
      "grad_norm": 8.989287574747005,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1157
    },
    {
      "epoch": 1.2924107142857144,
      "grad_norm": 9.411917641431922,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1158
    },
    {
      "epoch": 1.2935267857142856,
      "grad_norm": 8.896835711962666,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1159
    },
    {
      "epoch": 1.2946428571428572,
      "grad_norm": 8.209105704049929,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1160
    },
    {
      "epoch": 1.2957589285714286,
      "grad_norm": 9.410771645945152,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1161
    },
    {
      "epoch": 1.296875,
      "grad_norm": 8.624665570177559,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1162
    },
    {
      "epoch": 1.2979910714285714,
      "grad_norm": 8.286585118761423,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1163
    },
    {
      "epoch": 1.2991071428571428,
      "grad_norm": 10.851284576159845,
      "learning_rate": 2e-05,
      "loss": 1.3281,
      "step": 1164
    },
    {
      "epoch": 1.3002232142857144,
      "grad_norm": 9.135330665471441,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1165
    },
    {
      "epoch": 1.3013392857142856,
      "grad_norm": 10.167618742023384,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1166
    },
    {
      "epoch": 1.3024553571428572,
      "grad_norm": 9.620639593277623,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1167
    },
    {
      "epoch": 1.3035714285714286,
      "grad_norm": 8.03741849248842,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1168
    },
    {
      "epoch": 1.3046875,
      "grad_norm": 9.867957511031967,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1169
    },
    {
      "epoch": 1.3058035714285714,
      "grad_norm": 7.717896910066155,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1170
    },
    {
      "epoch": 1.3069196428571428,
      "grad_norm": 9.777731918416134,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1171
    },
    {
      "epoch": 1.3080357142857144,
      "grad_norm": 11.117498258752406,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1172
    },
    {
      "epoch": 1.3091517857142856,
      "grad_norm": 7.4903544784898255,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1173
    },
    {
      "epoch": 1.3102678571428572,
      "grad_norm": 10.974716339687923,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1174
    },
    {
      "epoch": 1.3113839285714286,
      "grad_norm": 10.135457743163899,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1175
    },
    {
      "epoch": 1.3125,
      "grad_norm": 8.268364851943613,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1176
    },
    {
      "epoch": 1.3136160714285714,
      "grad_norm": 10.235575709805547,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 1177
    },
    {
      "epoch": 1.3147321428571428,
      "grad_norm": 8.71103065038037,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1178
    },
    {
      "epoch": 1.3158482142857144,
      "grad_norm": 9.261195602404078,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1179
    },
    {
      "epoch": 1.3169642857142856,
      "grad_norm": 7.545785946999469,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1180
    },
    {
      "epoch": 1.3180803571428572,
      "grad_norm": 9.102385022991381,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1181
    },
    {
      "epoch": 1.3191964285714286,
      "grad_norm": 9.264027113282925,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1182
    },
    {
      "epoch": 1.3203125,
      "grad_norm": 9.384896620436999,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1183
    },
    {
      "epoch": 1.3214285714285714,
      "grad_norm": 8.262065278480032,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1184
    },
    {
      "epoch": 1.3225446428571428,
      "grad_norm": 10.396835398587372,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1185
    },
    {
      "epoch": 1.3236607142857144,
      "grad_norm": 8.72527311956647,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1186
    },
    {
      "epoch": 1.3247767857142856,
      "grad_norm": 8.720779476449058,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1187
    },
    {
      "epoch": 1.3258928571428572,
      "grad_norm": 9.397950899720895,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1188
    },
    {
      "epoch": 1.3270089285714286,
      "grad_norm": 7.895769998445729,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1189
    },
    {
      "epoch": 1.328125,
      "grad_norm": 8.635018496329652,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1190
    },
    {
      "epoch": 1.3292410714285714,
      "grad_norm": 10.627723842876042,
      "learning_rate": 2e-05,
      "loss": 1.5,
      "step": 1191
    },
    {
      "epoch": 1.3303571428571428,
      "grad_norm": 7.509472199583603,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1192
    },
    {
      "epoch": 1.3314732142857144,
      "grad_norm": 8.972348640164384,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 1193
    },
    {
      "epoch": 1.3325892857142856,
      "grad_norm": 8.963677823419317,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1194
    },
    {
      "epoch": 1.3337053571428572,
      "grad_norm": 10.913900657621017,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1195
    },
    {
      "epoch": 1.3348214285714286,
      "grad_norm": 9.393144920900719,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1196
    },
    {
      "epoch": 1.3359375,
      "grad_norm": 8.116816043728063,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1197
    },
    {
      "epoch": 1.3370535714285714,
      "grad_norm": 9.595421232775031,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1198
    },
    {
      "epoch": 1.3381696428571428,
      "grad_norm": 9.685649730639401,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1199
    },
    {
      "epoch": 1.3392857142857144,
      "grad_norm": 7.4569866475420055,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1200
    },
    {
      "epoch": 1.3404017857142856,
      "grad_norm": 7.592941596952568,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 1201
    },
    {
      "epoch": 1.3415178571428572,
      "grad_norm": 8.343161606729224,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1202
    },
    {
      "epoch": 1.3426339285714286,
      "grad_norm": 9.556678114992287,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1203
    },
    {
      "epoch": 1.34375,
      "grad_norm": 9.99194397519716,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1204
    },
    {
      "epoch": 1.3448660714285714,
      "grad_norm": 7.989208631286538,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 1205
    },
    {
      "epoch": 1.3459821428571428,
      "grad_norm": 10.940848690752347,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 1206
    },
    {
      "epoch": 1.3470982142857144,
      "grad_norm": 7.6599173637343245,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1207
    },
    {
      "epoch": 1.3482142857142856,
      "grad_norm": 9.420871136343617,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1208
    },
    {
      "epoch": 1.3493303571428572,
      "grad_norm": 8.775636075711487,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1209
    },
    {
      "epoch": 1.3504464285714286,
      "grad_norm": 8.808656183646793,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1210
    },
    {
      "epoch": 1.3515625,
      "grad_norm": 8.790718208540653,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1211
    },
    {
      "epoch": 1.3526785714285714,
      "grad_norm": 8.289810898612133,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1212
    },
    {
      "epoch": 1.3537946428571428,
      "grad_norm": 8.829937020826023,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1213
    },
    {
      "epoch": 1.3549107142857144,
      "grad_norm": 9.256337076269721,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1214
    },
    {
      "epoch": 1.3560267857142856,
      "grad_norm": 7.506571999498742,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1215
    },
    {
      "epoch": 1.3571428571428572,
      "grad_norm": 8.619595250551384,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1216
    },
    {
      "epoch": 1.3582589285714286,
      "grad_norm": 9.250413782198942,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1217
    },
    {
      "epoch": 1.359375,
      "grad_norm": 9.50373195543006,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1218
    },
    {
      "epoch": 1.3604910714285714,
      "grad_norm": 9.731338355062372,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1219
    },
    {
      "epoch": 1.3616071428571428,
      "grad_norm": 10.251342469808453,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1220
    },
    {
      "epoch": 1.3627232142857144,
      "grad_norm": 9.304201879122632,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1221
    },
    {
      "epoch": 1.3638392857142856,
      "grad_norm": 7.3023154491070565,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1222
    },
    {
      "epoch": 1.3649553571428572,
      "grad_norm": 7.986516415040767,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1223
    },
    {
      "epoch": 1.3660714285714286,
      "grad_norm": 9.159230807520133,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1224
    },
    {
      "epoch": 1.3671875,
      "grad_norm": 9.798305113949336,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1225
    },
    {
      "epoch": 1.3683035714285714,
      "grad_norm": 8.31630395716757,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1226
    },
    {
      "epoch": 1.3694196428571428,
      "grad_norm": 10.03912732212497,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1227
    },
    {
      "epoch": 1.3705357142857144,
      "grad_norm": 9.400539486534997,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1228
    },
    {
      "epoch": 1.3716517857142856,
      "grad_norm": 11.839738639190529,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 1229
    },
    {
      "epoch": 1.3727678571428572,
      "grad_norm": 7.999537099565585,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1230
    },
    {
      "epoch": 1.3738839285714286,
      "grad_norm": 8.216767983951215,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 1231
    },
    {
      "epoch": 1.375,
      "grad_norm": 11.444326064395453,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1232
    },
    {
      "epoch": 1.3761160714285714,
      "grad_norm": 8.097724406118463,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1233
    },
    {
      "epoch": 1.3772321428571428,
      "grad_norm": 12.600222637947544,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 1234
    },
    {
      "epoch": 1.3783482142857144,
      "grad_norm": 9.968216890134098,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 1235
    },
    {
      "epoch": 1.3794642857142856,
      "grad_norm": 9.847527960927193,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1236
    },
    {
      "epoch": 1.3805803571428572,
      "grad_norm": 9.108000633081765,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1237
    },
    {
      "epoch": 1.3816964285714286,
      "grad_norm": 7.884622817270073,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1238
    },
    {
      "epoch": 1.3828125,
      "grad_norm": 8.601721366863462,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1239
    },
    {
      "epoch": 1.3839285714285714,
      "grad_norm": 8.492218878592322,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1240
    },
    {
      "epoch": 1.3850446428571428,
      "grad_norm": 8.23919244548802,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1241
    },
    {
      "epoch": 1.3861607142857144,
      "grad_norm": 8.19658816488533,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1242
    },
    {
      "epoch": 1.3872767857142856,
      "grad_norm": 9.780506147593325,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1243
    },
    {
      "epoch": 1.3883928571428572,
      "grad_norm": 7.689079101093529,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1244
    },
    {
      "epoch": 1.3895089285714286,
      "grad_norm": 11.405380297675658,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1245
    },
    {
      "epoch": 1.390625,
      "grad_norm": 7.877677450575858,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1246
    },
    {
      "epoch": 1.3917410714285714,
      "grad_norm": 10.142678111313343,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1247
    },
    {
      "epoch": 1.3928571428571428,
      "grad_norm": 8.84911245118999,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1248
    },
    {
      "epoch": 1.3939732142857144,
      "grad_norm": 8.349508257739474,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1249
    },
    {
      "epoch": 1.3950892857142856,
      "grad_norm": 10.953375164944068,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1250
    },
    {
      "epoch": 1.3962053571428572,
      "grad_norm": 8.57949834005786,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1251
    },
    {
      "epoch": 1.3973214285714286,
      "grad_norm": 8.635171186114034,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1252
    },
    {
      "epoch": 1.3984375,
      "grad_norm": 10.207946251753595,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1253
    },
    {
      "epoch": 1.3995535714285714,
      "grad_norm": 8.656249736232539,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1254
    },
    {
      "epoch": 1.4006696428571428,
      "grad_norm": 8.555930372543912,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1255
    },
    {
      "epoch": 1.4017857142857144,
      "grad_norm": 8.443056612790487,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1256
    },
    {
      "epoch": 1.4029017857142856,
      "grad_norm": 7.654865151270271,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1257
    },
    {
      "epoch": 1.4040178571428572,
      "grad_norm": 9.254546395630177,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1258
    },
    {
      "epoch": 1.4051339285714286,
      "grad_norm": 10.150048178692769,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1259
    },
    {
      "epoch": 1.40625,
      "grad_norm": 8.203115966267552,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1260
    },
    {
      "epoch": 1.4073660714285714,
      "grad_norm": 9.08234696514549,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1261
    },
    {
      "epoch": 1.4084821428571428,
      "grad_norm": 10.420010674522723,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 1262
    },
    {
      "epoch": 1.4095982142857144,
      "grad_norm": 9.094539098411033,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1263
    },
    {
      "epoch": 1.4107142857142856,
      "grad_norm": 7.826211792493097,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1264
    },
    {
      "epoch": 1.4118303571428572,
      "grad_norm": 8.711283084903403,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1265
    },
    {
      "epoch": 1.4129464285714286,
      "grad_norm": 8.780770832805022,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1266
    },
    {
      "epoch": 1.4140625,
      "grad_norm": 5.643139211801992,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1267
    },
    {
      "epoch": 1.4151785714285714,
      "grad_norm": 7.1558335492974035,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1268
    },
    {
      "epoch": 1.4162946428571428,
      "grad_norm": 9.682229773748837,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 1269
    },
    {
      "epoch": 1.4174107142857144,
      "grad_norm": 8.06674522474829,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1270
    },
    {
      "epoch": 1.4185267857142856,
      "grad_norm": 8.364376229573383,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 1271
    },
    {
      "epoch": 1.4196428571428572,
      "grad_norm": 11.160626357556595,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1272
    },
    {
      "epoch": 1.4207589285714286,
      "grad_norm": 9.200783905082456,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1273
    },
    {
      "epoch": 1.421875,
      "grad_norm": 9.237998344330064,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1274
    },
    {
      "epoch": 1.4229910714285714,
      "grad_norm": 8.273756030099419,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1275
    },
    {
      "epoch": 1.4241071428571428,
      "grad_norm": 9.403904737488276,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1276
    },
    {
      "epoch": 1.4252232142857144,
      "grad_norm": 10.84949710995534,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1277
    },
    {
      "epoch": 1.4263392857142856,
      "grad_norm": 9.858320103105127,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1278
    },
    {
      "epoch": 1.4274553571428572,
      "grad_norm": 9.002876494422388,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 1279
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 10.36907855545062,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 1280
    },
    {
      "epoch": 1.4296875,
      "grad_norm": 8.220803375027813,
      "learning_rate": 2e-05,
      "loss": 1.4453,
      "step": 1281
    },
    {
      "epoch": 1.4308035714285714,
      "grad_norm": 10.891313878053658,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1282
    },
    {
      "epoch": 1.4319196428571428,
      "grad_norm": 9.59590762846664,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1283
    },
    {
      "epoch": 1.4330357142857144,
      "grad_norm": 11.437379488165508,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1284
    },
    {
      "epoch": 1.4341517857142856,
      "grad_norm": 9.666270198219761,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1285
    },
    {
      "epoch": 1.4352678571428572,
      "grad_norm": 9.827495539701932,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1286
    },
    {
      "epoch": 1.4363839285714286,
      "grad_norm": 9.257557918073589,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1287
    },
    {
      "epoch": 1.4375,
      "grad_norm": 8.294507959748042,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1288
    },
    {
      "epoch": 1.4386160714285714,
      "grad_norm": 9.006891008766962,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1289
    },
    {
      "epoch": 1.4397321428571428,
      "grad_norm": 8.488075850145647,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1290
    },
    {
      "epoch": 1.4408482142857144,
      "grad_norm": 10.835565471337143,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1291
    },
    {
      "epoch": 1.4419642857142856,
      "grad_norm": 10.77221035426753,
      "learning_rate": 2e-05,
      "loss": 1.4688,
      "step": 1292
    },
    {
      "epoch": 1.4430803571428572,
      "grad_norm": 11.151705526983818,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1293
    },
    {
      "epoch": 1.4441964285714286,
      "grad_norm": 9.98939825466068,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1294
    },
    {
      "epoch": 1.4453125,
      "grad_norm": 9.088514441344408,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1295
    },
    {
      "epoch": 1.4464285714285714,
      "grad_norm": 9.431136138281046,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1296
    },
    {
      "epoch": 1.4475446428571428,
      "grad_norm": 8.813406012793843,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 1297
    },
    {
      "epoch": 1.4486607142857144,
      "grad_norm": 7.321410948779888,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1298
    },
    {
      "epoch": 1.4497767857142856,
      "grad_norm": 9.347812457949315,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1299
    },
    {
      "epoch": 1.4508928571428572,
      "grad_norm": 7.051124288726215,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1300
    },
    {
      "epoch": 1.4520089285714286,
      "grad_norm": 6.587555223954526,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 1301
    },
    {
      "epoch": 1.453125,
      "grad_norm": 10.705552328043911,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1302
    },
    {
      "epoch": 1.4542410714285714,
      "grad_norm": 9.38049761572144,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1303
    },
    {
      "epoch": 1.4553571428571428,
      "grad_norm": 8.610715199580302,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1304
    },
    {
      "epoch": 1.4564732142857144,
      "grad_norm": 10.505908591298297,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1305
    },
    {
      "epoch": 1.4575892857142856,
      "grad_norm": 8.075456699354536,
      "learning_rate": 2e-05,
      "loss": 1.4219,
      "step": 1306
    },
    {
      "epoch": 1.4587053571428572,
      "grad_norm": 9.506790352065122,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1307
    },
    {
      "epoch": 1.4598214285714286,
      "grad_norm": 10.179823854286406,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1308
    },
    {
      "epoch": 1.4609375,
      "grad_norm": 10.95907634093396,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1309
    },
    {
      "epoch": 1.4620535714285714,
      "grad_norm": 11.350374338087741,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1310
    },
    {
      "epoch": 1.4631696428571428,
      "grad_norm": 8.98414415203287,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1311
    },
    {
      "epoch": 1.4642857142857144,
      "grad_norm": 12.248351815012791,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1312
    },
    {
      "epoch": 1.4654017857142856,
      "grad_norm": 9.782110975643725,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1313
    },
    {
      "epoch": 1.4665178571428572,
      "grad_norm": 9.637522556761839,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1314
    },
    {
      "epoch": 1.4676339285714286,
      "grad_norm": 10.17849804720928,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1315
    },
    {
      "epoch": 1.46875,
      "grad_norm": 10.220372139736691,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 1316
    },
    {
      "epoch": 1.4698660714285714,
      "grad_norm": 8.77208299382895,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1317
    },
    {
      "epoch": 1.4709821428571428,
      "grad_norm": 8.734981600484762,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 1318
    },
    {
      "epoch": 1.4720982142857144,
      "grad_norm": 10.619466330326123,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1319
    },
    {
      "epoch": 1.4732142857142856,
      "grad_norm": 8.795938947465466,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1320
    },
    {
      "epoch": 1.4743303571428572,
      "grad_norm": 8.561916476159835,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1321
    },
    {
      "epoch": 1.4754464285714286,
      "grad_norm": 9.910884241465471,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1322
    },
    {
      "epoch": 1.4765625,
      "grad_norm": 9.016103274377754,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1323
    },
    {
      "epoch": 1.4776785714285714,
      "grad_norm": 7.850106937457594,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1324
    },
    {
      "epoch": 1.4787946428571428,
      "grad_norm": 9.954364401055683,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1325
    },
    {
      "epoch": 1.4799107142857144,
      "grad_norm": 9.211949335693518,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1326
    },
    {
      "epoch": 1.4810267857142856,
      "grad_norm": 8.461458204640762,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1327
    },
    {
      "epoch": 1.4821428571428572,
      "grad_norm": 9.88457879600015,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1328
    },
    {
      "epoch": 1.4832589285714286,
      "grad_norm": 9.429982098518263,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1329
    },
    {
      "epoch": 1.484375,
      "grad_norm": 10.46942108365679,
      "learning_rate": 2e-05,
      "loss": 2.8594,
      "step": 1330
    },
    {
      "epoch": 1.4854910714285714,
      "grad_norm": 9.307743933558234,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1331
    },
    {
      "epoch": 1.4866071428571428,
      "grad_norm": 9.775190618209708,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 1332
    },
    {
      "epoch": 1.4877232142857144,
      "grad_norm": 7.94461890454244,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 1333
    },
    {
      "epoch": 1.4888392857142856,
      "grad_norm": 8.181808596298257,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 1334
    },
    {
      "epoch": 1.4899553571428572,
      "grad_norm": 8.577576800665486,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1335
    },
    {
      "epoch": 1.4910714285714286,
      "grad_norm": 6.308229841960129,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1336
    },
    {
      "epoch": 1.4921875,
      "grad_norm": 8.967499981950674,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1337
    },
    {
      "epoch": 1.4933035714285714,
      "grad_norm": 11.869718193808279,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1338
    },
    {
      "epoch": 1.4944196428571428,
      "grad_norm": 8.198769291208936,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1339
    },
    {
      "epoch": 1.4955357142857144,
      "grad_norm": 10.679253612630305,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 1340
    },
    {
      "epoch": 1.4966517857142856,
      "grad_norm": 9.473446615431198,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1341
    },
    {
      "epoch": 1.4977678571428572,
      "grad_norm": 10.156580206499383,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1342
    },
    {
      "epoch": 1.4988839285714286,
      "grad_norm": 8.989884554918486,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 1343
    },
    {
      "epoch": 1.5,
      "grad_norm": 9.400159850685863,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1344
    },
    {
      "epoch": 1.5011160714285714,
      "grad_norm": 7.613643791477073,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1345
    },
    {
      "epoch": 1.5022321428571428,
      "grad_norm": 8.316291270754624,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1346
    },
    {
      "epoch": 1.5033482142857144,
      "grad_norm": 9.136223699139029,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1347
    },
    {
      "epoch": 1.5044642857142856,
      "grad_norm": 8.714568576850318,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1348
    },
    {
      "epoch": 1.5055803571428572,
      "grad_norm": 9.315276465784633,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1349
    },
    {
      "epoch": 1.5066964285714286,
      "grad_norm": 6.036560483436557,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1350
    },
    {
      "epoch": 1.5078125,
      "grad_norm": 8.786591322202597,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1351
    },
    {
      "epoch": 1.5089285714285714,
      "grad_norm": 8.923725409661635,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1352
    },
    {
      "epoch": 1.5100446428571428,
      "grad_norm": 8.44215884090243,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1353
    },
    {
      "epoch": 1.5111607142857144,
      "grad_norm": 13.668143226424583,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 1354
    },
    {
      "epoch": 1.5122767857142856,
      "grad_norm": 8.872955853150549,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1355
    },
    {
      "epoch": 1.5133928571428572,
      "grad_norm": 8.51784721577343,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1356
    },
    {
      "epoch": 1.5145089285714286,
      "grad_norm": 10.178868859185512,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1357
    },
    {
      "epoch": 1.515625,
      "grad_norm": 6.830645718879109,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1358
    },
    {
      "epoch": 1.5167410714285714,
      "grad_norm": 9.44156260089097,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1359
    },
    {
      "epoch": 1.5178571428571428,
      "grad_norm": 9.794584733988376,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1360
    },
    {
      "epoch": 1.5189732142857144,
      "grad_norm": 6.649240390053075,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1361
    },
    {
      "epoch": 1.5200892857142856,
      "grad_norm": 5.9710737111926475,
      "learning_rate": 2e-05,
      "loss": 0.9648,
      "step": 1362
    },
    {
      "epoch": 1.5212053571428572,
      "grad_norm": 8.598490124505231,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1363
    },
    {
      "epoch": 1.5223214285714286,
      "grad_norm": 8.352090215515267,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1364
    },
    {
      "epoch": 1.5234375,
      "grad_norm": 9.720000480660136,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1365
    },
    {
      "epoch": 1.5245535714285714,
      "grad_norm": 8.983418450357972,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1366
    },
    {
      "epoch": 1.5256696428571428,
      "grad_norm": 9.69182131037961,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 1367
    },
    {
      "epoch": 1.5267857142857144,
      "grad_norm": 17.4536515251492,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1368
    },
    {
      "epoch": 1.5279017857142856,
      "grad_norm": 10.064386675694546,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 1369
    },
    {
      "epoch": 1.5290178571428572,
      "grad_norm": 7.626036909476946,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1370
    },
    {
      "epoch": 1.5301339285714286,
      "grad_norm": 8.547319871516027,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1371
    },
    {
      "epoch": 1.53125,
      "grad_norm": 10.290068210221953,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1372
    },
    {
      "epoch": 1.5323660714285714,
      "grad_norm": 10.519494395821514,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1373
    },
    {
      "epoch": 1.5334821428571428,
      "grad_norm": 9.723586090309666,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 1374
    },
    {
      "epoch": 1.5345982142857144,
      "grad_norm": 9.272419792891116,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1375
    },
    {
      "epoch": 1.5357142857142856,
      "grad_norm": 8.28458315819196,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 1376
    },
    {
      "epoch": 1.5368303571428572,
      "grad_norm": 9.566451166854504,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1377
    },
    {
      "epoch": 1.5379464285714286,
      "grad_norm": 6.477627828146688,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1378
    },
    {
      "epoch": 1.5390625,
      "grad_norm": 10.047897531427251,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1379
    },
    {
      "epoch": 1.5401785714285714,
      "grad_norm": 8.45973267097152,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1380
    },
    {
      "epoch": 1.5412946428571428,
      "grad_norm": 10.18591831912735,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1381
    },
    {
      "epoch": 1.5424107142857144,
      "grad_norm": 9.615141765825463,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1382
    },
    {
      "epoch": 1.5435267857142856,
      "grad_norm": 8.196316758217664,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 1383
    },
    {
      "epoch": 1.5446428571428572,
      "grad_norm": 9.348686007890919,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 1384
    },
    {
      "epoch": 1.5457589285714286,
      "grad_norm": 10.400151563814301,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1385
    },
    {
      "epoch": 1.546875,
      "grad_norm": 8.614021748596835,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1386
    },
    {
      "epoch": 1.5479910714285714,
      "grad_norm": 9.8648604500996,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 1387
    },
    {
      "epoch": 1.5491071428571428,
      "grad_norm": 8.662676330009363,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1388
    },
    {
      "epoch": 1.5502232142857144,
      "grad_norm": 8.704383371648056,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1389
    },
    {
      "epoch": 1.5513392857142856,
      "grad_norm": 8.867178361649243,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1390
    },
    {
      "epoch": 1.5524553571428572,
      "grad_norm": 8.847060171037617,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1391
    },
    {
      "epoch": 1.5535714285714286,
      "grad_norm": 7.151112385174206,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 1392
    },
    {
      "epoch": 1.5546875,
      "grad_norm": 8.96752773500939,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1393
    },
    {
      "epoch": 1.5558035714285714,
      "grad_norm": 9.485938892971276,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1394
    },
    {
      "epoch": 1.5569196428571428,
      "grad_norm": 9.029006611612331,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1395
    },
    {
      "epoch": 1.5580357142857144,
      "grad_norm": 9.547506900517366,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1396
    },
    {
      "epoch": 1.5591517857142856,
      "grad_norm": 10.300458744365,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1397
    },
    {
      "epoch": 1.5602678571428572,
      "grad_norm": 8.249641092226316,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1398
    },
    {
      "epoch": 1.5613839285714286,
      "grad_norm": 7.61290028950055,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1399
    },
    {
      "epoch": 1.5625,
      "grad_norm": 10.76733539896417,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1400
    },
    {
      "epoch": 1.5636160714285714,
      "grad_norm": 9.09189696387239,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1401
    },
    {
      "epoch": 1.5647321428571428,
      "grad_norm": 9.422831315441632,
      "learning_rate": 2e-05,
      "loss": 1.4375,
      "step": 1402
    },
    {
      "epoch": 1.5658482142857144,
      "grad_norm": 8.723724478977571,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1403
    },
    {
      "epoch": 1.5669642857142856,
      "grad_norm": 7.512133054215442,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1404
    },
    {
      "epoch": 1.5680803571428572,
      "grad_norm": 8.787479864749605,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1405
    },
    {
      "epoch": 1.5691964285714286,
      "grad_norm": 7.665087878903814,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1406
    },
    {
      "epoch": 1.5703125,
      "grad_norm": 10.491540601187404,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 1407
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 7.101735868165235,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1408
    },
    {
      "epoch": 1.5725446428571428,
      "grad_norm": 9.30588501022114,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 1409
    },
    {
      "epoch": 1.5736607142857144,
      "grad_norm": 11.0615284145666,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1410
    },
    {
      "epoch": 1.5747767857142856,
      "grad_norm": 9.397955897353127,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 1411
    },
    {
      "epoch": 1.5758928571428572,
      "grad_norm": 9.121632928060553,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1412
    },
    {
      "epoch": 1.5770089285714286,
      "grad_norm": 12.420809074694578,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1413
    },
    {
      "epoch": 1.578125,
      "grad_norm": 8.831113735422129,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1414
    },
    {
      "epoch": 1.5792410714285714,
      "grad_norm": 9.891142653562204,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 1415
    },
    {
      "epoch": 1.5803571428571428,
      "grad_norm": 9.482972724890704,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 1416
    },
    {
      "epoch": 1.5814732142857144,
      "grad_norm": 8.158664783720093,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1417
    },
    {
      "epoch": 1.5825892857142856,
      "grad_norm": 11.369955019646758,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1418
    },
    {
      "epoch": 1.5837053571428572,
      "grad_norm": 9.60203483873775,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1419
    },
    {
      "epoch": 1.5848214285714286,
      "grad_norm": 11.081047783881354,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1420
    },
    {
      "epoch": 1.5859375,
      "grad_norm": 10.584082052344462,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 1421
    },
    {
      "epoch": 1.5870535714285714,
      "grad_norm": 9.083302460358395,
      "learning_rate": 2e-05,
      "loss": 2.8438,
      "step": 1422
    },
    {
      "epoch": 1.5881696428571428,
      "grad_norm": 9.299721662728967,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1423
    },
    {
      "epoch": 1.5892857142857144,
      "grad_norm": 10.434119617236153,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1424
    },
    {
      "epoch": 1.5904017857142856,
      "grad_norm": 8.279427493733941,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1425
    },
    {
      "epoch": 1.5915178571428572,
      "grad_norm": 9.010498417662703,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1426
    },
    {
      "epoch": 1.5926339285714286,
      "grad_norm": 9.716984132111378,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1427
    },
    {
      "epoch": 1.59375,
      "grad_norm": 8.487157790408737,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1428
    },
    {
      "epoch": 1.5948660714285714,
      "grad_norm": 8.408484866806726,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1429
    },
    {
      "epoch": 1.5959821428571428,
      "grad_norm": 11.830329826981554,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1430
    },
    {
      "epoch": 1.5970982142857144,
      "grad_norm": 9.349285227785224,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1431
    },
    {
      "epoch": 1.5982142857142856,
      "grad_norm": 9.438477080280949,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1432
    },
    {
      "epoch": 1.5993303571428572,
      "grad_norm": 8.558669662965295,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1433
    },
    {
      "epoch": 1.6004464285714286,
      "grad_norm": 9.27347033122972,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1434
    },
    {
      "epoch": 1.6015625,
      "grad_norm": 9.034405340885417,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1435
    },
    {
      "epoch": 1.6026785714285714,
      "grad_norm": 10.417668835864772,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1436
    },
    {
      "epoch": 1.6037946428571428,
      "grad_norm": 10.954778964048971,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1437
    },
    {
      "epoch": 1.6049107142857144,
      "grad_norm": 8.262059728526577,
      "learning_rate": 2e-05,
      "loss": 2.8594,
      "step": 1438
    },
    {
      "epoch": 1.6060267857142856,
      "grad_norm": 8.152251275325046,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1439
    },
    {
      "epoch": 1.6071428571428572,
      "grad_norm": 10.201402789725414,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 1440
    },
    {
      "epoch": 1.6082589285714286,
      "grad_norm": 10.674213719399992,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1441
    },
    {
      "epoch": 1.609375,
      "grad_norm": 10.406447891545909,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1442
    },
    {
      "epoch": 1.6104910714285714,
      "grad_norm": 9.839225536353041,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 1443
    },
    {
      "epoch": 1.6116071428571428,
      "grad_norm": 9.146105676663144,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1444
    },
    {
      "epoch": 1.6127232142857144,
      "grad_norm": 7.399758047591472,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1445
    },
    {
      "epoch": 1.6138392857142856,
      "grad_norm": 8.39525697340261,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1446
    },
    {
      "epoch": 1.6149553571428572,
      "grad_norm": 9.335204404936587,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1447
    },
    {
      "epoch": 1.6160714285714286,
      "grad_norm": 8.742659366869468,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1448
    },
    {
      "epoch": 1.6171875,
      "grad_norm": 8.687213062894054,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1449
    },
    {
      "epoch": 1.6183035714285714,
      "grad_norm": 10.520121734282036,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1450
    },
    {
      "epoch": 1.6194196428571428,
      "grad_norm": 10.022705680397964,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1451
    },
    {
      "epoch": 1.6205357142857144,
      "grad_norm": 12.738272070105687,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1452
    },
    {
      "epoch": 1.6216517857142856,
      "grad_norm": 8.025051960351693,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 1453
    },
    {
      "epoch": 1.6227678571428572,
      "grad_norm": 7.232327378541648,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1454
    },
    {
      "epoch": 1.6238839285714286,
      "grad_norm": 9.001494871591028,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1455
    },
    {
      "epoch": 1.625,
      "grad_norm": 7.873937614254038,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1456
    },
    {
      "epoch": 1.6261160714285714,
      "grad_norm": 10.804973658571797,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 1457
    },
    {
      "epoch": 1.6272321428571428,
      "grad_norm": 9.296202554886131,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1458
    },
    {
      "epoch": 1.6283482142857144,
      "grad_norm": 9.172746619827555,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1459
    },
    {
      "epoch": 1.6294642857142856,
      "grad_norm": 10.827993653419353,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1460
    },
    {
      "epoch": 1.6305803571428572,
      "grad_norm": 10.18924581128832,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 1461
    },
    {
      "epoch": 1.6316964285714286,
      "grad_norm": 9.86582123332929,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1462
    },
    {
      "epoch": 1.6328125,
      "grad_norm": 7.7278268731600575,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1463
    },
    {
      "epoch": 1.6339285714285714,
      "grad_norm": 10.540990019458526,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1464
    },
    {
      "epoch": 1.6350446428571428,
      "grad_norm": 9.88449852242838,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1465
    },
    {
      "epoch": 1.6361607142857144,
      "grad_norm": 10.84407272738642,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 1466
    },
    {
      "epoch": 1.6372767857142856,
      "grad_norm": 9.583963783484819,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 1467
    },
    {
      "epoch": 1.6383928571428572,
      "grad_norm": 10.195317508265969,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1468
    },
    {
      "epoch": 1.6395089285714286,
      "grad_norm": 10.393428961421769,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1469
    },
    {
      "epoch": 1.640625,
      "grad_norm": 9.257461479246551,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1470
    },
    {
      "epoch": 1.6417410714285714,
      "grad_norm": 8.81021040365599,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1471
    },
    {
      "epoch": 1.6428571428571428,
      "grad_norm": 9.50218978026321,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1472
    },
    {
      "epoch": 1.6439732142857144,
      "grad_norm": 11.75096703828102,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1473
    },
    {
      "epoch": 1.6450892857142856,
      "grad_norm": 9.411958872615486,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1474
    },
    {
      "epoch": 1.6462053571428572,
      "grad_norm": 10.760741682943815,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1475
    },
    {
      "epoch": 1.6473214285714286,
      "grad_norm": 10.20473688165343,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1476
    },
    {
      "epoch": 1.6484375,
      "grad_norm": 9.976419189783183,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1477
    },
    {
      "epoch": 1.6495535714285714,
      "grad_norm": 8.149696286413501,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1478
    },
    {
      "epoch": 1.6506696428571428,
      "grad_norm": 10.505897991229702,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1479
    },
    {
      "epoch": 1.6517857142857144,
      "grad_norm": 6.9229638268818166,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 1480
    },
    {
      "epoch": 1.6529017857142856,
      "grad_norm": 9.329856541391585,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1481
    },
    {
      "epoch": 1.6540178571428572,
      "grad_norm": 9.163345135553094,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 1482
    },
    {
      "epoch": 1.6551339285714286,
      "grad_norm": 9.66013585130318,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1483
    },
    {
      "epoch": 1.65625,
      "grad_norm": 10.08574560912204,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1484
    },
    {
      "epoch": 1.6573660714285714,
      "grad_norm": 10.468492628865842,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1485
    },
    {
      "epoch": 1.6584821428571428,
      "grad_norm": 10.674015434915315,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 1486
    },
    {
      "epoch": 1.6595982142857144,
      "grad_norm": 8.58887065082251,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1487
    },
    {
      "epoch": 1.6607142857142856,
      "grad_norm": 8.73652306720952,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1488
    },
    {
      "epoch": 1.6618303571428572,
      "grad_norm": 8.831044986374536,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1489
    },
    {
      "epoch": 1.6629464285714286,
      "grad_norm": 10.044332822231855,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1490
    },
    {
      "epoch": 1.6640625,
      "grad_norm": 8.51243712444697,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 1491
    },
    {
      "epoch": 1.6651785714285714,
      "grad_norm": 10.107345226301298,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 1492
    },
    {
      "epoch": 1.6662946428571428,
      "grad_norm": 8.337971001323886,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1493
    },
    {
      "epoch": 1.6674107142857144,
      "grad_norm": 11.303626951832044,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 1494
    },
    {
      "epoch": 1.6685267857142856,
      "grad_norm": 9.315530569125,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1495
    },
    {
      "epoch": 1.6696428571428572,
      "grad_norm": 10.16649067990225,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1496
    },
    {
      "epoch": 1.6707589285714286,
      "grad_norm": 9.897336062227508,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 1497
    },
    {
      "epoch": 1.671875,
      "grad_norm": 10.455875599436014,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1498
    },
    {
      "epoch": 1.6729910714285714,
      "grad_norm": 8.284090277682271,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1499
    },
    {
      "epoch": 1.6741071428571428,
      "grad_norm": 10.152193948154004,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1500
    },
    {
      "epoch": 1.6752232142857144,
      "grad_norm": 10.621433481989495,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1501
    },
    {
      "epoch": 1.6763392857142856,
      "grad_norm": 8.085968044919502,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 1502
    },
    {
      "epoch": 1.6774553571428572,
      "grad_norm": 9.464612862215388,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1503
    },
    {
      "epoch": 1.6785714285714286,
      "grad_norm": 9.587155534553935,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1504
    },
    {
      "epoch": 1.6796875,
      "grad_norm": 9.55863892106362,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1505
    },
    {
      "epoch": 1.6808035714285714,
      "grad_norm": 9.814868069696766,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1506
    },
    {
      "epoch": 1.6819196428571428,
      "grad_norm": 6.8593764240852995,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1507
    },
    {
      "epoch": 1.6830357142857144,
      "grad_norm": 11.711669492900151,
      "learning_rate": 2e-05,
      "loss": 1.3047,
      "step": 1508
    },
    {
      "epoch": 1.6841517857142856,
      "grad_norm": 10.300507137427546,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1509
    },
    {
      "epoch": 1.6852678571428572,
      "grad_norm": 8.670511781808676,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 1510
    },
    {
      "epoch": 1.6863839285714286,
      "grad_norm": 9.657758409710015,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1511
    },
    {
      "epoch": 1.6875,
      "grad_norm": 7.00623982356805,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 1512
    },
    {
      "epoch": 1.6886160714285714,
      "grad_norm": 9.639097240398668,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1513
    },
    {
      "epoch": 1.6897321428571428,
      "grad_norm": 8.059257663936911,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1514
    },
    {
      "epoch": 1.6908482142857144,
      "grad_norm": 8.53953069742539,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1515
    },
    {
      "epoch": 1.6919642857142856,
      "grad_norm": 10.870223205714398,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1516
    },
    {
      "epoch": 1.6930803571428572,
      "grad_norm": 10.094956024373237,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1517
    },
    {
      "epoch": 1.6941964285714286,
      "grad_norm": 9.156690745762047,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 1518
    },
    {
      "epoch": 1.6953125,
      "grad_norm": 11.76138804696912,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 1519
    },
    {
      "epoch": 1.6964285714285714,
      "grad_norm": 10.659342655778657,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1520
    },
    {
      "epoch": 1.6975446428571428,
      "grad_norm": 7.573792116913332,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1521
    },
    {
      "epoch": 1.6986607142857144,
      "grad_norm": 11.295936274022013,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1522
    },
    {
      "epoch": 1.6997767857142856,
      "grad_norm": 9.802649937758591,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1523
    },
    {
      "epoch": 1.7008928571428572,
      "grad_norm": 11.71234458031626,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1524
    },
    {
      "epoch": 1.7020089285714286,
      "grad_norm": 10.090231904086346,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 1525
    },
    {
      "epoch": 1.703125,
      "grad_norm": 9.999926385254822,
      "learning_rate": 2e-05,
      "loss": 1.4688,
      "step": 1526
    },
    {
      "epoch": 1.7042410714285714,
      "grad_norm": 11.425452430809928,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1527
    },
    {
      "epoch": 1.7053571428571428,
      "grad_norm": 8.634454485530753,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1528
    },
    {
      "epoch": 1.7064732142857144,
      "grad_norm": 11.992402490053337,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1529
    },
    {
      "epoch": 1.7075892857142856,
      "grad_norm": 7.28372679412276,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 1530
    },
    {
      "epoch": 1.7087053571428572,
      "grad_norm": 10.162881705652747,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1531
    },
    {
      "epoch": 1.7098214285714286,
      "grad_norm": 10.0830685330925,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 1532
    },
    {
      "epoch": 1.7109375,
      "grad_norm": 7.506713039975602,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1533
    },
    {
      "epoch": 1.7120535714285714,
      "grad_norm": 10.312520459610095,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1534
    },
    {
      "epoch": 1.7131696428571428,
      "grad_norm": 9.337195485980004,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1535
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 11.523490158078772,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1536
    },
    {
      "epoch": 1.7154017857142856,
      "grad_norm": 10.658520244675378,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1537
    },
    {
      "epoch": 1.7165178571428572,
      "grad_norm": 9.524333337365045,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1538
    },
    {
      "epoch": 1.7176339285714286,
      "grad_norm": 11.138438811925717,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1539
    },
    {
      "epoch": 1.71875,
      "grad_norm": 7.903162416322438,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1540
    },
    {
      "epoch": 1.7198660714285714,
      "grad_norm": 9.370548263502164,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1541
    },
    {
      "epoch": 1.7209821428571428,
      "grad_norm": 7.813952242089537,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1542
    },
    {
      "epoch": 1.7220982142857144,
      "grad_norm": 11.50809232986242,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 1543
    },
    {
      "epoch": 1.7232142857142856,
      "grad_norm": 11.028865569688707,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1544
    },
    {
      "epoch": 1.7243303571428572,
      "grad_norm": 10.762200184960374,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1545
    },
    {
      "epoch": 1.7254464285714286,
      "grad_norm": 10.080920989546247,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1546
    },
    {
      "epoch": 1.7265625,
      "grad_norm": 10.136918817808874,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1547
    },
    {
      "epoch": 1.7276785714285714,
      "grad_norm": 8.711546305747957,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1548
    },
    {
      "epoch": 1.7287946428571428,
      "grad_norm": 11.217259501513423,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1549
    },
    {
      "epoch": 1.7299107142857144,
      "grad_norm": 7.8866690967634785,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1550
    },
    {
      "epoch": 1.7310267857142856,
      "grad_norm": 8.799561212378615,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1551
    },
    {
      "epoch": 1.7321428571428572,
      "grad_norm": 10.767253536996297,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1552
    },
    {
      "epoch": 1.7332589285714286,
      "grad_norm": 9.659188770261075,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1553
    },
    {
      "epoch": 1.734375,
      "grad_norm": 8.757451585655382,
      "learning_rate": 2e-05,
      "loss": 2.9844,
      "step": 1554
    },
    {
      "epoch": 1.7354910714285714,
      "grad_norm": 9.895290637928069,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 1555
    },
    {
      "epoch": 1.7366071428571428,
      "grad_norm": 10.317908956550177,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 1556
    },
    {
      "epoch": 1.7377232142857144,
      "grad_norm": 8.892185696846486,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1557
    },
    {
      "epoch": 1.7388392857142856,
      "grad_norm": 9.211938507234029,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1558
    },
    {
      "epoch": 1.7399553571428572,
      "grad_norm": 9.137725741947497,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1559
    },
    {
      "epoch": 1.7410714285714286,
      "grad_norm": 9.113203899692163,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1560
    },
    {
      "epoch": 1.7421875,
      "grad_norm": 9.963271403425138,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 1561
    },
    {
      "epoch": 1.7433035714285714,
      "grad_norm": 10.077116382282732,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1562
    },
    {
      "epoch": 1.7444196428571428,
      "grad_norm": 10.340914282603212,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 1563
    },
    {
      "epoch": 1.7455357142857144,
      "grad_norm": 9.838463356756959,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1564
    },
    {
      "epoch": 1.7466517857142856,
      "grad_norm": 9.587612705431498,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1565
    },
    {
      "epoch": 1.7477678571428572,
      "grad_norm": 10.430997376531623,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1566
    },
    {
      "epoch": 1.7488839285714286,
      "grad_norm": 10.991644091424158,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 1567
    },
    {
      "epoch": 1.75,
      "grad_norm": 10.304030102912575,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1568
    },
    {
      "epoch": 1.7511160714285714,
      "grad_norm": 8.488900692014889,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1569
    },
    {
      "epoch": 1.7522321428571428,
      "grad_norm": 10.09457223386765,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1570
    },
    {
      "epoch": 1.7533482142857144,
      "grad_norm": 10.86070895304293,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1571
    },
    {
      "epoch": 1.7544642857142856,
      "grad_norm": 9.742796692856764,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1572
    },
    {
      "epoch": 1.7555803571428572,
      "grad_norm": 8.989504521144076,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1573
    },
    {
      "epoch": 1.7566964285714286,
      "grad_norm": 8.88135505758998,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1574
    },
    {
      "epoch": 1.7578125,
      "grad_norm": 9.429579710957274,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1575
    },
    {
      "epoch": 1.7589285714285714,
      "grad_norm": 10.549343782424321,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1576
    },
    {
      "epoch": 1.7600446428571428,
      "grad_norm": 9.590110306827963,
      "learning_rate": 2e-05,
      "loss": 2.9062,
      "step": 1577
    },
    {
      "epoch": 1.7611607142857144,
      "grad_norm": 10.307408006120234,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1578
    },
    {
      "epoch": 1.7622767857142856,
      "grad_norm": 9.382544586659419,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1579
    },
    {
      "epoch": 1.7633928571428572,
      "grad_norm": 8.31246743621136,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 1580
    },
    {
      "epoch": 1.7645089285714286,
      "grad_norm": 7.586334378462209,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 1581
    },
    {
      "epoch": 1.765625,
      "grad_norm": 9.975486409583873,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1582
    },
    {
      "epoch": 1.7667410714285714,
      "grad_norm": 9.928501767543032,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 1583
    },
    {
      "epoch": 1.7678571428571428,
      "grad_norm": 10.172553199484938,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1584
    },
    {
      "epoch": 1.7689732142857144,
      "grad_norm": 10.572718002898002,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1585
    },
    {
      "epoch": 1.7700892857142856,
      "grad_norm": 7.811589141223647,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1586
    },
    {
      "epoch": 1.7712053571428572,
      "grad_norm": 9.76108995198616,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1587
    },
    {
      "epoch": 1.7723214285714286,
      "grad_norm": 9.966853323064576,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 1588
    },
    {
      "epoch": 1.7734375,
      "grad_norm": 10.833503303564294,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1589
    },
    {
      "epoch": 1.7745535714285714,
      "grad_norm": 11.901549944038779,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1590
    },
    {
      "epoch": 1.7756696428571428,
      "grad_norm": 12.20660603368522,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1591
    },
    {
      "epoch": 1.7767857142857144,
      "grad_norm": 11.339669107650025,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1592
    },
    {
      "epoch": 1.7779017857142856,
      "grad_norm": 9.802099355616575,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1593
    },
    {
      "epoch": 1.7790178571428572,
      "grad_norm": 9.44264324914429,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 1594
    },
    {
      "epoch": 1.7801339285714286,
      "grad_norm": 10.031245697306366,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1595
    },
    {
      "epoch": 1.78125,
      "grad_norm": 9.25010545643392,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 1596
    },
    {
      "epoch": 1.7823660714285714,
      "grad_norm": 8.077338729620717,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 1597
    },
    {
      "epoch": 1.7834821428571428,
      "grad_norm": 10.833625823613541,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1598
    },
    {
      "epoch": 1.7845982142857144,
      "grad_norm": 9.80398216438199,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1599
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 9.405221969865373,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 1600
    },
    {
      "epoch": 1.7868303571428572,
      "grad_norm": 9.859562311183602,
      "learning_rate": 2e-05,
      "loss": 1.4453,
      "step": 1601
    },
    {
      "epoch": 1.7879464285714286,
      "grad_norm": 9.86173919198722,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1602
    },
    {
      "epoch": 1.7890625,
      "grad_norm": 8.926609556476459,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1603
    },
    {
      "epoch": 1.7901785714285714,
      "grad_norm": 10.260868742385913,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 1604
    },
    {
      "epoch": 1.7912946428571428,
      "grad_norm": 10.632305267948183,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 1605
    },
    {
      "epoch": 1.7924107142857144,
      "grad_norm": 9.97627931091052,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 1606
    },
    {
      "epoch": 1.7935267857142856,
      "grad_norm": 10.755730928141354,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1607
    },
    {
      "epoch": 1.7946428571428572,
      "grad_norm": 8.78854722961166,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1608
    },
    {
      "epoch": 1.7957589285714286,
      "grad_norm": 10.535091925806332,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1609
    },
    {
      "epoch": 1.796875,
      "grad_norm": 7.885350011758516,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1610
    },
    {
      "epoch": 1.7979910714285714,
      "grad_norm": 10.733561986650319,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 1611
    },
    {
      "epoch": 1.7991071428571428,
      "grad_norm": 8.602645399337955,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1612
    },
    {
      "epoch": 1.8002232142857144,
      "grad_norm": 9.549953121297268,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1613
    },
    {
      "epoch": 1.8013392857142856,
      "grad_norm": 7.157198633711598,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1614
    },
    {
      "epoch": 1.8024553571428572,
      "grad_norm": 9.67929283449173,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 1615
    },
    {
      "epoch": 1.8035714285714286,
      "grad_norm": 8.413273812862302,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1616
    },
    {
      "epoch": 1.8046875,
      "grad_norm": 9.486725041923808,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1617
    },
    {
      "epoch": 1.8058035714285714,
      "grad_norm": 9.144409799672129,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1618
    },
    {
      "epoch": 1.8069196428571428,
      "grad_norm": 8.459607603892005,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1619
    },
    {
      "epoch": 1.8080357142857144,
      "grad_norm": 9.376265414525813,
      "learning_rate": 2e-05,
      "loss": 3.0,
      "step": 1620
    },
    {
      "epoch": 1.8091517857142856,
      "grad_norm": 11.03019296249102,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 1621
    },
    {
      "epoch": 1.8102678571428572,
      "grad_norm": 11.076514011170323,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1622
    },
    {
      "epoch": 1.8113839285714286,
      "grad_norm": 8.601728474691932,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1623
    },
    {
      "epoch": 1.8125,
      "grad_norm": 11.391650974670567,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1624
    },
    {
      "epoch": 1.8136160714285714,
      "grad_norm": 8.432808375740258,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1625
    },
    {
      "epoch": 1.8147321428571428,
      "grad_norm": 7.545789283199662,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1626
    },
    {
      "epoch": 1.8158482142857144,
      "grad_norm": 9.215908668259866,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1627
    },
    {
      "epoch": 1.8169642857142856,
      "grad_norm": 12.782585447901802,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 1628
    },
    {
      "epoch": 1.8180803571428572,
      "grad_norm": 7.983057343582754,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 1629
    },
    {
      "epoch": 1.8191964285714286,
      "grad_norm": 9.675660780172572,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1630
    },
    {
      "epoch": 1.8203125,
      "grad_norm": 10.530148374025568,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1631
    },
    {
      "epoch": 1.8214285714285714,
      "grad_norm": 8.892547878894627,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1632
    },
    {
      "epoch": 1.8225446428571428,
      "grad_norm": 12.90111891377626,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1633
    },
    {
      "epoch": 1.8236607142857144,
      "grad_norm": 9.039755729415202,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1634
    },
    {
      "epoch": 1.8247767857142856,
      "grad_norm": 10.829027949586697,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1635
    },
    {
      "epoch": 1.8258928571428572,
      "grad_norm": 9.336189165029324,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1636
    },
    {
      "epoch": 1.8270089285714286,
      "grad_norm": 10.02137680706756,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1637
    },
    {
      "epoch": 1.828125,
      "grad_norm": 11.267114921201639,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1638
    },
    {
      "epoch": 1.8292410714285714,
      "grad_norm": 8.515333252448265,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1639
    },
    {
      "epoch": 1.8303571428571428,
      "grad_norm": 8.778107881703919,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1640
    },
    {
      "epoch": 1.8314732142857144,
      "grad_norm": 9.839984574793846,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 1641
    },
    {
      "epoch": 1.8325892857142856,
      "grad_norm": 9.820930772482251,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1642
    },
    {
      "epoch": 1.8337053571428572,
      "grad_norm": 8.283303478377722,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1643
    },
    {
      "epoch": 1.8348214285714286,
      "grad_norm": 7.688819819730179,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1644
    },
    {
      "epoch": 1.8359375,
      "grad_norm": 8.321704039134598,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1645
    },
    {
      "epoch": 1.8370535714285714,
      "grad_norm": 6.792893584150218,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1646
    },
    {
      "epoch": 1.8381696428571428,
      "grad_norm": 8.876865271557234,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1647
    },
    {
      "epoch": 1.8392857142857144,
      "grad_norm": 8.54974677806238,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1648
    },
    {
      "epoch": 1.8404017857142856,
      "grad_norm": 10.295483817485769,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1649
    },
    {
      "epoch": 1.8415178571428572,
      "grad_norm": 7.2595692003141785,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1650
    },
    {
      "epoch": 1.8426339285714286,
      "grad_norm": 8.735477876043511,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1651
    },
    {
      "epoch": 1.84375,
      "grad_norm": 10.134718894848323,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1652
    },
    {
      "epoch": 1.8448660714285714,
      "grad_norm": 10.293313578994193,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 1653
    },
    {
      "epoch": 1.8459821428571428,
      "grad_norm": 12.840859422526622,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1654
    },
    {
      "epoch": 1.8470982142857144,
      "grad_norm": 8.87176459709003,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1655
    },
    {
      "epoch": 1.8482142857142856,
      "grad_norm": 11.400481393753363,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1656
    },
    {
      "epoch": 1.8493303571428572,
      "grad_norm": 10.595405763084624,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1657
    },
    {
      "epoch": 1.8504464285714286,
      "grad_norm": 10.218443060028893,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 1658
    },
    {
      "epoch": 1.8515625,
      "grad_norm": 8.66237820427389,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1659
    },
    {
      "epoch": 1.8526785714285714,
      "grad_norm": 9.904146860070998,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 1660
    },
    {
      "epoch": 1.8537946428571428,
      "grad_norm": 10.68181626323851,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1661
    },
    {
      "epoch": 1.8549107142857144,
      "grad_norm": 9.97667739959965,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1662
    },
    {
      "epoch": 1.8560267857142856,
      "grad_norm": 11.658074399588212,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1663
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 9.749817515574192,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 1664
    },
    {
      "epoch": 1.8582589285714286,
      "grad_norm": 10.775523914945083,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1665
    },
    {
      "epoch": 1.859375,
      "grad_norm": 10.378550561696924,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1666
    },
    {
      "epoch": 1.8604910714285714,
      "grad_norm": 9.840983969072857,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1667
    },
    {
      "epoch": 1.8616071428571428,
      "grad_norm": 10.970001274734262,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1668
    },
    {
      "epoch": 1.8627232142857144,
      "grad_norm": 10.618673989889645,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1669
    },
    {
      "epoch": 1.8638392857142856,
      "grad_norm": 11.252967431615822,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1670
    },
    {
      "epoch": 1.8649553571428572,
      "grad_norm": 10.371918863416644,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1671
    },
    {
      "epoch": 1.8660714285714286,
      "grad_norm": 7.8193408474023665,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1672
    },
    {
      "epoch": 1.8671875,
      "grad_norm": 9.411241055129647,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1673
    },
    {
      "epoch": 1.8683035714285714,
      "grad_norm": 9.414966131186825,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1674
    },
    {
      "epoch": 1.8694196428571428,
      "grad_norm": 8.32469282584879,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1675
    },
    {
      "epoch": 1.8705357142857144,
      "grad_norm": 9.769738852041113,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1676
    },
    {
      "epoch": 1.8716517857142856,
      "grad_norm": 10.209432651712854,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1677
    },
    {
      "epoch": 1.8727678571428572,
      "grad_norm": 9.088950228865613,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1678
    },
    {
      "epoch": 1.8738839285714286,
      "grad_norm": 9.58093991763409,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1679
    },
    {
      "epoch": 1.875,
      "grad_norm": 9.522280796463065,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1680
    },
    {
      "epoch": 1.8761160714285714,
      "grad_norm": 9.27885138737079,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1681
    },
    {
      "epoch": 1.8772321428571428,
      "grad_norm": 12.75419949017588,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 1682
    },
    {
      "epoch": 1.8783482142857144,
      "grad_norm": 10.473041109338052,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1683
    },
    {
      "epoch": 1.8794642857142856,
      "grad_norm": 10.582227072602768,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1684
    },
    {
      "epoch": 1.8805803571428572,
      "grad_norm": 10.698220684490545,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 1685
    },
    {
      "epoch": 1.8816964285714286,
      "grad_norm": 10.227876378298372,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1686
    },
    {
      "epoch": 1.8828125,
      "grad_norm": 9.610139261788499,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1687
    },
    {
      "epoch": 1.8839285714285714,
      "grad_norm": 9.23868300988373,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 1688
    },
    {
      "epoch": 1.8850446428571428,
      "grad_norm": 12.867527041740663,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1689
    },
    {
      "epoch": 1.8861607142857144,
      "grad_norm": 8.425284703708526,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1690
    },
    {
      "epoch": 1.8872767857142856,
      "grad_norm": 9.406551722117985,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1691
    },
    {
      "epoch": 1.8883928571428572,
      "grad_norm": 12.734712279568136,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1692
    },
    {
      "epoch": 1.8895089285714286,
      "grad_norm": 8.61863211435957,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1693
    },
    {
      "epoch": 1.890625,
      "grad_norm": 10.753697797632718,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 1694
    },
    {
      "epoch": 1.8917410714285714,
      "grad_norm": 7.94847463500134,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1695
    },
    {
      "epoch": 1.8928571428571428,
      "grad_norm": 8.277609361437907,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1696
    },
    {
      "epoch": 1.8939732142857144,
      "grad_norm": 10.98744064222938,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1697
    },
    {
      "epoch": 1.8950892857142856,
      "grad_norm": 9.358059402577629,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1698
    },
    {
      "epoch": 1.8962053571428572,
      "grad_norm": 11.588995942234728,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 1699
    },
    {
      "epoch": 1.8973214285714286,
      "grad_norm": 9.827513328425779,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 1700
    },
    {
      "epoch": 1.8984375,
      "grad_norm": 9.573707389165287,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1701
    },
    {
      "epoch": 1.8995535714285714,
      "grad_norm": 11.078636059926156,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1702
    },
    {
      "epoch": 1.9006696428571428,
      "grad_norm": 7.2402299665792995,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1703
    },
    {
      "epoch": 1.9017857142857144,
      "grad_norm": 7.739196831229188,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1704
    },
    {
      "epoch": 1.9029017857142856,
      "grad_norm": 7.943202105006223,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1705
    },
    {
      "epoch": 1.9040178571428572,
      "grad_norm": 11.612016318625576,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1706
    },
    {
      "epoch": 1.9051339285714286,
      "grad_norm": 9.400878010335711,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1707
    },
    {
      "epoch": 1.90625,
      "grad_norm": 8.480633339746271,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1708
    },
    {
      "epoch": 1.9073660714285714,
      "grad_norm": 7.429167107640977,
      "learning_rate": 2e-05,
      "loss": 3.0469,
      "step": 1709
    },
    {
      "epoch": 1.9084821428571428,
      "grad_norm": 10.216740441497215,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1710
    },
    {
      "epoch": 1.9095982142857144,
      "grad_norm": 9.040280626506672,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1711
    },
    {
      "epoch": 1.9107142857142856,
      "grad_norm": 8.918653834004807,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1712
    },
    {
      "epoch": 1.9118303571428572,
      "grad_norm": 9.707229224109765,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1713
    },
    {
      "epoch": 1.9129464285714286,
      "grad_norm": 7.274243803964139,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1714
    },
    {
      "epoch": 1.9140625,
      "grad_norm": 8.844013867995526,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1715
    },
    {
      "epoch": 1.9151785714285714,
      "grad_norm": 8.463725858352895,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1716
    },
    {
      "epoch": 1.9162946428571428,
      "grad_norm": 10.072204305316681,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1717
    },
    {
      "epoch": 1.9174107142857144,
      "grad_norm": 10.031945033500966,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1718
    },
    {
      "epoch": 1.9185267857142856,
      "grad_norm": 9.233492292993372,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1719
    },
    {
      "epoch": 1.9196428571428572,
      "grad_norm": 7.788690863908381,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 1720
    },
    {
      "epoch": 1.9207589285714286,
      "grad_norm": 9.0063435458262,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1721
    },
    {
      "epoch": 1.921875,
      "grad_norm": 9.781567635874245,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1722
    },
    {
      "epoch": 1.9229910714285714,
      "grad_norm": 9.285743413302525,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1723
    },
    {
      "epoch": 1.9241071428571428,
      "grad_norm": 8.343611700777373,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1724
    },
    {
      "epoch": 1.9252232142857144,
      "grad_norm": 7.782840141031084,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1725
    },
    {
      "epoch": 1.9263392857142856,
      "grad_norm": 9.68028874763218,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 1726
    },
    {
      "epoch": 1.9274553571428572,
      "grad_norm": 11.622674883006043,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1727
    },
    {
      "epoch": 1.9285714285714286,
      "grad_norm": 10.071798265496833,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1728
    },
    {
      "epoch": 1.9296875,
      "grad_norm": 8.499510282748028,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1729
    },
    {
      "epoch": 1.9308035714285714,
      "grad_norm": 9.100388623446856,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1730
    },
    {
      "epoch": 1.9319196428571428,
      "grad_norm": 9.140741682024682,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1731
    },
    {
      "epoch": 1.9330357142857144,
      "grad_norm": 10.256971612583882,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1732
    },
    {
      "epoch": 1.9341517857142856,
      "grad_norm": 9.03840988112905,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1733
    },
    {
      "epoch": 1.9352678571428572,
      "grad_norm": 8.413764807816188,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1734
    },
    {
      "epoch": 1.9363839285714286,
      "grad_norm": 8.603688494282132,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1735
    },
    {
      "epoch": 1.9375,
      "grad_norm": 10.755013558586404,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 1736
    },
    {
      "epoch": 1.9386160714285714,
      "grad_norm": 10.53478208083376,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1737
    },
    {
      "epoch": 1.9397321428571428,
      "grad_norm": 9.618269394900478,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 1738
    },
    {
      "epoch": 1.9408482142857144,
      "grad_norm": 6.517698456710877,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1739
    },
    {
      "epoch": 1.9419642857142856,
      "grad_norm": 11.055972111553466,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1740
    },
    {
      "epoch": 1.9430803571428572,
      "grad_norm": 9.570950200228353,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1741
    },
    {
      "epoch": 1.9441964285714286,
      "grad_norm": 11.57738266523505,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1742
    },
    {
      "epoch": 1.9453125,
      "grad_norm": 11.82463245472156,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 1743
    },
    {
      "epoch": 1.9464285714285714,
      "grad_norm": 9.362474870567445,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1744
    },
    {
      "epoch": 1.9475446428571428,
      "grad_norm": 10.405714959488638,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1745
    },
    {
      "epoch": 1.9486607142857144,
      "grad_norm": 11.050380865691077,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1746
    },
    {
      "epoch": 1.9497767857142856,
      "grad_norm": 9.631923974468885,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1747
    },
    {
      "epoch": 1.9508928571428572,
      "grad_norm": 10.52115156232622,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1748
    },
    {
      "epoch": 1.9520089285714286,
      "grad_norm": 10.094428136831787,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1749
    },
    {
      "epoch": 1.953125,
      "grad_norm": 8.929552831293108,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1750
    },
    {
      "epoch": 1.9542410714285714,
      "grad_norm": 9.449950700103633,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1751
    },
    {
      "epoch": 1.9553571428571428,
      "grad_norm": 12.712116003173792,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1752
    },
    {
      "epoch": 1.9564732142857144,
      "grad_norm": 9.558614992170579,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1753
    },
    {
      "epoch": 1.9575892857142856,
      "grad_norm": 10.291693895589965,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1754
    },
    {
      "epoch": 1.9587053571428572,
      "grad_norm": 9.915628568849538,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1755
    },
    {
      "epoch": 1.9598214285714286,
      "grad_norm": 9.649151999552155,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1756
    },
    {
      "epoch": 1.9609375,
      "grad_norm": 9.4659274101641,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1757
    },
    {
      "epoch": 1.9620535714285714,
      "grad_norm": 8.978353629684182,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1758
    },
    {
      "epoch": 1.9631696428571428,
      "grad_norm": 9.864076764777964,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1759
    },
    {
      "epoch": 1.9642857142857144,
      "grad_norm": 11.619123723767606,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 1760
    },
    {
      "epoch": 1.9654017857142856,
      "grad_norm": 11.238611213800137,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 1761
    },
    {
      "epoch": 1.9665178571428572,
      "grad_norm": 7.036616567696682,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 1762
    },
    {
      "epoch": 1.9676339285714286,
      "grad_norm": 10.211430045734723,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1763
    },
    {
      "epoch": 1.96875,
      "grad_norm": 10.23651228457571,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1764
    },
    {
      "epoch": 1.9698660714285714,
      "grad_norm": 9.845849141032541,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1765
    },
    {
      "epoch": 1.9709821428571428,
      "grad_norm": 8.247950448304659,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1766
    },
    {
      "epoch": 1.9720982142857144,
      "grad_norm": 10.095508445837714,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1767
    },
    {
      "epoch": 1.9732142857142856,
      "grad_norm": 10.082328117176965,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1768
    },
    {
      "epoch": 1.9743303571428572,
      "grad_norm": 10.698613653180322,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1769
    },
    {
      "epoch": 1.9754464285714286,
      "grad_norm": 10.394831828865932,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1770
    },
    {
      "epoch": 1.9765625,
      "grad_norm": 9.255084086526033,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1771
    },
    {
      "epoch": 1.9776785714285714,
      "grad_norm": 10.411039251310347,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1772
    },
    {
      "epoch": 1.9787946428571428,
      "grad_norm": 7.752771367261172,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1773
    },
    {
      "epoch": 1.9799107142857144,
      "grad_norm": 8.567242487843156,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1774
    },
    {
      "epoch": 1.9810267857142856,
      "grad_norm": 10.54959135736178,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 1775
    },
    {
      "epoch": 1.9821428571428572,
      "grad_norm": 11.134362021177067,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1776
    },
    {
      "epoch": 1.9832589285714286,
      "grad_norm": 11.22117008393948,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1777
    },
    {
      "epoch": 1.984375,
      "grad_norm": 10.29937583208718,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1778
    },
    {
      "epoch": 1.9854910714285714,
      "grad_norm": 10.238587030926682,
      "learning_rate": 2e-05,
      "loss": 1.3438,
      "step": 1779
    },
    {
      "epoch": 1.9866071428571428,
      "grad_norm": 8.37012970597976,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1780
    },
    {
      "epoch": 1.9877232142857144,
      "grad_norm": 9.651044501444947,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1781
    },
    {
      "epoch": 1.9888392857142856,
      "grad_norm": 11.40789594606158,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1782
    },
    {
      "epoch": 1.9899553571428572,
      "grad_norm": 12.091061993545628,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1783
    },
    {
      "epoch": 1.9910714285714286,
      "grad_norm": 9.713484162986596,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1784
    },
    {
      "epoch": 1.9921875,
      "grad_norm": 12.424716400955532,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1785
    },
    {
      "epoch": 1.9933035714285714,
      "grad_norm": 9.68039652522805,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 1786
    },
    {
      "epoch": 1.9944196428571428,
      "grad_norm": 9.857936340554588,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1787
    },
    {
      "epoch": 1.9955357142857144,
      "grad_norm": 7.558957714233424,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1788
    },
    {
      "epoch": 1.9966517857142856,
      "grad_norm": 9.981136234333295,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1789
    },
    {
      "epoch": 1.9977678571428572,
      "grad_norm": 9.150995893452588,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1790
    },
    {
      "epoch": 1.9988839285714286,
      "grad_norm": 12.592409409825034,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1791
    },
    {
      "epoch": 2.0,
      "grad_norm": 12.106029582937747,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1792
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 3584,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "total_flos": 34815004901376.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
