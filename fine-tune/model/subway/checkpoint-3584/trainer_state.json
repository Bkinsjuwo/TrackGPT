{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 3584,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0011160714285714285,
      "grad_norm": 8.714164174460775,
      "learning_rate": 2e-05,
      "loss": 3.2969,
      "step": 1
    },
    {
      "epoch": 0.002232142857142857,
      "grad_norm": 8.128486322384092,
      "learning_rate": 2e-05,
      "loss": 3.3594,
      "step": 2
    },
    {
      "epoch": 0.0033482142857142855,
      "grad_norm": 11.915371164647565,
      "learning_rate": 2e-05,
      "loss": 3.1719,
      "step": 3
    },
    {
      "epoch": 0.004464285714285714,
      "grad_norm": 11.408181156121259,
      "learning_rate": 2e-05,
      "loss": 3.1719,
      "step": 4
    },
    {
      "epoch": 0.005580357142857143,
      "grad_norm": 7.410388430792192,
      "learning_rate": 2e-05,
      "loss": 3.2656,
      "step": 5
    },
    {
      "epoch": 0.006696428571428571,
      "grad_norm": 10.210658826761904,
      "learning_rate": 2e-05,
      "loss": 3.4844,
      "step": 6
    },
    {
      "epoch": 0.0078125,
      "grad_norm": 6.2256575973149095,
      "learning_rate": 2e-05,
      "loss": 3.5312,
      "step": 7
    },
    {
      "epoch": 0.008928571428571428,
      "grad_norm": 8.226034935319733,
      "learning_rate": 2e-05,
      "loss": 3.0781,
      "step": 8
    },
    {
      "epoch": 0.010044642857142858,
      "grad_norm": 8.017552136568476,
      "learning_rate": 2e-05,
      "loss": 3.4219,
      "step": 9
    },
    {
      "epoch": 0.011160714285714286,
      "grad_norm": 9.580740289235301,
      "learning_rate": 2e-05,
      "loss": 3.6406,
      "step": 10
    },
    {
      "epoch": 0.012276785714285714,
      "grad_norm": 6.580472320930182,
      "learning_rate": 2e-05,
      "loss": 3.7969,
      "step": 11
    },
    {
      "epoch": 0.013392857142857142,
      "grad_norm": 8.950205081738448,
      "learning_rate": 2e-05,
      "loss": 3.7188,
      "step": 12
    },
    {
      "epoch": 0.014508928571428572,
      "grad_norm": 9.249130772894745,
      "learning_rate": 2e-05,
      "loss": 2.9531,
      "step": 13
    },
    {
      "epoch": 0.015625,
      "grad_norm": 12.141656431544279,
      "learning_rate": 2e-05,
      "loss": 3.0781,
      "step": 14
    },
    {
      "epoch": 0.016741071428571428,
      "grad_norm": 33.93048922623467,
      "learning_rate": 2e-05,
      "loss": 3.3125,
      "step": 15
    },
    {
      "epoch": 0.017857142857142856,
      "grad_norm": 9.337238368722058,
      "learning_rate": 2e-05,
      "loss": 3.1562,
      "step": 16
    },
    {
      "epoch": 0.018973214285714284,
      "grad_norm": 11.029510825327677,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 17
    },
    {
      "epoch": 0.020089285714285716,
      "grad_norm": 11.70375218990609,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 18
    },
    {
      "epoch": 0.021205357142857144,
      "grad_norm": 7.214853201256005,
      "learning_rate": 2e-05,
      "loss": 2.9531,
      "step": 19
    },
    {
      "epoch": 0.022321428571428572,
      "grad_norm": 6.79025276963439,
      "learning_rate": 2e-05,
      "loss": 3.0312,
      "step": 20
    },
    {
      "epoch": 0.0234375,
      "grad_norm": 6.24740592895989,
      "learning_rate": 2e-05,
      "loss": 3.1094,
      "step": 21
    },
    {
      "epoch": 0.024553571428571428,
      "grad_norm": 10.375773543428478,
      "learning_rate": 2e-05,
      "loss": 3.3906,
      "step": 22
    },
    {
      "epoch": 0.025669642857142856,
      "grad_norm": 9.753379612530864,
      "learning_rate": 2e-05,
      "loss": 3.3125,
      "step": 23
    },
    {
      "epoch": 0.026785714285714284,
      "grad_norm": 13.487060172979637,
      "learning_rate": 2e-05,
      "loss": 3.125,
      "step": 24
    },
    {
      "epoch": 0.027901785714285716,
      "grad_norm": 10.482803374502716,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 25
    },
    {
      "epoch": 0.029017857142857144,
      "grad_norm": 12.323622600485368,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 26
    },
    {
      "epoch": 0.030133928571428572,
      "grad_norm": 7.732238438481504,
      "learning_rate": 2e-05,
      "loss": 3.6875,
      "step": 27
    },
    {
      "epoch": 0.03125,
      "grad_norm": 14.585271907120244,
      "learning_rate": 2e-05,
      "loss": 3.2656,
      "step": 28
    },
    {
      "epoch": 0.03236607142857143,
      "grad_norm": 10.060856015814013,
      "learning_rate": 2e-05,
      "loss": 3.0156,
      "step": 29
    },
    {
      "epoch": 0.033482142857142856,
      "grad_norm": 10.638996642140716,
      "learning_rate": 2e-05,
      "loss": 2.9062,
      "step": 30
    },
    {
      "epoch": 0.03459821428571429,
      "grad_norm": 5.682474307743042,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 31
    },
    {
      "epoch": 0.03571428571428571,
      "grad_norm": 8.462105780228269,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 32
    },
    {
      "epoch": 0.036830357142857144,
      "grad_norm": 12.9593568887898,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 33
    },
    {
      "epoch": 0.03794642857142857,
      "grad_norm": 9.766670242434621,
      "learning_rate": 2e-05,
      "loss": 3.4375,
      "step": 34
    },
    {
      "epoch": 0.0390625,
      "grad_norm": 8.68065181492996,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 35
    },
    {
      "epoch": 0.04017857142857143,
      "grad_norm": 6.641247307035858,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 36
    },
    {
      "epoch": 0.041294642857142856,
      "grad_norm": 6.861348755022539,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 37
    },
    {
      "epoch": 0.04241071428571429,
      "grad_norm": 5.900988240530282,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 38
    },
    {
      "epoch": 0.04352678571428571,
      "grad_norm": 9.62879180130691,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 39
    },
    {
      "epoch": 0.044642857142857144,
      "grad_norm": 10.069632475943388,
      "learning_rate": 2e-05,
      "loss": 3.0156,
      "step": 40
    },
    {
      "epoch": 0.04575892857142857,
      "grad_norm": 6.399836160670785,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 41
    },
    {
      "epoch": 0.046875,
      "grad_norm": 8.154120603779646,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 42
    },
    {
      "epoch": 0.04799107142857143,
      "grad_norm": 6.540864545099026,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 43
    },
    {
      "epoch": 0.049107142857142856,
      "grad_norm": 6.179297670838394,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 44
    },
    {
      "epoch": 0.05022321428571429,
      "grad_norm": 5.9164193745863045,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 45
    },
    {
      "epoch": 0.05133928571428571,
      "grad_norm": 5.213136646124924,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 46
    },
    {
      "epoch": 0.052455357142857144,
      "grad_norm": 7.581984144528733,
      "learning_rate": 2e-05,
      "loss": 2.8438,
      "step": 47
    },
    {
      "epoch": 0.05357142857142857,
      "grad_norm": 6.2324928560410315,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 48
    },
    {
      "epoch": 0.0546875,
      "grad_norm": 4.661068382685757,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 49
    },
    {
      "epoch": 0.05580357142857143,
      "grad_norm": 5.733918148085847,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 50
    },
    {
      "epoch": 0.056919642857142856,
      "grad_norm": 6.1210780790092345,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 51
    },
    {
      "epoch": 0.05803571428571429,
      "grad_norm": 6.129837412297985,
      "learning_rate": 2e-05,
      "loss": 2.9844,
      "step": 52
    },
    {
      "epoch": 0.05915178571428571,
      "grad_norm": 6.93503430350704,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 53
    },
    {
      "epoch": 0.060267857142857144,
      "grad_norm": 12.691960155294792,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 54
    },
    {
      "epoch": 0.06138392857142857,
      "grad_norm": 5.005629787320814,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 55
    },
    {
      "epoch": 0.0625,
      "grad_norm": 5.651187530245846,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 56
    },
    {
      "epoch": 0.06361607142857142,
      "grad_norm": 4.038033895067932,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 57
    },
    {
      "epoch": 0.06473214285714286,
      "grad_norm": 6.438456401260694,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 58
    },
    {
      "epoch": 0.06584821428571429,
      "grad_norm": 5.745731504927555,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 59
    },
    {
      "epoch": 0.06696428571428571,
      "grad_norm": 6.841252472633887,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 60
    },
    {
      "epoch": 0.06808035714285714,
      "grad_norm": 5.997203136648066,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 61
    },
    {
      "epoch": 0.06919642857142858,
      "grad_norm": 5.587411571079513,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 62
    },
    {
      "epoch": 0.0703125,
      "grad_norm": 6.187783477094426,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 63
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 5.848047770729405,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 64
    },
    {
      "epoch": 0.07254464285714286,
      "grad_norm": 4.364303256188071,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 65
    },
    {
      "epoch": 0.07366071428571429,
      "grad_norm": 5.894046035424198,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 66
    },
    {
      "epoch": 0.07477678571428571,
      "grad_norm": 4.188722537348376,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 67
    },
    {
      "epoch": 0.07589285714285714,
      "grad_norm": 5.338077657329879,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 68
    },
    {
      "epoch": 0.07700892857142858,
      "grad_norm": 6.432186512205561,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 69
    },
    {
      "epoch": 0.078125,
      "grad_norm": 5.355334501508085,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 70
    },
    {
      "epoch": 0.07924107142857142,
      "grad_norm": 6.336103241273201,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 71
    },
    {
      "epoch": 0.08035714285714286,
      "grad_norm": 5.459706147832072,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 72
    },
    {
      "epoch": 0.08147321428571429,
      "grad_norm": 4.1566987094408585,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 73
    },
    {
      "epoch": 0.08258928571428571,
      "grad_norm": 5.42232549814063,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 74
    },
    {
      "epoch": 0.08370535714285714,
      "grad_norm": 4.616704349550249,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 75
    },
    {
      "epoch": 0.08482142857142858,
      "grad_norm": 5.170182091793704,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 76
    },
    {
      "epoch": 0.0859375,
      "grad_norm": 7.5025892903524865,
      "learning_rate": 2e-05,
      "loss": 2.8281,
      "step": 77
    },
    {
      "epoch": 0.08705357142857142,
      "grad_norm": 5.8911760382637555,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 78
    },
    {
      "epoch": 0.08816964285714286,
      "grad_norm": 4.211172336958261,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 79
    },
    {
      "epoch": 0.08928571428571429,
      "grad_norm": 4.876182123666695,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 80
    },
    {
      "epoch": 0.09040178571428571,
      "grad_norm": 4.880607340204337,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 81
    },
    {
      "epoch": 0.09151785714285714,
      "grad_norm": 5.675387226031149,
      "learning_rate": 2e-05,
      "loss": 2.9219,
      "step": 82
    },
    {
      "epoch": 0.09263392857142858,
      "grad_norm": 4.822244926498086,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 83
    },
    {
      "epoch": 0.09375,
      "grad_norm": 5.246993994638488,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 84
    },
    {
      "epoch": 0.09486607142857142,
      "grad_norm": 5.365437669016616,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 85
    },
    {
      "epoch": 0.09598214285714286,
      "grad_norm": 4.693012149991979,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 86
    },
    {
      "epoch": 0.09709821428571429,
      "grad_norm": 5.449327693418202,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 87
    },
    {
      "epoch": 0.09821428571428571,
      "grad_norm": 4.885869174488196,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 88
    },
    {
      "epoch": 0.09933035714285714,
      "grad_norm": 4.919588518513742,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 89
    },
    {
      "epoch": 0.10044642857142858,
      "grad_norm": 5.011515684358079,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 90
    },
    {
      "epoch": 0.1015625,
      "grad_norm": 5.192746392384502,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 91
    },
    {
      "epoch": 0.10267857142857142,
      "grad_norm": 5.744993884964599,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 92
    },
    {
      "epoch": 0.10379464285714286,
      "grad_norm": 3.800407046105842,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 93
    },
    {
      "epoch": 0.10491071428571429,
      "grad_norm": 4.238044802514492,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 94
    },
    {
      "epoch": 0.10602678571428571,
      "grad_norm": 5.07951714597911,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 95
    },
    {
      "epoch": 0.10714285714285714,
      "grad_norm": 4.780198280740559,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 96
    },
    {
      "epoch": 0.10825892857142858,
      "grad_norm": 4.272564684212395,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 97
    },
    {
      "epoch": 0.109375,
      "grad_norm": 4.726894705469157,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 98
    },
    {
      "epoch": 0.11049107142857142,
      "grad_norm": 4.96539974104562,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 99
    },
    {
      "epoch": 0.11160714285714286,
      "grad_norm": 5.043302341718162,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 100
    },
    {
      "epoch": 0.11272321428571429,
      "grad_norm": 4.547819817378021,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 101
    },
    {
      "epoch": 0.11383928571428571,
      "grad_norm": 5.327671400186427,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 102
    },
    {
      "epoch": 0.11495535714285714,
      "grad_norm": 5.012937991854735,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 103
    },
    {
      "epoch": 0.11607142857142858,
      "grad_norm": 4.340149108887595,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 104
    },
    {
      "epoch": 0.1171875,
      "grad_norm": 4.956152466926903,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 105
    },
    {
      "epoch": 0.11830357142857142,
      "grad_norm": 4.785804944126185,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 106
    },
    {
      "epoch": 0.11941964285714286,
      "grad_norm": 4.431269369599007,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 107
    },
    {
      "epoch": 0.12053571428571429,
      "grad_norm": 4.7913980866313395,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 108
    },
    {
      "epoch": 0.12165178571428571,
      "grad_norm": 5.924198309153987,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 109
    },
    {
      "epoch": 0.12276785714285714,
      "grad_norm": 5.377041684494544,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 110
    },
    {
      "epoch": 0.12388392857142858,
      "grad_norm": 5.358904182996805,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 111
    },
    {
      "epoch": 0.125,
      "grad_norm": 4.979890590026488,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 112
    },
    {
      "epoch": 0.12611607142857142,
      "grad_norm": 5.186079196422482,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 113
    },
    {
      "epoch": 0.12723214285714285,
      "grad_norm": 5.129254376263673,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 114
    },
    {
      "epoch": 0.12834821428571427,
      "grad_norm": 4.767729206516721,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 115
    },
    {
      "epoch": 0.12946428571428573,
      "grad_norm": 5.573094659249564,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 116
    },
    {
      "epoch": 0.13058035714285715,
      "grad_norm": 5.347652091324301,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 117
    },
    {
      "epoch": 0.13169642857142858,
      "grad_norm": 5.421429622860902,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 118
    },
    {
      "epoch": 0.1328125,
      "grad_norm": 4.752026631762671,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 119
    },
    {
      "epoch": 0.13392857142857142,
      "grad_norm": 5.974699339998891,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 120
    },
    {
      "epoch": 0.13504464285714285,
      "grad_norm": 5.381149135432819,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 121
    },
    {
      "epoch": 0.13616071428571427,
      "grad_norm": 5.067710492577204,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 122
    },
    {
      "epoch": 0.13727678571428573,
      "grad_norm": 4.729334265765399,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 123
    },
    {
      "epoch": 0.13839285714285715,
      "grad_norm": 4.630528978500444,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 124
    },
    {
      "epoch": 0.13950892857142858,
      "grad_norm": 4.720231090368908,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 125
    },
    {
      "epoch": 0.140625,
      "grad_norm": 5.864592231775595,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 126
    },
    {
      "epoch": 0.14174107142857142,
      "grad_norm": 5.55009570482248,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 127
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 4.77610964730505,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 128
    },
    {
      "epoch": 0.14397321428571427,
      "grad_norm": 5.353534214215445,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 129
    },
    {
      "epoch": 0.14508928571428573,
      "grad_norm": 6.0802820928348424,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 130
    },
    {
      "epoch": 0.14620535714285715,
      "grad_norm": 6.287590694294681,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 131
    },
    {
      "epoch": 0.14732142857142858,
      "grad_norm": 4.910417236773405,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 132
    },
    {
      "epoch": 0.1484375,
      "grad_norm": 4.464446508360941,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 133
    },
    {
      "epoch": 0.14955357142857142,
      "grad_norm": 5.040095053261613,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 134
    },
    {
      "epoch": 0.15066964285714285,
      "grad_norm": 4.561919707363219,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 135
    },
    {
      "epoch": 0.15178571428571427,
      "grad_norm": 4.733170255165138,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 136
    },
    {
      "epoch": 0.15290178571428573,
      "grad_norm": 6.65344704806108,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 137
    },
    {
      "epoch": 0.15401785714285715,
      "grad_norm": 5.356209447932266,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 138
    },
    {
      "epoch": 0.15513392857142858,
      "grad_norm": 5.3580387456693055,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 139
    },
    {
      "epoch": 0.15625,
      "grad_norm": 5.569688087950964,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 140
    },
    {
      "epoch": 0.15736607142857142,
      "grad_norm": 5.824046549502556,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 141
    },
    {
      "epoch": 0.15848214285714285,
      "grad_norm": 4.977314491783535,
      "learning_rate": 2e-05,
      "loss": 2.8906,
      "step": 142
    },
    {
      "epoch": 0.15959821428571427,
      "grad_norm": 5.063110574174531,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 143
    },
    {
      "epoch": 0.16071428571428573,
      "grad_norm": 6.339391661897655,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 144
    },
    {
      "epoch": 0.16183035714285715,
      "grad_norm": 4.582333281181453,
      "learning_rate": 2e-05,
      "loss": 2.9688,
      "step": 145
    },
    {
      "epoch": 0.16294642857142858,
      "grad_norm": 5.367377203244943,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 146
    },
    {
      "epoch": 0.1640625,
      "grad_norm": 6.680092903686295,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 147
    },
    {
      "epoch": 0.16517857142857142,
      "grad_norm": 4.472201453539309,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 148
    },
    {
      "epoch": 0.16629464285714285,
      "grad_norm": 4.642459438092517,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 149
    },
    {
      "epoch": 0.16741071428571427,
      "grad_norm": 4.61954425833527,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 150
    },
    {
      "epoch": 0.16852678571428573,
      "grad_norm": 5.9386776620981365,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 151
    },
    {
      "epoch": 0.16964285714285715,
      "grad_norm": 6.084532124228088,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 152
    },
    {
      "epoch": 0.17075892857142858,
      "grad_norm": 4.087681681776893,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 153
    },
    {
      "epoch": 0.171875,
      "grad_norm": 6.224213591288618,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 154
    },
    {
      "epoch": 0.17299107142857142,
      "grad_norm": 4.368275464215769,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 155
    },
    {
      "epoch": 0.17410714285714285,
      "grad_norm": 4.596235125350103,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 156
    },
    {
      "epoch": 0.17522321428571427,
      "grad_norm": 4.104998762834547,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 157
    },
    {
      "epoch": 0.17633928571428573,
      "grad_norm": 4.911848926113566,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 158
    },
    {
      "epoch": 0.17745535714285715,
      "grad_norm": 4.9981036728800925,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 159
    },
    {
      "epoch": 0.17857142857142858,
      "grad_norm": 5.510851467998093,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 160
    },
    {
      "epoch": 0.1796875,
      "grad_norm": 5.139795591294709,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 161
    },
    {
      "epoch": 0.18080357142857142,
      "grad_norm": 4.437112357067863,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 162
    },
    {
      "epoch": 0.18191964285714285,
      "grad_norm": 5.690554318954808,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 163
    },
    {
      "epoch": 0.18303571428571427,
      "grad_norm": 5.34529343117522,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 164
    },
    {
      "epoch": 0.18415178571428573,
      "grad_norm": 4.872261742232438,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 165
    },
    {
      "epoch": 0.18526785714285715,
      "grad_norm": 4.157230479263477,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 166
    },
    {
      "epoch": 0.18638392857142858,
      "grad_norm": 4.349506969868449,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 167
    },
    {
      "epoch": 0.1875,
      "grad_norm": 4.497870585174852,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 168
    },
    {
      "epoch": 0.18861607142857142,
      "grad_norm": 5.917971395374744,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 169
    },
    {
      "epoch": 0.18973214285714285,
      "grad_norm": 5.802441588850042,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 170
    },
    {
      "epoch": 0.19084821428571427,
      "grad_norm": 5.316338204099572,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 171
    },
    {
      "epoch": 0.19196428571428573,
      "grad_norm": 4.6337257817105915,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 172
    },
    {
      "epoch": 0.19308035714285715,
      "grad_norm": 5.544854809139442,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 173
    },
    {
      "epoch": 0.19419642857142858,
      "grad_norm": 5.0812840280191285,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 174
    },
    {
      "epoch": 0.1953125,
      "grad_norm": 5.007017664080652,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 175
    },
    {
      "epoch": 0.19642857142857142,
      "grad_norm": 5.987366589781244,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 176
    },
    {
      "epoch": 0.19754464285714285,
      "grad_norm": 4.721525528452891,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 177
    },
    {
      "epoch": 0.19866071428571427,
      "grad_norm": 5.019483872417742,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 178
    },
    {
      "epoch": 0.19977678571428573,
      "grad_norm": 4.697501312780749,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 179
    },
    {
      "epoch": 0.20089285714285715,
      "grad_norm": 6.046201115248955,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 180
    },
    {
      "epoch": 0.20200892857142858,
      "grad_norm": 5.910415472171007,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 181
    },
    {
      "epoch": 0.203125,
      "grad_norm": 5.473306389087776,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 182
    },
    {
      "epoch": 0.20424107142857142,
      "grad_norm": 4.565682727749405,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 183
    },
    {
      "epoch": 0.20535714285714285,
      "grad_norm": 6.2328227238187806,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 184
    },
    {
      "epoch": 0.20647321428571427,
      "grad_norm": 5.93364014209044,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 185
    },
    {
      "epoch": 0.20758928571428573,
      "grad_norm": 5.318288597782362,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 186
    },
    {
      "epoch": 0.20870535714285715,
      "grad_norm": 9.244094239583756,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 187
    },
    {
      "epoch": 0.20982142857142858,
      "grad_norm": 5.2493517713917885,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 188
    },
    {
      "epoch": 0.2109375,
      "grad_norm": 4.718023847907817,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 189
    },
    {
      "epoch": 0.21205357142857142,
      "grad_norm": 5.1117706451353495,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 190
    },
    {
      "epoch": 0.21316964285714285,
      "grad_norm": 6.5400312346390415,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 191
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 4.622534073536627,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 192
    },
    {
      "epoch": 0.21540178571428573,
      "grad_norm": 4.9784670189788365,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 193
    },
    {
      "epoch": 0.21651785714285715,
      "grad_norm": 5.326543381816517,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 194
    },
    {
      "epoch": 0.21763392857142858,
      "grad_norm": 4.283091419725382,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 195
    },
    {
      "epoch": 0.21875,
      "grad_norm": 4.648454954980838,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 196
    },
    {
      "epoch": 0.21986607142857142,
      "grad_norm": 4.725599795986592,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 197
    },
    {
      "epoch": 0.22098214285714285,
      "grad_norm": 4.9415366236178055,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 198
    },
    {
      "epoch": 0.22209821428571427,
      "grad_norm": 5.543879086202631,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 199
    },
    {
      "epoch": 0.22321428571428573,
      "grad_norm": 4.6114693631699915,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 200
    },
    {
      "epoch": 0.22433035714285715,
      "grad_norm": 4.455777071656654,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 201
    },
    {
      "epoch": 0.22544642857142858,
      "grad_norm": 5.426585916659562,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 202
    },
    {
      "epoch": 0.2265625,
      "grad_norm": 5.37508598796026,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 203
    },
    {
      "epoch": 0.22767857142857142,
      "grad_norm": 4.504463553703966,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 204
    },
    {
      "epoch": 0.22879464285714285,
      "grad_norm": 5.632933312255413,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 205
    },
    {
      "epoch": 0.22991071428571427,
      "grad_norm": 4.820290487959167,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 206
    },
    {
      "epoch": 0.23102678571428573,
      "grad_norm": 5.0591413578384525,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 207
    },
    {
      "epoch": 0.23214285714285715,
      "grad_norm": 4.90720053919676,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 208
    },
    {
      "epoch": 0.23325892857142858,
      "grad_norm": 5.584408987152432,
      "learning_rate": 2e-05,
      "loss": 2.9531,
      "step": 209
    },
    {
      "epoch": 0.234375,
      "grad_norm": 6.483344982186383,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 210
    },
    {
      "epoch": 0.23549107142857142,
      "grad_norm": 4.891662989812903,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 211
    },
    {
      "epoch": 0.23660714285714285,
      "grad_norm": 4.715300941994764,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 212
    },
    {
      "epoch": 0.23772321428571427,
      "grad_norm": 4.917011966533966,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 213
    },
    {
      "epoch": 0.23883928571428573,
      "grad_norm": 5.344035610548399,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 214
    },
    {
      "epoch": 0.23995535714285715,
      "grad_norm": 5.841296825708515,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 215
    },
    {
      "epoch": 0.24107142857142858,
      "grad_norm": 5.567403122420545,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 216
    },
    {
      "epoch": 0.2421875,
      "grad_norm": 7.110422885257681,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 217
    },
    {
      "epoch": 0.24330357142857142,
      "grad_norm": 5.8911103376815355,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 218
    },
    {
      "epoch": 0.24441964285714285,
      "grad_norm": 5.159853234572748,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 219
    },
    {
      "epoch": 0.24553571428571427,
      "grad_norm": 4.25010334015838,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 220
    },
    {
      "epoch": 0.24665178571428573,
      "grad_norm": 5.926599948903188,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 221
    },
    {
      "epoch": 0.24776785714285715,
      "grad_norm": 5.114772930689475,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 222
    },
    {
      "epoch": 0.24888392857142858,
      "grad_norm": 5.616855086548894,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 223
    },
    {
      "epoch": 0.25,
      "grad_norm": 4.884625570370895,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 224
    },
    {
      "epoch": 0.25111607142857145,
      "grad_norm": 6.505135334760109,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 225
    },
    {
      "epoch": 0.25223214285714285,
      "grad_norm": 7.250049962995516,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 226
    },
    {
      "epoch": 0.2533482142857143,
      "grad_norm": 5.640007119784958,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 227
    },
    {
      "epoch": 0.2544642857142857,
      "grad_norm": 4.8684104062266265,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 228
    },
    {
      "epoch": 0.25558035714285715,
      "grad_norm": 5.246299659371866,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 229
    },
    {
      "epoch": 0.25669642857142855,
      "grad_norm": 6.197864698442289,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 230
    },
    {
      "epoch": 0.2578125,
      "grad_norm": 5.156051058484374,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 231
    },
    {
      "epoch": 0.25892857142857145,
      "grad_norm": 5.42501117012951,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 232
    },
    {
      "epoch": 0.26004464285714285,
      "grad_norm": 6.101554902984205,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 233
    },
    {
      "epoch": 0.2611607142857143,
      "grad_norm": 4.673240223255446,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 234
    },
    {
      "epoch": 0.2622767857142857,
      "grad_norm": 5.682365722933665,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 235
    },
    {
      "epoch": 0.26339285714285715,
      "grad_norm": 5.92508714921797,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 236
    },
    {
      "epoch": 0.26450892857142855,
      "grad_norm": 5.527002715594196,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 237
    },
    {
      "epoch": 0.265625,
      "grad_norm": 4.861116207642927,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 238
    },
    {
      "epoch": 0.26674107142857145,
      "grad_norm": 4.9088171632362085,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 239
    },
    {
      "epoch": 0.26785714285714285,
      "grad_norm": 5.855620912498212,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 240
    },
    {
      "epoch": 0.2689732142857143,
      "grad_norm": 5.0733099057221684,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 241
    },
    {
      "epoch": 0.2700892857142857,
      "grad_norm": 5.989621966709335,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 242
    },
    {
      "epoch": 0.27120535714285715,
      "grad_norm": 4.982177302652287,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 243
    },
    {
      "epoch": 0.27232142857142855,
      "grad_norm": 5.222638222315441,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 244
    },
    {
      "epoch": 0.2734375,
      "grad_norm": 5.154905593698026,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 245
    },
    {
      "epoch": 0.27455357142857145,
      "grad_norm": 6.035031319020624,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 246
    },
    {
      "epoch": 0.27566964285714285,
      "grad_norm": 4.8919104357200816,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 247
    },
    {
      "epoch": 0.2767857142857143,
      "grad_norm": 3.995860097179417,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 248
    },
    {
      "epoch": 0.2779017857142857,
      "grad_norm": 4.607390122853896,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 249
    },
    {
      "epoch": 0.27901785714285715,
      "grad_norm": 4.394756002697058,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 250
    },
    {
      "epoch": 0.28013392857142855,
      "grad_norm": 4.581849334008666,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 251
    },
    {
      "epoch": 0.28125,
      "grad_norm": 4.570974643313947,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 252
    },
    {
      "epoch": 0.28236607142857145,
      "grad_norm": 5.2103735682870616,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 253
    },
    {
      "epoch": 0.28348214285714285,
      "grad_norm": 6.02447386620731,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 254
    },
    {
      "epoch": 0.2845982142857143,
      "grad_norm": 4.995303310974033,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 255
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 7.167986441114688,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 256
    },
    {
      "epoch": 0.28683035714285715,
      "grad_norm": 6.524993932244898,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 257
    },
    {
      "epoch": 0.28794642857142855,
      "grad_norm": 4.578586452874299,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 258
    },
    {
      "epoch": 0.2890625,
      "grad_norm": 5.738987692650044,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 259
    },
    {
      "epoch": 0.29017857142857145,
      "grad_norm": 5.721515767690259,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 260
    },
    {
      "epoch": 0.29129464285714285,
      "grad_norm": 5.5721865096830046,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 261
    },
    {
      "epoch": 0.2924107142857143,
      "grad_norm": 5.65983154115369,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 262
    },
    {
      "epoch": 0.2935267857142857,
      "grad_norm": 5.837739419844417,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 263
    },
    {
      "epoch": 0.29464285714285715,
      "grad_norm": 6.330631551983623,
      "learning_rate": 2e-05,
      "loss": 3.0312,
      "step": 264
    },
    {
      "epoch": 0.29575892857142855,
      "grad_norm": 5.489079083264709,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 265
    },
    {
      "epoch": 0.296875,
      "grad_norm": 5.480703314702249,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 266
    },
    {
      "epoch": 0.29799107142857145,
      "grad_norm": 5.41364619266832,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 267
    },
    {
      "epoch": 0.29910714285714285,
      "grad_norm": 5.152914532654801,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 268
    },
    {
      "epoch": 0.3002232142857143,
      "grad_norm": 5.470606194602912,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 269
    },
    {
      "epoch": 0.3013392857142857,
      "grad_norm": 5.889479608238374,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 270
    },
    {
      "epoch": 0.30245535714285715,
      "grad_norm": 6.245747382885006,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 271
    },
    {
      "epoch": 0.30357142857142855,
      "grad_norm": 5.623342948736502,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 272
    },
    {
      "epoch": 0.3046875,
      "grad_norm": 5.90200300337235,
      "learning_rate": 2e-05,
      "loss": 2.8594,
      "step": 273
    },
    {
      "epoch": 0.30580357142857145,
      "grad_norm": 5.194870702425177,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 274
    },
    {
      "epoch": 0.30691964285714285,
      "grad_norm": 5.794914889446456,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 275
    },
    {
      "epoch": 0.3080357142857143,
      "grad_norm": 6.381987639823284,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 276
    },
    {
      "epoch": 0.3091517857142857,
      "grad_norm": 6.1834017563839225,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 277
    },
    {
      "epoch": 0.31026785714285715,
      "grad_norm": 5.170698168348872,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 278
    },
    {
      "epoch": 0.31138392857142855,
      "grad_norm": 6.17602319354166,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 279
    },
    {
      "epoch": 0.3125,
      "grad_norm": 5.725733454018046,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 280
    },
    {
      "epoch": 0.31361607142857145,
      "grad_norm": 4.930488614452633,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 281
    },
    {
      "epoch": 0.31473214285714285,
      "grad_norm": 5.356077894774006,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 282
    },
    {
      "epoch": 0.3158482142857143,
      "grad_norm": 7.393537275045752,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 283
    },
    {
      "epoch": 0.3169642857142857,
      "grad_norm": 6.313266564480613,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 284
    },
    {
      "epoch": 0.31808035714285715,
      "grad_norm": 6.256324757762068,
      "learning_rate": 2e-05,
      "loss": 2.8906,
      "step": 285
    },
    {
      "epoch": 0.31919642857142855,
      "grad_norm": 6.937892129876937,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 286
    },
    {
      "epoch": 0.3203125,
      "grad_norm": 5.939261007145628,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 287
    },
    {
      "epoch": 0.32142857142857145,
      "grad_norm": 4.970244513880297,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 288
    },
    {
      "epoch": 0.32254464285714285,
      "grad_norm": 6.100740073085076,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 289
    },
    {
      "epoch": 0.3236607142857143,
      "grad_norm": 7.817134691450799,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 290
    },
    {
      "epoch": 0.3247767857142857,
      "grad_norm": 5.9876404696188645,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 291
    },
    {
      "epoch": 0.32589285714285715,
      "grad_norm": 6.7560981222037455,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 292
    },
    {
      "epoch": 0.32700892857142855,
      "grad_norm": 4.8528760131213575,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 293
    },
    {
      "epoch": 0.328125,
      "grad_norm": 6.479218529036824,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 294
    },
    {
      "epoch": 0.32924107142857145,
      "grad_norm": 5.970693461617088,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 295
    },
    {
      "epoch": 0.33035714285714285,
      "grad_norm": 5.985771874748177,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 296
    },
    {
      "epoch": 0.3314732142857143,
      "grad_norm": 7.120535499634246,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 297
    },
    {
      "epoch": 0.3325892857142857,
      "grad_norm": 5.5017586636818585,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 298
    },
    {
      "epoch": 0.33370535714285715,
      "grad_norm": 8.306176640131177,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 299
    },
    {
      "epoch": 0.33482142857142855,
      "grad_norm": 4.681558910307139,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 300
    },
    {
      "epoch": 0.3359375,
      "grad_norm": 6.403087530825947,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 301
    },
    {
      "epoch": 0.33705357142857145,
      "grad_norm": 5.738328659066505,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 302
    },
    {
      "epoch": 0.33816964285714285,
      "grad_norm": 5.202189004026315,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 303
    },
    {
      "epoch": 0.3392857142857143,
      "grad_norm": 5.012230752872492,
      "learning_rate": 2e-05,
      "loss": 1.0859,
      "step": 304
    },
    {
      "epoch": 0.3404017857142857,
      "grad_norm": 5.595198360432458,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 305
    },
    {
      "epoch": 0.34151785714285715,
      "grad_norm": 4.108237430988085,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 306
    },
    {
      "epoch": 0.34263392857142855,
      "grad_norm": 7.445207832035633,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 307
    },
    {
      "epoch": 0.34375,
      "grad_norm": 5.102228760086666,
      "learning_rate": 2e-05,
      "loss": 1.2734,
      "step": 308
    },
    {
      "epoch": 0.34486607142857145,
      "grad_norm": 5.918815693531516,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 309
    },
    {
      "epoch": 0.34598214285714285,
      "grad_norm": 6.017954914081264,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 310
    },
    {
      "epoch": 0.3470982142857143,
      "grad_norm": 6.405339743119702,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 311
    },
    {
      "epoch": 0.3482142857142857,
      "grad_norm": 5.943174245522186,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 312
    },
    {
      "epoch": 0.34933035714285715,
      "grad_norm": 6.14619126533703,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 313
    },
    {
      "epoch": 0.35044642857142855,
      "grad_norm": 4.043394727049005,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 314
    },
    {
      "epoch": 0.3515625,
      "grad_norm": 6.491212784856782,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 315
    },
    {
      "epoch": 0.35267857142857145,
      "grad_norm": 6.122075727934848,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 316
    },
    {
      "epoch": 0.35379464285714285,
      "grad_norm": 6.7001201964117225,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 317
    },
    {
      "epoch": 0.3549107142857143,
      "grad_norm": 5.246304713916152,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 318
    },
    {
      "epoch": 0.3560267857142857,
      "grad_norm": 5.38420903793464,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 319
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 5.468480386425721,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 320
    },
    {
      "epoch": 0.35825892857142855,
      "grad_norm": 6.521058901771744,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 321
    },
    {
      "epoch": 0.359375,
      "grad_norm": 5.998039052021094,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 322
    },
    {
      "epoch": 0.36049107142857145,
      "grad_norm": 5.6877215069394245,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 323
    },
    {
      "epoch": 0.36160714285714285,
      "grad_norm": 5.796934576746362,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 324
    },
    {
      "epoch": 0.3627232142857143,
      "grad_norm": 5.75865595517348,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 325
    },
    {
      "epoch": 0.3638392857142857,
      "grad_norm": 6.259569392547488,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 326
    },
    {
      "epoch": 0.36495535714285715,
      "grad_norm": 6.219423948128885,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 327
    },
    {
      "epoch": 0.36607142857142855,
      "grad_norm": 5.7428639267416335,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 328
    },
    {
      "epoch": 0.3671875,
      "grad_norm": 5.829919905472288,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 329
    },
    {
      "epoch": 0.36830357142857145,
      "grad_norm": 5.836898358141586,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 330
    },
    {
      "epoch": 0.36941964285714285,
      "grad_norm": 6.685005677280061,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 331
    },
    {
      "epoch": 0.3705357142857143,
      "grad_norm": 6.981256958896409,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 332
    },
    {
      "epoch": 0.3716517857142857,
      "grad_norm": 5.260740722332293,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 333
    },
    {
      "epoch": 0.37276785714285715,
      "grad_norm": 6.946577895491614,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 334
    },
    {
      "epoch": 0.37388392857142855,
      "grad_norm": 5.898512311305371,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 335
    },
    {
      "epoch": 0.375,
      "grad_norm": 6.6867196651542,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 336
    },
    {
      "epoch": 0.37611607142857145,
      "grad_norm": 6.2167047098151205,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 337
    },
    {
      "epoch": 0.37723214285714285,
      "grad_norm": 6.481859342806182,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 338
    },
    {
      "epoch": 0.3783482142857143,
      "grad_norm": 6.672480363971137,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 339
    },
    {
      "epoch": 0.3794642857142857,
      "grad_norm": 6.474222763479225,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 340
    },
    {
      "epoch": 0.38058035714285715,
      "grad_norm": 6.865369480979002,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 341
    },
    {
      "epoch": 0.38169642857142855,
      "grad_norm": 5.700214233178537,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 342
    },
    {
      "epoch": 0.3828125,
      "grad_norm": 4.330382195168137,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 343
    },
    {
      "epoch": 0.38392857142857145,
      "grad_norm": 5.990920890336209,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 344
    },
    {
      "epoch": 0.38504464285714285,
      "grad_norm": 7.716361785749903,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 345
    },
    {
      "epoch": 0.3861607142857143,
      "grad_norm": 5.641672105532815,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 346
    },
    {
      "epoch": 0.3872767857142857,
      "grad_norm": 6.247139794342922,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 347
    },
    {
      "epoch": 0.38839285714285715,
      "grad_norm": 4.526405318878593,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 348
    },
    {
      "epoch": 0.38950892857142855,
      "grad_norm": 5.84802183091192,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 349
    },
    {
      "epoch": 0.390625,
      "grad_norm": 6.3075716987046135,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 350
    },
    {
      "epoch": 0.39174107142857145,
      "grad_norm": 5.3962683021827855,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 351
    },
    {
      "epoch": 0.39285714285714285,
      "grad_norm": 6.859895369254169,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 352
    },
    {
      "epoch": 0.3939732142857143,
      "grad_norm": 5.985377035158306,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 353
    },
    {
      "epoch": 0.3950892857142857,
      "grad_norm": 5.410371489631165,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 354
    },
    {
      "epoch": 0.39620535714285715,
      "grad_norm": 6.665188916366339,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 355
    },
    {
      "epoch": 0.39732142857142855,
      "grad_norm": 5.391209519947215,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 356
    },
    {
      "epoch": 0.3984375,
      "grad_norm": 6.001163232692434,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 357
    },
    {
      "epoch": 0.39955357142857145,
      "grad_norm": 6.603579911502081,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 358
    },
    {
      "epoch": 0.40066964285714285,
      "grad_norm": 6.579369133187988,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 359
    },
    {
      "epoch": 0.4017857142857143,
      "grad_norm": 5.545349699122646,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 360
    },
    {
      "epoch": 0.4029017857142857,
      "grad_norm": 6.105307562591906,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 361
    },
    {
      "epoch": 0.40401785714285715,
      "grad_norm": 6.7813003407687695,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 362
    },
    {
      "epoch": 0.40513392857142855,
      "grad_norm": 7.046860244609957,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 363
    },
    {
      "epoch": 0.40625,
      "grad_norm": 5.426861969461977,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 364
    },
    {
      "epoch": 0.40736607142857145,
      "grad_norm": 6.195293264878027,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 365
    },
    {
      "epoch": 0.40848214285714285,
      "grad_norm": 6.200447050861281,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 366
    },
    {
      "epoch": 0.4095982142857143,
      "grad_norm": 5.595228344398224,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 367
    },
    {
      "epoch": 0.4107142857142857,
      "grad_norm": 5.8432601289481125,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 368
    },
    {
      "epoch": 0.41183035714285715,
      "grad_norm": 6.079638208423657,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 369
    },
    {
      "epoch": 0.41294642857142855,
      "grad_norm": 5.402080857317698,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 370
    },
    {
      "epoch": 0.4140625,
      "grad_norm": 5.373155401106747,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 371
    },
    {
      "epoch": 0.41517857142857145,
      "grad_norm": 5.212789404143644,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 372
    },
    {
      "epoch": 0.41629464285714285,
      "grad_norm": 6.592401396032539,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 373
    },
    {
      "epoch": 0.4174107142857143,
      "grad_norm": 5.444279805970692,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 374
    },
    {
      "epoch": 0.4185267857142857,
      "grad_norm": 6.129844170249731,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 375
    },
    {
      "epoch": 0.41964285714285715,
      "grad_norm": 6.34720215493731,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 376
    },
    {
      "epoch": 0.42075892857142855,
      "grad_norm": 5.276766947606829,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 377
    },
    {
      "epoch": 0.421875,
      "grad_norm": 6.5468063152206115,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 378
    },
    {
      "epoch": 0.42299107142857145,
      "grad_norm": 6.590926383115588,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 379
    },
    {
      "epoch": 0.42410714285714285,
      "grad_norm": 6.499396319898452,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 380
    },
    {
      "epoch": 0.4252232142857143,
      "grad_norm": 6.024348317348521,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 381
    },
    {
      "epoch": 0.4263392857142857,
      "grad_norm": 4.818488782305947,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 382
    },
    {
      "epoch": 0.42745535714285715,
      "grad_norm": 6.097826451283656,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 383
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 5.6921555500568575,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 384
    },
    {
      "epoch": 0.4296875,
      "grad_norm": 7.504548885171974,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 385
    },
    {
      "epoch": 0.43080357142857145,
      "grad_norm": 6.243656641408445,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 386
    },
    {
      "epoch": 0.43191964285714285,
      "grad_norm": 7.082077410520427,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 387
    },
    {
      "epoch": 0.4330357142857143,
      "grad_norm": 6.263167564970838,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 388
    },
    {
      "epoch": 0.4341517857142857,
      "grad_norm": 5.9703564545701795,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 389
    },
    {
      "epoch": 0.43526785714285715,
      "grad_norm": 6.70410265204994,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 390
    },
    {
      "epoch": 0.43638392857142855,
      "grad_norm": 5.855712967355312,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 391
    },
    {
      "epoch": 0.4375,
      "grad_norm": 4.492800181110915,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 392
    },
    {
      "epoch": 0.43861607142857145,
      "grad_norm": 6.5544548419871544,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 393
    },
    {
      "epoch": 0.43973214285714285,
      "grad_norm": 7.247362772010449,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 394
    },
    {
      "epoch": 0.4408482142857143,
      "grad_norm": 5.884066207580705,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 395
    },
    {
      "epoch": 0.4419642857142857,
      "grad_norm": 6.8943334875672315,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 396
    },
    {
      "epoch": 0.44308035714285715,
      "grad_norm": 6.864360297879285,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 397
    },
    {
      "epoch": 0.44419642857142855,
      "grad_norm": 6.140238793237744,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 398
    },
    {
      "epoch": 0.4453125,
      "grad_norm": 5.3291157336995285,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 399
    },
    {
      "epoch": 0.44642857142857145,
      "grad_norm": 5.598887102514742,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 400
    },
    {
      "epoch": 0.44754464285714285,
      "grad_norm": 8.286684196049563,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 401
    },
    {
      "epoch": 0.4486607142857143,
      "grad_norm": 5.788371395199885,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 402
    },
    {
      "epoch": 0.4497767857142857,
      "grad_norm": 5.244714959085075,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 403
    },
    {
      "epoch": 0.45089285714285715,
      "grad_norm": 7.9648950652684505,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 404
    },
    {
      "epoch": 0.45200892857142855,
      "grad_norm": 5.840505536000766,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 405
    },
    {
      "epoch": 0.453125,
      "grad_norm": 5.325114943786698,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 406
    },
    {
      "epoch": 0.45424107142857145,
      "grad_norm": 6.860372358841779,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 407
    },
    {
      "epoch": 0.45535714285714285,
      "grad_norm": 5.283241798012223,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 408
    },
    {
      "epoch": 0.4564732142857143,
      "grad_norm": 6.14959717966234,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 409
    },
    {
      "epoch": 0.4575892857142857,
      "grad_norm": 6.778686519742661,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 410
    },
    {
      "epoch": 0.45870535714285715,
      "grad_norm": 7.232801883524477,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 411
    },
    {
      "epoch": 0.45982142857142855,
      "grad_norm": 6.145644586883981,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 412
    },
    {
      "epoch": 0.4609375,
      "grad_norm": 5.898979644439567,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 413
    },
    {
      "epoch": 0.46205357142857145,
      "grad_norm": 5.569789972706816,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 414
    },
    {
      "epoch": 0.46316964285714285,
      "grad_norm": 5.045999108828418,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 415
    },
    {
      "epoch": 0.4642857142857143,
      "grad_norm": 4.894099241373748,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 416
    },
    {
      "epoch": 0.4654017857142857,
      "grad_norm": 6.449555308330442,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 417
    },
    {
      "epoch": 0.46651785714285715,
      "grad_norm": 6.093921261346725,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 418
    },
    {
      "epoch": 0.46763392857142855,
      "grad_norm": 5.105227635925224,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 419
    },
    {
      "epoch": 0.46875,
      "grad_norm": 4.195421234821704,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 420
    },
    {
      "epoch": 0.46986607142857145,
      "grad_norm": 5.84584469759069,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 421
    },
    {
      "epoch": 0.47098214285714285,
      "grad_norm": 7.367747184371759,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 422
    },
    {
      "epoch": 0.4720982142857143,
      "grad_norm": 6.056601658643672,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 423
    },
    {
      "epoch": 0.4732142857142857,
      "grad_norm": 6.8024343216931245,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 424
    },
    {
      "epoch": 0.47433035714285715,
      "grad_norm": 7.195987718691981,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 425
    },
    {
      "epoch": 0.47544642857142855,
      "grad_norm": 7.293579492177266,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 426
    },
    {
      "epoch": 0.4765625,
      "grad_norm": 6.210461453096692,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 427
    },
    {
      "epoch": 0.47767857142857145,
      "grad_norm": 7.465061823243178,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 428
    },
    {
      "epoch": 0.47879464285714285,
      "grad_norm": 6.780365154528446,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 429
    },
    {
      "epoch": 0.4799107142857143,
      "grad_norm": 5.973983523759283,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 430
    },
    {
      "epoch": 0.4810267857142857,
      "grad_norm": 7.040120640539666,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 431
    },
    {
      "epoch": 0.48214285714285715,
      "grad_norm": 5.181417951339893,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 432
    },
    {
      "epoch": 0.48325892857142855,
      "grad_norm": 9.015996162493627,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 433
    },
    {
      "epoch": 0.484375,
      "grad_norm": 6.086607805504361,
      "learning_rate": 2e-05,
      "loss": 1.4688,
      "step": 434
    },
    {
      "epoch": 0.48549107142857145,
      "grad_norm": 7.675171980234482,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 435
    },
    {
      "epoch": 0.48660714285714285,
      "grad_norm": 7.196074650717554,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 436
    },
    {
      "epoch": 0.4877232142857143,
      "grad_norm": 7.247741196750318,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 437
    },
    {
      "epoch": 0.4888392857142857,
      "grad_norm": 7.079544525807087,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 438
    },
    {
      "epoch": 0.48995535714285715,
      "grad_norm": 5.821062052032175,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 439
    },
    {
      "epoch": 0.49107142857142855,
      "grad_norm": 7.621067196834796,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 440
    },
    {
      "epoch": 0.4921875,
      "grad_norm": 6.191078994075534,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 441
    },
    {
      "epoch": 0.49330357142857145,
      "grad_norm": 7.656098911630931,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 442
    },
    {
      "epoch": 0.49441964285714285,
      "grad_norm": 6.580625077145156,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 443
    },
    {
      "epoch": 0.4955357142857143,
      "grad_norm": 5.615320082987715,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 444
    },
    {
      "epoch": 0.4966517857142857,
      "grad_norm": 5.540945424055651,
      "learning_rate": 2e-05,
      "loss": 2.8594,
      "step": 445
    },
    {
      "epoch": 0.49776785714285715,
      "grad_norm": 7.179715882567501,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 446
    },
    {
      "epoch": 0.49888392857142855,
      "grad_norm": 6.506292836312854,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 447
    },
    {
      "epoch": 0.5,
      "grad_norm": 10.789402229886935,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 448
    },
    {
      "epoch": 0.5011160714285714,
      "grad_norm": 6.1447604043094195,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 449
    },
    {
      "epoch": 0.5022321428571429,
      "grad_norm": 8.116584067649637,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 450
    },
    {
      "epoch": 0.5033482142857143,
      "grad_norm": 5.982471564908289,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 451
    },
    {
      "epoch": 0.5044642857142857,
      "grad_norm": 7.097930942452871,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 452
    },
    {
      "epoch": 0.5055803571428571,
      "grad_norm": 5.4296084797052,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 453
    },
    {
      "epoch": 0.5066964285714286,
      "grad_norm": 6.182447680161965,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 454
    },
    {
      "epoch": 0.5078125,
      "grad_norm": 6.603200965927261,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 455
    },
    {
      "epoch": 0.5089285714285714,
      "grad_norm": 7.816722907344761,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 456
    },
    {
      "epoch": 0.5100446428571429,
      "grad_norm": 7.048079859801893,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 457
    },
    {
      "epoch": 0.5111607142857143,
      "grad_norm": 8.243049639406017,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 458
    },
    {
      "epoch": 0.5122767857142857,
      "grad_norm": 6.659956671556887,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 459
    },
    {
      "epoch": 0.5133928571428571,
      "grad_norm": 6.126250657764653,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 460
    },
    {
      "epoch": 0.5145089285714286,
      "grad_norm": 6.124736043291105,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 461
    },
    {
      "epoch": 0.515625,
      "grad_norm": 6.371795427964414,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 462
    },
    {
      "epoch": 0.5167410714285714,
      "grad_norm": 6.476056901849557,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 463
    },
    {
      "epoch": 0.5178571428571429,
      "grad_norm": 7.232511118933011,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 464
    },
    {
      "epoch": 0.5189732142857143,
      "grad_norm": 17.891802830408736,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 465
    },
    {
      "epoch": 0.5200892857142857,
      "grad_norm": 6.473444054020356,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 466
    },
    {
      "epoch": 0.5212053571428571,
      "grad_norm": 7.4946040906703475,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 467
    },
    {
      "epoch": 0.5223214285714286,
      "grad_norm": 7.517101274756477,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 468
    },
    {
      "epoch": 0.5234375,
      "grad_norm": 5.484951120052039,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 469
    },
    {
      "epoch": 0.5245535714285714,
      "grad_norm": 5.403223141452,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 470
    },
    {
      "epoch": 0.5256696428571429,
      "grad_norm": 6.032150457320694,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 471
    },
    {
      "epoch": 0.5267857142857143,
      "grad_norm": 6.908303312738406,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 472
    },
    {
      "epoch": 0.5279017857142857,
      "grad_norm": 6.01720104010625,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 473
    },
    {
      "epoch": 0.5290178571428571,
      "grad_norm": 5.289694474262655,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 474
    },
    {
      "epoch": 0.5301339285714286,
      "grad_norm": 6.054021005714556,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 475
    },
    {
      "epoch": 0.53125,
      "grad_norm": 7.315500183203906,
      "learning_rate": 2e-05,
      "loss": 2.8906,
      "step": 476
    },
    {
      "epoch": 0.5323660714285714,
      "grad_norm": 5.959600683071562,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 477
    },
    {
      "epoch": 0.5334821428571429,
      "grad_norm": 7.471727914337743,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 478
    },
    {
      "epoch": 0.5345982142857143,
      "grad_norm": 5.418746173238664,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 479
    },
    {
      "epoch": 0.5357142857142857,
      "grad_norm": 6.576982561255422,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 480
    },
    {
      "epoch": 0.5368303571428571,
      "grad_norm": 7.05482827603986,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 481
    },
    {
      "epoch": 0.5379464285714286,
      "grad_norm": 6.775847449417505,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 482
    },
    {
      "epoch": 0.5390625,
      "grad_norm": 6.810206150635688,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 483
    },
    {
      "epoch": 0.5401785714285714,
      "grad_norm": 5.231155553741888,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 484
    },
    {
      "epoch": 0.5412946428571429,
      "grad_norm": 5.5019347897277235,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 485
    },
    {
      "epoch": 0.5424107142857143,
      "grad_norm": 7.681471626862553,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 486
    },
    {
      "epoch": 0.5435267857142857,
      "grad_norm": 7.045405885975734,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 487
    },
    {
      "epoch": 0.5446428571428571,
      "grad_norm": 7.22107619181274,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 488
    },
    {
      "epoch": 0.5457589285714286,
      "grad_norm": 6.758795494377998,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 489
    },
    {
      "epoch": 0.546875,
      "grad_norm": 7.046430042982425,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 490
    },
    {
      "epoch": 0.5479910714285714,
      "grad_norm": 7.555019298129235,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 491
    },
    {
      "epoch": 0.5491071428571429,
      "grad_norm": 21.320133622636387,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 492
    },
    {
      "epoch": 0.5502232142857143,
      "grad_norm": 5.647300531352911,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 493
    },
    {
      "epoch": 0.5513392857142857,
      "grad_norm": 7.457379278094384,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 494
    },
    {
      "epoch": 0.5524553571428571,
      "grad_norm": 6.462934028920638,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 495
    },
    {
      "epoch": 0.5535714285714286,
      "grad_norm": 6.832618514429236,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 496
    },
    {
      "epoch": 0.5546875,
      "grad_norm": 6.924903099903694,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 497
    },
    {
      "epoch": 0.5558035714285714,
      "grad_norm": 7.745826770596821,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 498
    },
    {
      "epoch": 0.5569196428571429,
      "grad_norm": 6.476621917596814,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 499
    },
    {
      "epoch": 0.5580357142857143,
      "grad_norm": 6.37764145462918,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 500
    },
    {
      "epoch": 0.5591517857142857,
      "grad_norm": 7.287321350753652,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 501
    },
    {
      "epoch": 0.5602678571428571,
      "grad_norm": 7.544470055865645,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 502
    },
    {
      "epoch": 0.5613839285714286,
      "grad_norm": 5.433592106725318,
      "learning_rate": 2e-05,
      "loss": 2.9219,
      "step": 503
    },
    {
      "epoch": 0.5625,
      "grad_norm": 7.33444143982492,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 504
    },
    {
      "epoch": 0.5636160714285714,
      "grad_norm": 5.713758572542107,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 505
    },
    {
      "epoch": 0.5647321428571429,
      "grad_norm": 6.707458073683937,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 506
    },
    {
      "epoch": 0.5658482142857143,
      "grad_norm": 6.803334840536561,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 507
    },
    {
      "epoch": 0.5669642857142857,
      "grad_norm": 6.693282613415061,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 508
    },
    {
      "epoch": 0.5680803571428571,
      "grad_norm": 7.0015250940395095,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 509
    },
    {
      "epoch": 0.5691964285714286,
      "grad_norm": 7.398563117347831,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 510
    },
    {
      "epoch": 0.5703125,
      "grad_norm": 5.926340025577637,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 511
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 7.858769337353693,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 512
    },
    {
      "epoch": 0.5725446428571429,
      "grad_norm": 9.15600071934991,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 513
    },
    {
      "epoch": 0.5736607142857143,
      "grad_norm": 8.089059350747743,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 514
    },
    {
      "epoch": 0.5747767857142857,
      "grad_norm": 6.603976613525273,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 515
    },
    {
      "epoch": 0.5758928571428571,
      "grad_norm": 6.785829823358593,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 516
    },
    {
      "epoch": 0.5770089285714286,
      "grad_norm": 6.787069524523533,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 517
    },
    {
      "epoch": 0.578125,
      "grad_norm": 7.404594138145085,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 518
    },
    {
      "epoch": 0.5792410714285714,
      "grad_norm": 5.197338671742264,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 519
    },
    {
      "epoch": 0.5803571428571429,
      "grad_norm": 6.503093819456712,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 520
    },
    {
      "epoch": 0.5814732142857143,
      "grad_norm": 7.234193494894187,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 521
    },
    {
      "epoch": 0.5825892857142857,
      "grad_norm": 6.382905598785433,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 522
    },
    {
      "epoch": 0.5837053571428571,
      "grad_norm": 6.259005172333159,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 523
    },
    {
      "epoch": 0.5848214285714286,
      "grad_norm": 7.500613290315144,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 524
    },
    {
      "epoch": 0.5859375,
      "grad_norm": 6.752121141840796,
      "learning_rate": 2e-05,
      "loss": 2.8438,
      "step": 525
    },
    {
      "epoch": 0.5870535714285714,
      "grad_norm": 8.674907180311699,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 526
    },
    {
      "epoch": 0.5881696428571429,
      "grad_norm": 6.230415798326659,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 527
    },
    {
      "epoch": 0.5892857142857143,
      "grad_norm": 7.473396496556749,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 528
    },
    {
      "epoch": 0.5904017857142857,
      "grad_norm": 7.0905655610625,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 529
    },
    {
      "epoch": 0.5915178571428571,
      "grad_norm": 7.547132878917807,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 530
    },
    {
      "epoch": 0.5926339285714286,
      "grad_norm": 7.368677873502134,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 531
    },
    {
      "epoch": 0.59375,
      "grad_norm": 7.629273979369373,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 532
    },
    {
      "epoch": 0.5948660714285714,
      "grad_norm": 6.343202035930694,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 533
    },
    {
      "epoch": 0.5959821428571429,
      "grad_norm": 7.113872410593122,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 534
    },
    {
      "epoch": 0.5970982142857143,
      "grad_norm": 6.861135827815423,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 535
    },
    {
      "epoch": 0.5982142857142857,
      "grad_norm": 8.750357965847202,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 536
    },
    {
      "epoch": 0.5993303571428571,
      "grad_norm": 6.641071597773124,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 537
    },
    {
      "epoch": 0.6004464285714286,
      "grad_norm": 6.5215686581069745,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 538
    },
    {
      "epoch": 0.6015625,
      "grad_norm": 7.165276449631838,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 539
    },
    {
      "epoch": 0.6026785714285714,
      "grad_norm": 7.737082160252734,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 540
    },
    {
      "epoch": 0.6037946428571429,
      "grad_norm": 6.885279340195057,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 541
    },
    {
      "epoch": 0.6049107142857143,
      "grad_norm": 7.257508557159585,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 542
    },
    {
      "epoch": 0.6060267857142857,
      "grad_norm": 7.391386065709816,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 543
    },
    {
      "epoch": 0.6071428571428571,
      "grad_norm": 7.137488746959001,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 544
    },
    {
      "epoch": 0.6082589285714286,
      "grad_norm": 7.748661890383331,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 545
    },
    {
      "epoch": 0.609375,
      "grad_norm": 6.582130996381901,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 546
    },
    {
      "epoch": 0.6104910714285714,
      "grad_norm": 7.0629110368915695,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 547
    },
    {
      "epoch": 0.6116071428571429,
      "grad_norm": 6.734276089021543,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 548
    },
    {
      "epoch": 0.6127232142857143,
      "grad_norm": 5.535353287322365,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 549
    },
    {
      "epoch": 0.6138392857142857,
      "grad_norm": 6.178656547762734,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 550
    },
    {
      "epoch": 0.6149553571428571,
      "grad_norm": 7.039181863661586,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 551
    },
    {
      "epoch": 0.6160714285714286,
      "grad_norm": 7.165958925195559,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 552
    },
    {
      "epoch": 0.6171875,
      "grad_norm": 7.368164764534582,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 553
    },
    {
      "epoch": 0.6183035714285714,
      "grad_norm": 8.243520899076342,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 554
    },
    {
      "epoch": 0.6194196428571429,
      "grad_norm": 7.486382789499762,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 555
    },
    {
      "epoch": 0.6205357142857143,
      "grad_norm": 6.640119412702982,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 556
    },
    {
      "epoch": 0.6216517857142857,
      "grad_norm": 7.786718397435241,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 557
    },
    {
      "epoch": 0.6227678571428571,
      "grad_norm": 8.481859862736677,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 558
    },
    {
      "epoch": 0.6238839285714286,
      "grad_norm": 7.27861097670762,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 559
    },
    {
      "epoch": 0.625,
      "grad_norm": 7.574828353868911,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 560
    },
    {
      "epoch": 0.6261160714285714,
      "grad_norm": 6.6340484097224355,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 561
    },
    {
      "epoch": 0.6272321428571429,
      "grad_norm": 6.500167685041858,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 562
    },
    {
      "epoch": 0.6283482142857143,
      "grad_norm": 6.1237783123238545,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 563
    },
    {
      "epoch": 0.6294642857142857,
      "grad_norm": 5.12880085763775,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 564
    },
    {
      "epoch": 0.6305803571428571,
      "grad_norm": 8.077846784437718,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 565
    },
    {
      "epoch": 0.6316964285714286,
      "grad_norm": 7.850332157185584,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 566
    },
    {
      "epoch": 0.6328125,
      "grad_norm": 6.525401122005646,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 567
    },
    {
      "epoch": 0.6339285714285714,
      "grad_norm": 6.828137853245754,
      "learning_rate": 2e-05,
      "loss": 3.1094,
      "step": 568
    },
    {
      "epoch": 0.6350446428571429,
      "grad_norm": 6.933946469483155,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 569
    },
    {
      "epoch": 0.6361607142857143,
      "grad_norm": 6.4225118553788745,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 570
    },
    {
      "epoch": 0.6372767857142857,
      "grad_norm": 9.055079622552078,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 571
    },
    {
      "epoch": 0.6383928571428571,
      "grad_norm": 6.9392919932494745,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 572
    },
    {
      "epoch": 0.6395089285714286,
      "grad_norm": 6.714547797727804,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 573
    },
    {
      "epoch": 0.640625,
      "grad_norm": 7.462917953744773,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 574
    },
    {
      "epoch": 0.6417410714285714,
      "grad_norm": 6.019678968935781,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 575
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 5.319139837355859,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 576
    },
    {
      "epoch": 0.6439732142857143,
      "grad_norm": 6.890259971829131,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 577
    },
    {
      "epoch": 0.6450892857142857,
      "grad_norm": 6.343981202862711,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 578
    },
    {
      "epoch": 0.6462053571428571,
      "grad_norm": 6.687299925702872,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 579
    },
    {
      "epoch": 0.6473214285714286,
      "grad_norm": 6.654496776350769,
      "learning_rate": 2e-05,
      "loss": 3.0938,
      "step": 580
    },
    {
      "epoch": 0.6484375,
      "grad_norm": 7.05216497002948,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 581
    },
    {
      "epoch": 0.6495535714285714,
      "grad_norm": 7.185968454340757,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 582
    },
    {
      "epoch": 0.6506696428571429,
      "grad_norm": 6.925185750341311,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 583
    },
    {
      "epoch": 0.6517857142857143,
      "grad_norm": 7.231674884097017,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 584
    },
    {
      "epoch": 0.6529017857142857,
      "grad_norm": 6.345907813514099,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 585
    },
    {
      "epoch": 0.6540178571428571,
      "grad_norm": 7.886918775010117,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 586
    },
    {
      "epoch": 0.6551339285714286,
      "grad_norm": 6.24959111035519,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 587
    },
    {
      "epoch": 0.65625,
      "grad_norm": 6.796481894674375,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 588
    },
    {
      "epoch": 0.6573660714285714,
      "grad_norm": 7.819064229184331,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 589
    },
    {
      "epoch": 0.6584821428571429,
      "grad_norm": 5.8413055916330165,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 590
    },
    {
      "epoch": 0.6595982142857143,
      "grad_norm": 8.268594819742033,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 591
    },
    {
      "epoch": 0.6607142857142857,
      "grad_norm": 7.8517082476443445,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 592
    },
    {
      "epoch": 0.6618303571428571,
      "grad_norm": 7.3243181307788925,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 593
    },
    {
      "epoch": 0.6629464285714286,
      "grad_norm": 7.396669674394314,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 594
    },
    {
      "epoch": 0.6640625,
      "grad_norm": 6.486828848442188,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 595
    },
    {
      "epoch": 0.6651785714285714,
      "grad_norm": 9.034632567352785,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 596
    },
    {
      "epoch": 0.6662946428571429,
      "grad_norm": 8.297827583814112,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 597
    },
    {
      "epoch": 0.6674107142857143,
      "grad_norm": 8.69103226112985,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 598
    },
    {
      "epoch": 0.6685267857142857,
      "grad_norm": 7.847905503198117,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 599
    },
    {
      "epoch": 0.6696428571428571,
      "grad_norm": 11.909023856182442,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 600
    },
    {
      "epoch": 0.6707589285714286,
      "grad_norm": 8.049127314411349,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 601
    },
    {
      "epoch": 0.671875,
      "grad_norm": 6.093627224198067,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 602
    },
    {
      "epoch": 0.6729910714285714,
      "grad_norm": 7.460176050458266,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 603
    },
    {
      "epoch": 0.6741071428571429,
      "grad_norm": 8.680047266521463,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 604
    },
    {
      "epoch": 0.6752232142857143,
      "grad_norm": 6.449522781940608,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 605
    },
    {
      "epoch": 0.6763392857142857,
      "grad_norm": 6.6233250634896566,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 606
    },
    {
      "epoch": 0.6774553571428571,
      "grad_norm": 7.827692141312697,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 607
    },
    {
      "epoch": 0.6785714285714286,
      "grad_norm": 13.597371567260781,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 608
    },
    {
      "epoch": 0.6796875,
      "grad_norm": 7.578447552180426,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 609
    },
    {
      "epoch": 0.6808035714285714,
      "grad_norm": 8.338432520821854,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 610
    },
    {
      "epoch": 0.6819196428571429,
      "grad_norm": 7.317481846904897,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 611
    },
    {
      "epoch": 0.6830357142857143,
      "grad_norm": 7.362421322426254,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 612
    },
    {
      "epoch": 0.6841517857142857,
      "grad_norm": 5.206645099891276,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 613
    },
    {
      "epoch": 0.6852678571428571,
      "grad_norm": 6.530204837885705,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 614
    },
    {
      "epoch": 0.6863839285714286,
      "grad_norm": 8.563800334057776,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 615
    },
    {
      "epoch": 0.6875,
      "grad_norm": 6.337579807856006,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 616
    },
    {
      "epoch": 0.6886160714285714,
      "grad_norm": 7.741881110306949,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 617
    },
    {
      "epoch": 0.6897321428571429,
      "grad_norm": 7.215246397096221,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 618
    },
    {
      "epoch": 0.6908482142857143,
      "grad_norm": 5.124279706126531,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 619
    },
    {
      "epoch": 0.6919642857142857,
      "grad_norm": 8.702251162301915,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 620
    },
    {
      "epoch": 0.6930803571428571,
      "grad_norm": 8.414264255254512,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 621
    },
    {
      "epoch": 0.6941964285714286,
      "grad_norm": 6.884341711234197,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 622
    },
    {
      "epoch": 0.6953125,
      "grad_norm": 7.441513791789359,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 623
    },
    {
      "epoch": 0.6964285714285714,
      "grad_norm": 7.0293900375822265,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 624
    },
    {
      "epoch": 0.6975446428571429,
      "grad_norm": 7.239744578628313,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 625
    },
    {
      "epoch": 0.6986607142857143,
      "grad_norm": 8.200515413058048,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 626
    },
    {
      "epoch": 0.6997767857142857,
      "grad_norm": 7.54549419799597,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 627
    },
    {
      "epoch": 0.7008928571428571,
      "grad_norm": 7.369938171985584,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 628
    },
    {
      "epoch": 0.7020089285714286,
      "grad_norm": 7.692149336453549,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 629
    },
    {
      "epoch": 0.703125,
      "grad_norm": 8.114366300501242,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 630
    },
    {
      "epoch": 0.7042410714285714,
      "grad_norm": 6.909344860456727,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 631
    },
    {
      "epoch": 0.7053571428571429,
      "grad_norm": 6.626275043118414,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 632
    },
    {
      "epoch": 0.7064732142857143,
      "grad_norm": 9.22769722379324,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 633
    },
    {
      "epoch": 0.7075892857142857,
      "grad_norm": 7.802051115433907,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 634
    },
    {
      "epoch": 0.7087053571428571,
      "grad_norm": 8.944644090925104,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 635
    },
    {
      "epoch": 0.7098214285714286,
      "grad_norm": 6.92652515600993,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 636
    },
    {
      "epoch": 0.7109375,
      "grad_norm": 6.812532015268174,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 637
    },
    {
      "epoch": 0.7120535714285714,
      "grad_norm": 6.894567142747653,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 638
    },
    {
      "epoch": 0.7131696428571429,
      "grad_norm": 8.09618501611102,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 639
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 8.48950473722746,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 640
    },
    {
      "epoch": 0.7154017857142857,
      "grad_norm": 6.859688482929468,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 641
    },
    {
      "epoch": 0.7165178571428571,
      "grad_norm": 8.190078940248425,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 642
    },
    {
      "epoch": 0.7176339285714286,
      "grad_norm": 6.43415608795316,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 643
    },
    {
      "epoch": 0.71875,
      "grad_norm": 7.264083390180525,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 644
    },
    {
      "epoch": 0.7198660714285714,
      "grad_norm": 6.197882300103818,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 645
    },
    {
      "epoch": 0.7209821428571429,
      "grad_norm": 8.66468277840732,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 646
    },
    {
      "epoch": 0.7220982142857143,
      "grad_norm": 8.189051036002342,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 647
    },
    {
      "epoch": 0.7232142857142857,
      "grad_norm": 5.863270120115275,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 648
    },
    {
      "epoch": 0.7243303571428571,
      "grad_norm": 7.5682111696073076,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 649
    },
    {
      "epoch": 0.7254464285714286,
      "grad_norm": 6.75854755331268,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 650
    },
    {
      "epoch": 0.7265625,
      "grad_norm": 6.97848641724375,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 651
    },
    {
      "epoch": 0.7276785714285714,
      "grad_norm": 6.518540836545243,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 652
    },
    {
      "epoch": 0.7287946428571429,
      "grad_norm": 8.39757695297506,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 653
    },
    {
      "epoch": 0.7299107142857143,
      "grad_norm": 7.002154417262316,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 654
    },
    {
      "epoch": 0.7310267857142857,
      "grad_norm": 7.652785148280761,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 655
    },
    {
      "epoch": 0.7321428571428571,
      "grad_norm": 7.58861480616225,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 656
    },
    {
      "epoch": 0.7332589285714286,
      "grad_norm": 7.014376724110922,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 657
    },
    {
      "epoch": 0.734375,
      "grad_norm": 6.139948883690198,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 658
    },
    {
      "epoch": 0.7354910714285714,
      "grad_norm": 7.439759946247817,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 659
    },
    {
      "epoch": 0.7366071428571429,
      "grad_norm": 6.8249113237600145,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 660
    },
    {
      "epoch": 0.7377232142857143,
      "grad_norm": 6.914146794957794,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 661
    },
    {
      "epoch": 0.7388392857142857,
      "grad_norm": 6.837275664608894,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 662
    },
    {
      "epoch": 0.7399553571428571,
      "grad_norm": 7.598402800495714,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 663
    },
    {
      "epoch": 0.7410714285714286,
      "grad_norm": 6.403469630272328,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 664
    },
    {
      "epoch": 0.7421875,
      "grad_norm": 6.00450945960009,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 665
    },
    {
      "epoch": 0.7433035714285714,
      "grad_norm": 8.037821345312222,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 666
    },
    {
      "epoch": 0.7444196428571429,
      "grad_norm": 6.6797290359445745,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 667
    },
    {
      "epoch": 0.7455357142857143,
      "grad_norm": 7.009173429277118,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 668
    },
    {
      "epoch": 0.7466517857142857,
      "grad_norm": 6.524823752456167,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 669
    },
    {
      "epoch": 0.7477678571428571,
      "grad_norm": 7.637059834023824,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 670
    },
    {
      "epoch": 0.7488839285714286,
      "grad_norm": 8.49148476677469,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 671
    },
    {
      "epoch": 0.75,
      "grad_norm": 8.338162586372338,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 672
    },
    {
      "epoch": 0.7511160714285714,
      "grad_norm": 5.69856314965629,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 673
    },
    {
      "epoch": 0.7522321428571429,
      "grad_norm": 7.631287716869639,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 674
    },
    {
      "epoch": 0.7533482142857143,
      "grad_norm": 6.987960668144088,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 675
    },
    {
      "epoch": 0.7544642857142857,
      "grad_norm": 6.487843927051443,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 676
    },
    {
      "epoch": 0.7555803571428571,
      "grad_norm": 5.618709956810665,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 677
    },
    {
      "epoch": 0.7566964285714286,
      "grad_norm": 7.430354842673078,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 678
    },
    {
      "epoch": 0.7578125,
      "grad_norm": 6.805536374070492,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 679
    },
    {
      "epoch": 0.7589285714285714,
      "grad_norm": 7.402628875469795,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 680
    },
    {
      "epoch": 0.7600446428571429,
      "grad_norm": 6.777933446889452,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 681
    },
    {
      "epoch": 0.7611607142857143,
      "grad_norm": 8.987087718675813,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 682
    },
    {
      "epoch": 0.7622767857142857,
      "grad_norm": 6.735449008398507,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 683
    },
    {
      "epoch": 0.7633928571428571,
      "grad_norm": 6.117891642385611,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 684
    },
    {
      "epoch": 0.7645089285714286,
      "grad_norm": 7.577422617049957,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 685
    },
    {
      "epoch": 0.765625,
      "grad_norm": 7.131983366478902,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 686
    },
    {
      "epoch": 0.7667410714285714,
      "grad_norm": 7.173704669490769,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 687
    },
    {
      "epoch": 0.7678571428571429,
      "grad_norm": 6.833493701191782,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 688
    },
    {
      "epoch": 0.7689732142857143,
      "grad_norm": 6.862652266855852,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 689
    },
    {
      "epoch": 0.7700892857142857,
      "grad_norm": 7.009071164719957,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 690
    },
    {
      "epoch": 0.7712053571428571,
      "grad_norm": 7.4444652975568255,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 691
    },
    {
      "epoch": 0.7723214285714286,
      "grad_norm": 7.071156172563195,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 692
    },
    {
      "epoch": 0.7734375,
      "grad_norm": 9.076918207109802,
      "learning_rate": 2e-05,
      "loss": 1.5312,
      "step": 693
    },
    {
      "epoch": 0.7745535714285714,
      "grad_norm": 9.049819640386074,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 694
    },
    {
      "epoch": 0.7756696428571429,
      "grad_norm": 7.700182006709742,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 695
    },
    {
      "epoch": 0.7767857142857143,
      "grad_norm": 8.244202926480916,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 696
    },
    {
      "epoch": 0.7779017857142857,
      "grad_norm": 8.011975509597269,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 697
    },
    {
      "epoch": 0.7790178571428571,
      "grad_norm": 8.235422546185953,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 698
    },
    {
      "epoch": 0.7801339285714286,
      "grad_norm": 8.237198966702296,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 699
    },
    {
      "epoch": 0.78125,
      "grad_norm": 6.958088283859897,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 700
    },
    {
      "epoch": 0.7823660714285714,
      "grad_norm": 8.080095292496676,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 701
    },
    {
      "epoch": 0.7834821428571429,
      "grad_norm": 8.613320075329357,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 702
    },
    {
      "epoch": 0.7845982142857143,
      "grad_norm": 7.267569669648207,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 703
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 7.163436934525759,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 704
    },
    {
      "epoch": 0.7868303571428571,
      "grad_norm": 7.5081256418136455,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 705
    },
    {
      "epoch": 0.7879464285714286,
      "grad_norm": 6.398911623566062,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 706
    },
    {
      "epoch": 0.7890625,
      "grad_norm": 7.357886595221823,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 707
    },
    {
      "epoch": 0.7901785714285714,
      "grad_norm": 8.742769070315433,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 708
    },
    {
      "epoch": 0.7912946428571429,
      "grad_norm": 7.223230135475636,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 709
    },
    {
      "epoch": 0.7924107142857143,
      "grad_norm": 7.385531215376964,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 710
    },
    {
      "epoch": 0.7935267857142857,
      "grad_norm": 8.494435773668522,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 711
    },
    {
      "epoch": 0.7946428571428571,
      "grad_norm": 7.555709835403667,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 712
    },
    {
      "epoch": 0.7957589285714286,
      "grad_norm": 7.115783696917038,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 713
    },
    {
      "epoch": 0.796875,
      "grad_norm": 8.179043577779913,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 714
    },
    {
      "epoch": 0.7979910714285714,
      "grad_norm": 7.9204184658056755,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 715
    },
    {
      "epoch": 0.7991071428571429,
      "grad_norm": 6.376362290456513,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 716
    },
    {
      "epoch": 0.8002232142857143,
      "grad_norm": 8.444289579939406,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 717
    },
    {
      "epoch": 0.8013392857142857,
      "grad_norm": 6.625989986426366,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 718
    },
    {
      "epoch": 0.8024553571428571,
      "grad_norm": 8.236021997988285,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 719
    },
    {
      "epoch": 0.8035714285714286,
      "grad_norm": 8.302409271271902,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 720
    },
    {
      "epoch": 0.8046875,
      "grad_norm": 9.737315346326259,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 721
    },
    {
      "epoch": 0.8058035714285714,
      "grad_norm": 8.886967551231395,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 722
    },
    {
      "epoch": 0.8069196428571429,
      "grad_norm": 7.21556747069122,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 723
    },
    {
      "epoch": 0.8080357142857143,
      "grad_norm": 7.942171871566063,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 724
    },
    {
      "epoch": 0.8091517857142857,
      "grad_norm": 7.627586842265554,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 725
    },
    {
      "epoch": 0.8102678571428571,
      "grad_norm": 7.349261648822988,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 726
    },
    {
      "epoch": 0.8113839285714286,
      "grad_norm": 8.407068878246223,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 727
    },
    {
      "epoch": 0.8125,
      "grad_norm": 7.318734018745202,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 728
    },
    {
      "epoch": 0.8136160714285714,
      "grad_norm": 6.396774982844105,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 729
    },
    {
      "epoch": 0.8147321428571429,
      "grad_norm": 7.660688635890924,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 730
    },
    {
      "epoch": 0.8158482142857143,
      "grad_norm": 7.187627556889666,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 731
    },
    {
      "epoch": 0.8169642857142857,
      "grad_norm": 5.9968303588111675,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 732
    },
    {
      "epoch": 0.8180803571428571,
      "grad_norm": 8.132129445268596,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 733
    },
    {
      "epoch": 0.8191964285714286,
      "grad_norm": 7.133863199586631,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 734
    },
    {
      "epoch": 0.8203125,
      "grad_norm": 7.109822789443284,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 735
    },
    {
      "epoch": 0.8214285714285714,
      "grad_norm": 8.63670134240111,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 736
    },
    {
      "epoch": 0.8225446428571429,
      "grad_norm": 7.3113072525929725,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 737
    },
    {
      "epoch": 0.8236607142857143,
      "grad_norm": 9.300369055243136,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 738
    },
    {
      "epoch": 0.8247767857142857,
      "grad_norm": 7.085335281545922,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 739
    },
    {
      "epoch": 0.8258928571428571,
      "grad_norm": 10.069946620389745,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 740
    },
    {
      "epoch": 0.8270089285714286,
      "grad_norm": 6.852805442574333,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 741
    },
    {
      "epoch": 0.828125,
      "grad_norm": 7.302169996038232,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 742
    },
    {
      "epoch": 0.8292410714285714,
      "grad_norm": 9.255867150465374,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 743
    },
    {
      "epoch": 0.8303571428571429,
      "grad_norm": 7.4413950850050306,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 744
    },
    {
      "epoch": 0.8314732142857143,
      "grad_norm": 7.7385358273103595,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 745
    },
    {
      "epoch": 0.8325892857142857,
      "grad_norm": 7.340417180177864,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 746
    },
    {
      "epoch": 0.8337053571428571,
      "grad_norm": 5.679604687666759,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 747
    },
    {
      "epoch": 0.8348214285714286,
      "grad_norm": 7.481594455169163,
      "learning_rate": 2e-05,
      "loss": 1.4453,
      "step": 748
    },
    {
      "epoch": 0.8359375,
      "grad_norm": 9.303365562523432,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 749
    },
    {
      "epoch": 0.8370535714285714,
      "grad_norm": 6.869555334216046,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 750
    },
    {
      "epoch": 0.8381696428571429,
      "grad_norm": 7.425644984434222,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 751
    },
    {
      "epoch": 0.8392857142857143,
      "grad_norm": 8.649820336507238,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 752
    },
    {
      "epoch": 0.8404017857142857,
      "grad_norm": 5.289604832401025,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 753
    },
    {
      "epoch": 0.8415178571428571,
      "grad_norm": 7.277697761309936,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 754
    },
    {
      "epoch": 0.8426339285714286,
      "grad_norm": 10.090745597411415,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 755
    },
    {
      "epoch": 0.84375,
      "grad_norm": 6.468005829214493,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 756
    },
    {
      "epoch": 0.8448660714285714,
      "grad_norm": 6.035282974460319,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 757
    },
    {
      "epoch": 0.8459821428571429,
      "grad_norm": 5.581858398047519,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 758
    },
    {
      "epoch": 0.8470982142857143,
      "grad_norm": 7.046060932972032,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 759
    },
    {
      "epoch": 0.8482142857142857,
      "grad_norm": 6.707705784166958,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 760
    },
    {
      "epoch": 0.8493303571428571,
      "grad_norm": 7.1766893492439525,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 761
    },
    {
      "epoch": 0.8504464285714286,
      "grad_norm": 7.933267883547185,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 762
    },
    {
      "epoch": 0.8515625,
      "grad_norm": 7.499401054942811,
      "learning_rate": 2e-05,
      "loss": 3.0,
      "step": 763
    },
    {
      "epoch": 0.8526785714285714,
      "grad_norm": 8.124846669315502,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 764
    },
    {
      "epoch": 0.8537946428571429,
      "grad_norm": 7.454956785124179,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 765
    },
    {
      "epoch": 0.8549107142857143,
      "grad_norm": 7.7574843361412835,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 766
    },
    {
      "epoch": 0.8560267857142857,
      "grad_norm": 6.671722876057147,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 767
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 9.982878960914976,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 768
    },
    {
      "epoch": 0.8582589285714286,
      "grad_norm": 7.906663708530348,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 769
    },
    {
      "epoch": 0.859375,
      "grad_norm": 5.388841252551708,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 770
    },
    {
      "epoch": 0.8604910714285714,
      "grad_norm": 5.778472453045861,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 771
    },
    {
      "epoch": 0.8616071428571429,
      "grad_norm": 7.582251002867786,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 772
    },
    {
      "epoch": 0.8627232142857143,
      "grad_norm": 8.006312658964319,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 773
    },
    {
      "epoch": 0.8638392857142857,
      "grad_norm": 7.895503372255256,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 774
    },
    {
      "epoch": 0.8649553571428571,
      "grad_norm": 7.679708314806106,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 775
    },
    {
      "epoch": 0.8660714285714286,
      "grad_norm": 7.623966888936106,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 776
    },
    {
      "epoch": 0.8671875,
      "grad_norm": 8.569040070861268,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 777
    },
    {
      "epoch": 0.8683035714285714,
      "grad_norm": 7.790891892802453,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 778
    },
    {
      "epoch": 0.8694196428571429,
      "grad_norm": 7.6613463134530235,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 779
    },
    {
      "epoch": 0.8705357142857143,
      "grad_norm": 7.0592210182544415,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 780
    },
    {
      "epoch": 0.8716517857142857,
      "grad_norm": 7.529115065034707,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 781
    },
    {
      "epoch": 0.8727678571428571,
      "grad_norm": 7.424860194958381,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 782
    },
    {
      "epoch": 0.8738839285714286,
      "grad_norm": 7.626131493352132,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 783
    },
    {
      "epoch": 0.875,
      "grad_norm": 7.876400909117194,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 784
    },
    {
      "epoch": 0.8761160714285714,
      "grad_norm": 7.870317858347874,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 785
    },
    {
      "epoch": 0.8772321428571429,
      "grad_norm": 6.9428170879681455,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 786
    },
    {
      "epoch": 0.8783482142857143,
      "grad_norm": 6.926305000659284,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 787
    },
    {
      "epoch": 0.8794642857142857,
      "grad_norm": 7.90865231714586,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 788
    },
    {
      "epoch": 0.8805803571428571,
      "grad_norm": 8.089814304439924,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 789
    },
    {
      "epoch": 0.8816964285714286,
      "grad_norm": 6.521415643620119,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 790
    },
    {
      "epoch": 0.8828125,
      "grad_norm": 9.17658570222275,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 791
    },
    {
      "epoch": 0.8839285714285714,
      "grad_norm": 7.129909145799193,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 792
    },
    {
      "epoch": 0.8850446428571429,
      "grad_norm": 6.6225561686866365,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 793
    },
    {
      "epoch": 0.8861607142857143,
      "grad_norm": 10.621948008079693,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 794
    },
    {
      "epoch": 0.8872767857142857,
      "grad_norm": 7.709827609705132,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 795
    },
    {
      "epoch": 0.8883928571428571,
      "grad_norm": 4.4416294940118295,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 796
    },
    {
      "epoch": 0.8895089285714286,
      "grad_norm": 8.362173413105783,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 797
    },
    {
      "epoch": 0.890625,
      "grad_norm": 5.68851529600558,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 798
    },
    {
      "epoch": 0.8917410714285714,
      "grad_norm": 8.356346198763275,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 799
    },
    {
      "epoch": 0.8928571428571429,
      "grad_norm": 7.762687151391418,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 800
    },
    {
      "epoch": 0.8939732142857143,
      "grad_norm": 6.5805818197757375,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 801
    },
    {
      "epoch": 0.8950892857142857,
      "grad_norm": 8.25122791610741,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 802
    },
    {
      "epoch": 0.8962053571428571,
      "grad_norm": 7.466197672511711,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 803
    },
    {
      "epoch": 0.8973214285714286,
      "grad_norm": 6.966245302351483,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 804
    },
    {
      "epoch": 0.8984375,
      "grad_norm": 6.7751623023385426,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 805
    },
    {
      "epoch": 0.8995535714285714,
      "grad_norm": 11.1491256155419,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 806
    },
    {
      "epoch": 0.9006696428571429,
      "grad_norm": 9.214656191446174,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 807
    },
    {
      "epoch": 0.9017857142857143,
      "grad_norm": 7.3660538769824395,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 808
    },
    {
      "epoch": 0.9029017857142857,
      "grad_norm": 7.429777127164266,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 809
    },
    {
      "epoch": 0.9040178571428571,
      "grad_norm": 6.883428898836841,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 810
    },
    {
      "epoch": 0.9051339285714286,
      "grad_norm": 6.320497591879356,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 811
    },
    {
      "epoch": 0.90625,
      "grad_norm": 8.43694707550417,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 812
    },
    {
      "epoch": 0.9073660714285714,
      "grad_norm": 7.132273889261657,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 813
    },
    {
      "epoch": 0.9084821428571429,
      "grad_norm": 7.385123139681288,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 814
    },
    {
      "epoch": 0.9095982142857143,
      "grad_norm": 8.061602711766193,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 815
    },
    {
      "epoch": 0.9107142857142857,
      "grad_norm": 6.572520918758388,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 816
    },
    {
      "epoch": 0.9118303571428571,
      "grad_norm": 6.915982889844661,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 817
    },
    {
      "epoch": 0.9129464285714286,
      "grad_norm": 7.724134960053868,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 818
    },
    {
      "epoch": 0.9140625,
      "grad_norm": 7.907718137289556,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 819
    },
    {
      "epoch": 0.9151785714285714,
      "grad_norm": 7.770451769764148,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 820
    },
    {
      "epoch": 0.9162946428571429,
      "grad_norm": 7.269102966777141,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 821
    },
    {
      "epoch": 0.9174107142857143,
      "grad_norm": 9.66235770318758,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 822
    },
    {
      "epoch": 0.9185267857142857,
      "grad_norm": 7.629217227520317,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 823
    },
    {
      "epoch": 0.9196428571428571,
      "grad_norm": 8.452368469098834,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 824
    },
    {
      "epoch": 0.9207589285714286,
      "grad_norm": 7.57519682426313,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 825
    },
    {
      "epoch": 0.921875,
      "grad_norm": 6.764097362158105,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 826
    },
    {
      "epoch": 0.9229910714285714,
      "grad_norm": 7.62189770198356,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 827
    },
    {
      "epoch": 0.9241071428571429,
      "grad_norm": 6.9485382094004615,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 828
    },
    {
      "epoch": 0.9252232142857143,
      "grad_norm": 7.708413330777987,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 829
    },
    {
      "epoch": 0.9263392857142857,
      "grad_norm": 8.255494452092904,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 830
    },
    {
      "epoch": 0.9274553571428571,
      "grad_norm": 6.691665341841153,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 831
    },
    {
      "epoch": 0.9285714285714286,
      "grad_norm": 8.439901138952038,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 832
    },
    {
      "epoch": 0.9296875,
      "grad_norm": 7.505067871747109,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 833
    },
    {
      "epoch": 0.9308035714285714,
      "grad_norm": 7.957964788608105,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 834
    },
    {
      "epoch": 0.9319196428571429,
      "grad_norm": 7.097183084473005,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 835
    },
    {
      "epoch": 0.9330357142857143,
      "grad_norm": 8.09717670978944,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 836
    },
    {
      "epoch": 0.9341517857142857,
      "grad_norm": 8.369563410119511,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 837
    },
    {
      "epoch": 0.9352678571428571,
      "grad_norm": 6.79453391453038,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 838
    },
    {
      "epoch": 0.9363839285714286,
      "grad_norm": 7.179151748633432,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 839
    },
    {
      "epoch": 0.9375,
      "grad_norm": 11.49101770775062,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 840
    },
    {
      "epoch": 0.9386160714285714,
      "grad_norm": 7.6322646452696254,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 841
    },
    {
      "epoch": 0.9397321428571429,
      "grad_norm": 8.569679600866866,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 842
    },
    {
      "epoch": 0.9408482142857143,
      "grad_norm": 7.924316767148923,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 843
    },
    {
      "epoch": 0.9419642857142857,
      "grad_norm": 8.190955496226874,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 844
    },
    {
      "epoch": 0.9430803571428571,
      "grad_norm": 9.056915547089309,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 845
    },
    {
      "epoch": 0.9441964285714286,
      "grad_norm": 6.240819633154437,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 846
    },
    {
      "epoch": 0.9453125,
      "grad_norm": 9.2630281755415,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 847
    },
    {
      "epoch": 0.9464285714285714,
      "grad_norm": 7.93785259584136,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 848
    },
    {
      "epoch": 0.9475446428571429,
      "grad_norm": 7.703943005271166,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 849
    },
    {
      "epoch": 0.9486607142857143,
      "grad_norm": 7.709231143205931,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 850
    },
    {
      "epoch": 0.9497767857142857,
      "grad_norm": 5.991929236206334,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 851
    },
    {
      "epoch": 0.9508928571428571,
      "grad_norm": 7.300431800124602,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 852
    },
    {
      "epoch": 0.9520089285714286,
      "grad_norm": 7.359055059295672,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 853
    },
    {
      "epoch": 0.953125,
      "grad_norm": 9.401226808903882,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 854
    },
    {
      "epoch": 0.9542410714285714,
      "grad_norm": 8.234120994876932,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 855
    },
    {
      "epoch": 0.9553571428571429,
      "grad_norm": 7.652783252392185,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 856
    },
    {
      "epoch": 0.9564732142857143,
      "grad_norm": 8.387724213253467,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 857
    },
    {
      "epoch": 0.9575892857142857,
      "grad_norm": 8.80450130650201,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 858
    },
    {
      "epoch": 0.9587053571428571,
      "grad_norm": 7.2414223383416765,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 859
    },
    {
      "epoch": 0.9598214285714286,
      "grad_norm": 6.277292458914491,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 860
    },
    {
      "epoch": 0.9609375,
      "grad_norm": 8.425193085914424,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 861
    },
    {
      "epoch": 0.9620535714285714,
      "grad_norm": 8.828768431233788,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 862
    },
    {
      "epoch": 0.9631696428571429,
      "grad_norm": 5.664319232822921,
      "learning_rate": 2e-05,
      "loss": 2.8281,
      "step": 863
    },
    {
      "epoch": 0.9642857142857143,
      "grad_norm": 7.196473451673784,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 864
    },
    {
      "epoch": 0.9654017857142857,
      "grad_norm": 9.109002661960586,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 865
    },
    {
      "epoch": 0.9665178571428571,
      "grad_norm": 7.3153509671100325,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 866
    },
    {
      "epoch": 0.9676339285714286,
      "grad_norm": 7.746957519809649,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 867
    },
    {
      "epoch": 0.96875,
      "grad_norm": 8.71259281907862,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 868
    },
    {
      "epoch": 0.9698660714285714,
      "grad_norm": 8.406368992644454,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 869
    },
    {
      "epoch": 0.9709821428571429,
      "grad_norm": 7.8730859516977505,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 870
    },
    {
      "epoch": 0.9720982142857143,
      "grad_norm": 7.363588014310592,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 871
    },
    {
      "epoch": 0.9732142857142857,
      "grad_norm": 7.384399565939538,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 872
    },
    {
      "epoch": 0.9743303571428571,
      "grad_norm": 6.48682564127364,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 873
    },
    {
      "epoch": 0.9754464285714286,
      "grad_norm": 8.175047863768238,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 874
    },
    {
      "epoch": 0.9765625,
      "grad_norm": 9.490286332023473,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 875
    },
    {
      "epoch": 0.9776785714285714,
      "grad_norm": 8.296203084604528,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 876
    },
    {
      "epoch": 0.9787946428571429,
      "grad_norm": 7.562503388027406,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 877
    },
    {
      "epoch": 0.9799107142857143,
      "grad_norm": 7.363696290211345,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 878
    },
    {
      "epoch": 0.9810267857142857,
      "grad_norm": 8.548006655887228,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 879
    },
    {
      "epoch": 0.9821428571428571,
      "grad_norm": 7.3071845684102605,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 880
    },
    {
      "epoch": 0.9832589285714286,
      "grad_norm": 7.967861274278176,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 881
    },
    {
      "epoch": 0.984375,
      "grad_norm": 8.45353461545602,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 882
    },
    {
      "epoch": 0.9854910714285714,
      "grad_norm": 7.576104875232474,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 883
    },
    {
      "epoch": 0.9866071428571429,
      "grad_norm": 8.840917981292552,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 884
    },
    {
      "epoch": 0.9877232142857143,
      "grad_norm": 7.028927444049435,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 885
    },
    {
      "epoch": 0.9888392857142857,
      "grad_norm": 8.935505644650485,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 886
    },
    {
      "epoch": 0.9899553571428571,
      "grad_norm": 6.885889557990625,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 887
    },
    {
      "epoch": 0.9910714285714286,
      "grad_norm": 7.473054535197915,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 888
    },
    {
      "epoch": 0.9921875,
      "grad_norm": 7.659368125704597,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 889
    },
    {
      "epoch": 0.9933035714285714,
      "grad_norm": 7.765536772815751,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 890
    },
    {
      "epoch": 0.9944196428571429,
      "grad_norm": 6.4654037182918564,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 891
    },
    {
      "epoch": 0.9955357142857143,
      "grad_norm": 8.559616449987061,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 892
    },
    {
      "epoch": 0.9966517857142857,
      "grad_norm": 6.7653584756777745,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 893
    },
    {
      "epoch": 0.9977678571428571,
      "grad_norm": 9.255095539737054,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 894
    },
    {
      "epoch": 0.9988839285714286,
      "grad_norm": 9.094258407653669,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 895
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.574840564248845,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 896
    },
    {
      "epoch": 1.0011160714285714,
      "grad_norm": 7.836884517044679,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 897
    },
    {
      "epoch": 1.0022321428571428,
      "grad_norm": 10.36284303856991,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 898
    },
    {
      "epoch": 1.0033482142857142,
      "grad_norm": 7.736698729434541,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 899
    },
    {
      "epoch": 1.0044642857142858,
      "grad_norm": 7.897631851874295,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 900
    },
    {
      "epoch": 1.0055803571428572,
      "grad_norm": 8.791893446111258,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 901
    },
    {
      "epoch": 1.0066964285714286,
      "grad_norm": 5.350713395040623,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 902
    },
    {
      "epoch": 1.0078125,
      "grad_norm": 7.75551036622426,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 903
    },
    {
      "epoch": 1.0089285714285714,
      "grad_norm": 6.11607642089473,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 904
    },
    {
      "epoch": 1.0100446428571428,
      "grad_norm": 5.801975086385737,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 905
    },
    {
      "epoch": 1.0111607142857142,
      "grad_norm": 4.970167692719291,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 906
    },
    {
      "epoch": 1.0122767857142858,
      "grad_norm": 6.951520488932862,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 907
    },
    {
      "epoch": 1.0133928571428572,
      "grad_norm": 6.674878701797229,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 908
    },
    {
      "epoch": 1.0145089285714286,
      "grad_norm": 6.5995679214515235,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 909
    },
    {
      "epoch": 1.015625,
      "grad_norm": 7.122932071524123,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 910
    },
    {
      "epoch": 1.0167410714285714,
      "grad_norm": 7.287356044674242,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 911
    },
    {
      "epoch": 1.0178571428571428,
      "grad_norm": 8.164035525736338,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 912
    },
    {
      "epoch": 1.0189732142857142,
      "grad_norm": 8.062432584109649,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 913
    },
    {
      "epoch": 1.0200892857142858,
      "grad_norm": 6.797912039392409,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 914
    },
    {
      "epoch": 1.0212053571428572,
      "grad_norm": 7.111123497908928,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 915
    },
    {
      "epoch": 1.0223214285714286,
      "grad_norm": 7.581576086747828,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 916
    },
    {
      "epoch": 1.0234375,
      "grad_norm": 7.424266571047113,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 917
    },
    {
      "epoch": 1.0245535714285714,
      "grad_norm": 6.528291360655316,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 918
    },
    {
      "epoch": 1.0256696428571428,
      "grad_norm": 8.246820967136108,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 919
    },
    {
      "epoch": 1.0267857142857142,
      "grad_norm": 8.128742999479359,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 920
    },
    {
      "epoch": 1.0279017857142858,
      "grad_norm": 5.899088100130955,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 921
    },
    {
      "epoch": 1.0290178571428572,
      "grad_norm": 5.9160859383423805,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 922
    },
    {
      "epoch": 1.0301339285714286,
      "grad_norm": 7.5461543233467765,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 923
    },
    {
      "epoch": 1.03125,
      "grad_norm": 7.544074649362211,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 924
    },
    {
      "epoch": 1.0323660714285714,
      "grad_norm": 7.76883865725328,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 925
    },
    {
      "epoch": 1.0334821428571428,
      "grad_norm": 8.583870587598536,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 926
    },
    {
      "epoch": 1.0345982142857142,
      "grad_norm": 7.487283711104325,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 927
    },
    {
      "epoch": 1.0357142857142858,
      "grad_norm": 9.107828902718827,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 928
    },
    {
      "epoch": 1.0368303571428572,
      "grad_norm": 8.895168222602944,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 929
    },
    {
      "epoch": 1.0379464285714286,
      "grad_norm": 7.863377387728573,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 930
    },
    {
      "epoch": 1.0390625,
      "grad_norm": 9.202896610015648,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 931
    },
    {
      "epoch": 1.0401785714285714,
      "grad_norm": 9.087757657779905,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 932
    },
    {
      "epoch": 1.0412946428571428,
      "grad_norm": 11.691979485750268,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 933
    },
    {
      "epoch": 1.0424107142857142,
      "grad_norm": 9.715944800997388,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 934
    },
    {
      "epoch": 1.0435267857142858,
      "grad_norm": 7.672755440691134,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 935
    },
    {
      "epoch": 1.0446428571428572,
      "grad_norm": 9.925779036540169,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 936
    },
    {
      "epoch": 1.0457589285714286,
      "grad_norm": 9.401621647923983,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 937
    },
    {
      "epoch": 1.046875,
      "grad_norm": 7.650264470374535,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 938
    },
    {
      "epoch": 1.0479910714285714,
      "grad_norm": 8.727995572648483,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 939
    },
    {
      "epoch": 1.0491071428571428,
      "grad_norm": 7.893928273255643,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 940
    },
    {
      "epoch": 1.0502232142857142,
      "grad_norm": 7.672924768594406,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 941
    },
    {
      "epoch": 1.0513392857142858,
      "grad_norm": 8.082149472744147,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 942
    },
    {
      "epoch": 1.0524553571428572,
      "grad_norm": 8.218914072374437,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 943
    },
    {
      "epoch": 1.0535714285714286,
      "grad_norm": 8.237424693387517,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 944
    },
    {
      "epoch": 1.0546875,
      "grad_norm": 8.797575395356088,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 945
    },
    {
      "epoch": 1.0558035714285714,
      "grad_norm": 7.594985207615466,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 946
    },
    {
      "epoch": 1.0569196428571428,
      "grad_norm": 7.744604566593887,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 947
    },
    {
      "epoch": 1.0580357142857142,
      "grad_norm": 8.48223230680141,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 948
    },
    {
      "epoch": 1.0591517857142858,
      "grad_norm": 8.902059912207811,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 949
    },
    {
      "epoch": 1.0602678571428572,
      "grad_norm": 7.019526192448254,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 950
    },
    {
      "epoch": 1.0613839285714286,
      "grad_norm": 8.716108099803701,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 951
    },
    {
      "epoch": 1.0625,
      "grad_norm": 9.668806835097852,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 952
    },
    {
      "epoch": 1.0636160714285714,
      "grad_norm": 9.103669555002366,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 953
    },
    {
      "epoch": 1.0647321428571428,
      "grad_norm": 7.803717006126733,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 954
    },
    {
      "epoch": 1.0658482142857142,
      "grad_norm": 9.779885447179158,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 955
    },
    {
      "epoch": 1.0669642857142858,
      "grad_norm": 8.960286891772821,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 956
    },
    {
      "epoch": 1.0680803571428572,
      "grad_norm": 6.929278173212102,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 957
    },
    {
      "epoch": 1.0691964285714286,
      "grad_norm": 7.989872824532442,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 958
    },
    {
      "epoch": 1.0703125,
      "grad_norm": 9.681681845923386,
      "learning_rate": 2e-05,
      "loss": 1.3516,
      "step": 959
    },
    {
      "epoch": 1.0714285714285714,
      "grad_norm": 8.473422556227794,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 960
    },
    {
      "epoch": 1.0725446428571428,
      "grad_norm": 8.116099409396508,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 961
    },
    {
      "epoch": 1.0736607142857142,
      "grad_norm": 8.935491306839863,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 962
    },
    {
      "epoch": 1.0747767857142858,
      "grad_norm": 7.938910962515295,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 963
    },
    {
      "epoch": 1.0758928571428572,
      "grad_norm": 7.76380593369636,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 964
    },
    {
      "epoch": 1.0770089285714286,
      "grad_norm": 8.5324099808491,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 965
    },
    {
      "epoch": 1.078125,
      "grad_norm": 7.069390423849135,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 966
    },
    {
      "epoch": 1.0792410714285714,
      "grad_norm": 6.732082234817774,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 967
    },
    {
      "epoch": 1.0803571428571428,
      "grad_norm": 7.473161761558064,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 968
    },
    {
      "epoch": 1.0814732142857142,
      "grad_norm": 8.90118774959711,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 969
    },
    {
      "epoch": 1.0825892857142858,
      "grad_norm": 9.210608618649541,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 970
    },
    {
      "epoch": 1.0837053571428572,
      "grad_norm": 8.325153440443083,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 971
    },
    {
      "epoch": 1.0848214285714286,
      "grad_norm": 7.81906056600338,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 972
    },
    {
      "epoch": 1.0859375,
      "grad_norm": 8.917449349581728,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 973
    },
    {
      "epoch": 1.0870535714285714,
      "grad_norm": 7.981455137513596,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 974
    },
    {
      "epoch": 1.0881696428571428,
      "grad_norm": 8.451815574715173,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 975
    },
    {
      "epoch": 1.0892857142857142,
      "grad_norm": 7.268212391740938,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 976
    },
    {
      "epoch": 1.0904017857142858,
      "grad_norm": 7.658409230270069,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 977
    },
    {
      "epoch": 1.0915178571428572,
      "grad_norm": 15.15205551152532,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 978
    },
    {
      "epoch": 1.0926339285714286,
      "grad_norm": 7.086359731314495,
      "learning_rate": 2e-05,
      "loss": 1.5312,
      "step": 979
    },
    {
      "epoch": 1.09375,
      "grad_norm": 7.619627171623271,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 980
    },
    {
      "epoch": 1.0948660714285714,
      "grad_norm": 8.707069423125219,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 981
    },
    {
      "epoch": 1.0959821428571428,
      "grad_norm": 9.100573958393236,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 982
    },
    {
      "epoch": 1.0970982142857142,
      "grad_norm": 9.26675235352146,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 983
    },
    {
      "epoch": 1.0982142857142858,
      "grad_norm": 8.44603573421521,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 984
    },
    {
      "epoch": 1.0993303571428572,
      "grad_norm": 9.204729369822285,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 985
    },
    {
      "epoch": 1.1004464285714286,
      "grad_norm": 7.983650234016037,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 986
    },
    {
      "epoch": 1.1015625,
      "grad_norm": 8.473150206530828,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 987
    },
    {
      "epoch": 1.1026785714285714,
      "grad_norm": 10.137037697407544,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 988
    },
    {
      "epoch": 1.1037946428571428,
      "grad_norm": 8.533561469453012,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 989
    },
    {
      "epoch": 1.1049107142857142,
      "grad_norm": 7.763177692201899,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 990
    },
    {
      "epoch": 1.1060267857142858,
      "grad_norm": 8.838175342696703,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 991
    },
    {
      "epoch": 1.1071428571428572,
      "grad_norm": 8.077163091266563,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 992
    },
    {
      "epoch": 1.1082589285714286,
      "grad_norm": 10.46047352204952,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 993
    },
    {
      "epoch": 1.109375,
      "grad_norm": 8.202858808069486,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 994
    },
    {
      "epoch": 1.1104910714285714,
      "grad_norm": 8.10209202916333,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 995
    },
    {
      "epoch": 1.1116071428571428,
      "grad_norm": 9.88314368222718,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 996
    },
    {
      "epoch": 1.1127232142857142,
      "grad_norm": 8.811686493964896,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 997
    },
    {
      "epoch": 1.1138392857142858,
      "grad_norm": 8.454040115631631,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 998
    },
    {
      "epoch": 1.1149553571428572,
      "grad_norm": 8.627732956717143,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 999
    },
    {
      "epoch": 1.1160714285714286,
      "grad_norm": 10.865512946521067,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1000
    },
    {
      "epoch": 1.1171875,
      "grad_norm": 10.72607964625994,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1001
    },
    {
      "epoch": 1.1183035714285714,
      "grad_norm": 8.044263677177966,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1002
    },
    {
      "epoch": 1.1194196428571428,
      "grad_norm": 8.549829168232444,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1003
    },
    {
      "epoch": 1.1205357142857142,
      "grad_norm": 9.930080761861092,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1004
    },
    {
      "epoch": 1.1216517857142858,
      "grad_norm": 7.3156586635194785,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 1005
    },
    {
      "epoch": 1.1227678571428572,
      "grad_norm": 7.796179714088146,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 1006
    },
    {
      "epoch": 1.1238839285714286,
      "grad_norm": 8.59099460751913,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1007
    },
    {
      "epoch": 1.125,
      "grad_norm": 7.416425829454912,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1008
    },
    {
      "epoch": 1.1261160714285714,
      "grad_norm": 8.734741268066015,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1009
    },
    {
      "epoch": 1.1272321428571428,
      "grad_norm": 8.515601806655788,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1010
    },
    {
      "epoch": 1.1283482142857142,
      "grad_norm": 8.579207310722206,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1011
    },
    {
      "epoch": 1.1294642857142858,
      "grad_norm": 8.992077433676771,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1012
    },
    {
      "epoch": 1.1305803571428572,
      "grad_norm": 8.02603237095462,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1013
    },
    {
      "epoch": 1.1316964285714286,
      "grad_norm": 8.129177794054137,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 1014
    },
    {
      "epoch": 1.1328125,
      "grad_norm": 6.4617671760269575,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 1015
    },
    {
      "epoch": 1.1339285714285714,
      "grad_norm": 7.842347044503524,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1016
    },
    {
      "epoch": 1.1350446428571428,
      "grad_norm": 8.359204361163812,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 1017
    },
    {
      "epoch": 1.1361607142857142,
      "grad_norm": 7.221328082768517,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 1018
    },
    {
      "epoch": 1.1372767857142858,
      "grad_norm": 8.987926647712927,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 1019
    },
    {
      "epoch": 1.1383928571428572,
      "grad_norm": 8.287388645727265,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 1020
    },
    {
      "epoch": 1.1395089285714286,
      "grad_norm": 8.970566659578541,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1021
    },
    {
      "epoch": 1.140625,
      "grad_norm": 7.602371479050258,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1022
    },
    {
      "epoch": 1.1417410714285714,
      "grad_norm": 7.0423087066433805,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1023
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 8.26616829713952,
      "learning_rate": 2e-05,
      "loss": 1.4141,
      "step": 1024
    },
    {
      "epoch": 1.1439732142857142,
      "grad_norm": 9.308414100971007,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1025
    },
    {
      "epoch": 1.1450892857142858,
      "grad_norm": 7.7435514160514725,
      "learning_rate": 2e-05,
      "loss": 1.5312,
      "step": 1026
    },
    {
      "epoch": 1.1462053571428572,
      "grad_norm": 9.942964338737516,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1027
    },
    {
      "epoch": 1.1473214285714286,
      "grad_norm": 6.118435737508498,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1028
    },
    {
      "epoch": 1.1484375,
      "grad_norm": 7.575108680752182,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1029
    },
    {
      "epoch": 1.1495535714285714,
      "grad_norm": 9.651488649970341,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1030
    },
    {
      "epoch": 1.1506696428571428,
      "grad_norm": 10.604687692639441,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1031
    },
    {
      "epoch": 1.1517857142857142,
      "grad_norm": 7.6076302546350165,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1032
    },
    {
      "epoch": 1.1529017857142858,
      "grad_norm": 8.062805893512195,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 1033
    },
    {
      "epoch": 1.1540178571428572,
      "grad_norm": 9.385699086030966,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 1034
    },
    {
      "epoch": 1.1551339285714286,
      "grad_norm": 8.19347330001589,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1035
    },
    {
      "epoch": 1.15625,
      "grad_norm": 7.635984178539151,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1036
    },
    {
      "epoch": 1.1573660714285714,
      "grad_norm": 8.512759836915512,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1037
    },
    {
      "epoch": 1.1584821428571428,
      "grad_norm": 8.005028976718982,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 1038
    },
    {
      "epoch": 1.1595982142857142,
      "grad_norm": 8.779959280610704,
      "learning_rate": 2e-05,
      "loss": 1.3438,
      "step": 1039
    },
    {
      "epoch": 1.1607142857142858,
      "grad_norm": 8.268418754102159,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1040
    },
    {
      "epoch": 1.1618303571428572,
      "grad_norm": 9.639217626370298,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1041
    },
    {
      "epoch": 1.1629464285714286,
      "grad_norm": 8.953470040388979,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1042
    },
    {
      "epoch": 1.1640625,
      "grad_norm": 8.579779110025848,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 1043
    },
    {
      "epoch": 1.1651785714285714,
      "grad_norm": 10.551453791998298,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1044
    },
    {
      "epoch": 1.1662946428571428,
      "grad_norm": 7.4168160983293445,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 1045
    },
    {
      "epoch": 1.1674107142857142,
      "grad_norm": 8.604528361185546,
      "learning_rate": 2e-05,
      "loss": 3.0312,
      "step": 1046
    },
    {
      "epoch": 1.1685267857142858,
      "grad_norm": 9.67384357023731,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1047
    },
    {
      "epoch": 1.1696428571428572,
      "grad_norm": 8.529282039894515,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1048
    },
    {
      "epoch": 1.1707589285714286,
      "grad_norm": 9.724380017630958,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1049
    },
    {
      "epoch": 1.171875,
      "grad_norm": 9.283640777875414,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1050
    },
    {
      "epoch": 1.1729910714285714,
      "grad_norm": 7.358900030024502,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1051
    },
    {
      "epoch": 1.1741071428571428,
      "grad_norm": 8.591281730702661,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1052
    },
    {
      "epoch": 1.1752232142857142,
      "grad_norm": 8.51461462420764,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1053
    },
    {
      "epoch": 1.1763392857142858,
      "grad_norm": 7.686485750424307,
      "learning_rate": 2e-05,
      "loss": 3.1094,
      "step": 1054
    },
    {
      "epoch": 1.1774553571428572,
      "grad_norm": 8.334625728165836,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1055
    },
    {
      "epoch": 1.1785714285714286,
      "grad_norm": 7.923350342359338,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1056
    },
    {
      "epoch": 1.1796875,
      "grad_norm": 9.293639327481415,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1057
    },
    {
      "epoch": 1.1808035714285714,
      "grad_norm": 7.737544348368465,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1058
    },
    {
      "epoch": 1.1819196428571428,
      "grad_norm": 7.231315275476082,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1059
    },
    {
      "epoch": 1.1830357142857142,
      "grad_norm": 8.457553593564558,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1060
    },
    {
      "epoch": 1.1841517857142858,
      "grad_norm": 9.273286248433536,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1061
    },
    {
      "epoch": 1.1852678571428572,
      "grad_norm": 8.640210076257679,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1062
    },
    {
      "epoch": 1.1863839285714286,
      "grad_norm": 8.3649142904507,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1063
    },
    {
      "epoch": 1.1875,
      "grad_norm": 10.601583963696328,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 1064
    },
    {
      "epoch": 1.1886160714285714,
      "grad_norm": 8.054787520446672,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 1065
    },
    {
      "epoch": 1.1897321428571428,
      "grad_norm": 8.762124703681765,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1066
    },
    {
      "epoch": 1.1908482142857142,
      "grad_norm": 9.698669373532327,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1067
    },
    {
      "epoch": 1.1919642857142858,
      "grad_norm": 8.757127486565853,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1068
    },
    {
      "epoch": 1.1930803571428572,
      "grad_norm": 8.32796346881753,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 1069
    },
    {
      "epoch": 1.1941964285714286,
      "grad_norm": 12.131552225431005,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 1070
    },
    {
      "epoch": 1.1953125,
      "grad_norm": 8.711621821851143,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1071
    },
    {
      "epoch": 1.1964285714285714,
      "grad_norm": 11.268444245472358,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1072
    },
    {
      "epoch": 1.1975446428571428,
      "grad_norm": 7.587655533704155,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 1073
    },
    {
      "epoch": 1.1986607142857142,
      "grad_norm": 7.560455332911672,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1074
    },
    {
      "epoch": 1.1997767857142858,
      "grad_norm": 9.461187203308292,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1075
    },
    {
      "epoch": 1.2008928571428572,
      "grad_norm": 8.556946347027845,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1076
    },
    {
      "epoch": 1.2020089285714286,
      "grad_norm": 8.608771441498698,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1077
    },
    {
      "epoch": 1.203125,
      "grad_norm": 9.717470201256367,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1078
    },
    {
      "epoch": 1.2042410714285714,
      "grad_norm": 7.7875110255487785,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1079
    },
    {
      "epoch": 1.2053571428571428,
      "grad_norm": 7.92812078374669,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 1080
    },
    {
      "epoch": 1.2064732142857142,
      "grad_norm": 9.795563262556863,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1081
    },
    {
      "epoch": 1.2075892857142858,
      "grad_norm": 7.890773681612872,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1082
    },
    {
      "epoch": 1.2087053571428572,
      "grad_norm": 9.258822404318401,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1083
    },
    {
      "epoch": 1.2098214285714286,
      "grad_norm": 9.200110660402915,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 1084
    },
    {
      "epoch": 1.2109375,
      "grad_norm": 7.193216243449202,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1085
    },
    {
      "epoch": 1.2120535714285714,
      "grad_norm": 7.695065753558855,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1086
    },
    {
      "epoch": 1.2131696428571428,
      "grad_norm": 9.483700741830932,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1087
    },
    {
      "epoch": 1.2142857142857142,
      "grad_norm": 8.18977507935526,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1088
    },
    {
      "epoch": 1.2154017857142858,
      "grad_norm": 7.927273611657882,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1089
    },
    {
      "epoch": 1.2165178571428572,
      "grad_norm": 7.345141593857942,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1090
    },
    {
      "epoch": 1.2176339285714286,
      "grad_norm": 8.793419555524935,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1091
    },
    {
      "epoch": 1.21875,
      "grad_norm": 8.954950863624344,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1092
    },
    {
      "epoch": 1.2198660714285714,
      "grad_norm": 7.051559454817158,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1093
    },
    {
      "epoch": 1.2209821428571428,
      "grad_norm": 7.396807715600818,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1094
    },
    {
      "epoch": 1.2220982142857142,
      "grad_norm": 7.273320516778204,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1095
    },
    {
      "epoch": 1.2232142857142858,
      "grad_norm": 7.332871683657693,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1096
    },
    {
      "epoch": 1.2243303571428572,
      "grad_norm": 7.180584955556742,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1097
    },
    {
      "epoch": 1.2254464285714286,
      "grad_norm": 9.073520396693636,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1098
    },
    {
      "epoch": 1.2265625,
      "grad_norm": 8.94497967541895,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1099
    },
    {
      "epoch": 1.2276785714285714,
      "grad_norm": 7.433490879587775,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1100
    },
    {
      "epoch": 1.2287946428571428,
      "grad_norm": 7.088334888936316,
      "learning_rate": 2e-05,
      "loss": 2.9062,
      "step": 1101
    },
    {
      "epoch": 1.2299107142857142,
      "grad_norm": 9.365330016231676,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1102
    },
    {
      "epoch": 1.2310267857142858,
      "grad_norm": 7.501848525083746,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1103
    },
    {
      "epoch": 1.2321428571428572,
      "grad_norm": 9.800509940163135,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1104
    },
    {
      "epoch": 1.2332589285714286,
      "grad_norm": 5.433651708390468,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1105
    },
    {
      "epoch": 1.234375,
      "grad_norm": 6.9717640879185945,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 1106
    },
    {
      "epoch": 1.2354910714285714,
      "grad_norm": 7.416075213593145,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1107
    },
    {
      "epoch": 1.2366071428571428,
      "grad_norm": 7.002940289490218,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1108
    },
    {
      "epoch": 1.2377232142857142,
      "grad_norm": 8.323599551129078,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1109
    },
    {
      "epoch": 1.2388392857142858,
      "grad_norm": 8.801550925084038,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1110
    },
    {
      "epoch": 1.2399553571428572,
      "grad_norm": 7.47376197990806,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1111
    },
    {
      "epoch": 1.2410714285714286,
      "grad_norm": 9.477262121320086,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1112
    },
    {
      "epoch": 1.2421875,
      "grad_norm": 9.750027813048904,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1113
    },
    {
      "epoch": 1.2433035714285714,
      "grad_norm": 7.6224898842338,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1114
    },
    {
      "epoch": 1.2444196428571428,
      "grad_norm": 8.579818516436614,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1115
    },
    {
      "epoch": 1.2455357142857142,
      "grad_norm": 8.386939561711719,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1116
    },
    {
      "epoch": 1.2466517857142858,
      "grad_norm": 9.272759645162939,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1117
    },
    {
      "epoch": 1.2477678571428572,
      "grad_norm": 9.781895226037618,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1118
    },
    {
      "epoch": 1.2488839285714286,
      "grad_norm": 9.594489196191308,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1119
    },
    {
      "epoch": 1.25,
      "grad_norm": 7.779636137947398,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1120
    },
    {
      "epoch": 1.2511160714285714,
      "grad_norm": 10.6243789007046,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1121
    },
    {
      "epoch": 1.2522321428571428,
      "grad_norm": 9.123386302121386,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1122
    },
    {
      "epoch": 1.2533482142857144,
      "grad_norm": 6.258053392685313,
      "learning_rate": 2e-05,
      "loss": 2.7656,
      "step": 1123
    },
    {
      "epoch": 1.2544642857142856,
      "grad_norm": 9.308791084917267,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1124
    },
    {
      "epoch": 1.2555803571428572,
      "grad_norm": 7.973828682349559,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1125
    },
    {
      "epoch": 1.2566964285714286,
      "grad_norm": 7.783847235239663,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1126
    },
    {
      "epoch": 1.2578125,
      "grad_norm": 8.328531522778615,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1127
    },
    {
      "epoch": 1.2589285714285714,
      "grad_norm": 9.591850186939295,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1128
    },
    {
      "epoch": 1.2600446428571428,
      "grad_norm": 9.313474808351392,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1129
    },
    {
      "epoch": 1.2611607142857144,
      "grad_norm": 9.17328491362963,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1130
    },
    {
      "epoch": 1.2622767857142856,
      "grad_norm": 9.278073709501776,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1131
    },
    {
      "epoch": 1.2633928571428572,
      "grad_norm": 8.966381934071176,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1132
    },
    {
      "epoch": 1.2645089285714286,
      "grad_norm": 10.849479065898334,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1133
    },
    {
      "epoch": 1.265625,
      "grad_norm": 7.6207994293623535,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1134
    },
    {
      "epoch": 1.2667410714285714,
      "grad_norm": 8.975259777546169,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1135
    },
    {
      "epoch": 1.2678571428571428,
      "grad_norm": 10.172079233832857,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1136
    },
    {
      "epoch": 1.2689732142857144,
      "grad_norm": 9.582853005773998,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1137
    },
    {
      "epoch": 1.2700892857142856,
      "grad_norm": 9.370396225075398,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1138
    },
    {
      "epoch": 1.2712053571428572,
      "grad_norm": 8.711094177569574,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1139
    },
    {
      "epoch": 1.2723214285714286,
      "grad_norm": 8.137463629134196,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 1140
    },
    {
      "epoch": 1.2734375,
      "grad_norm": 8.797478627555426,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 1141
    },
    {
      "epoch": 1.2745535714285714,
      "grad_norm": 9.300867451972676,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1142
    },
    {
      "epoch": 1.2756696428571428,
      "grad_norm": 6.922886751208065,
      "learning_rate": 2e-05,
      "loss": 3.0938,
      "step": 1143
    },
    {
      "epoch": 1.2767857142857144,
      "grad_norm": 8.021524858804636,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1144
    },
    {
      "epoch": 1.2779017857142856,
      "grad_norm": 8.209953593456431,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 1145
    },
    {
      "epoch": 1.2790178571428572,
      "grad_norm": 9.789382928443686,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1146
    },
    {
      "epoch": 1.2801339285714286,
      "grad_norm": 11.997242478403107,
      "learning_rate": 2e-05,
      "loss": 1.4062,
      "step": 1147
    },
    {
      "epoch": 1.28125,
      "grad_norm": 7.894543550784331,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1148
    },
    {
      "epoch": 1.2823660714285714,
      "grad_norm": 8.330450785563997,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1149
    },
    {
      "epoch": 1.2834821428571428,
      "grad_norm": 14.764561944864528,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1150
    },
    {
      "epoch": 1.2845982142857144,
      "grad_norm": 9.593684153020178,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1151
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 8.816782996412055,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1152
    },
    {
      "epoch": 1.2868303571428572,
      "grad_norm": 10.508542361930159,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1153
    },
    {
      "epoch": 1.2879464285714286,
      "grad_norm": 8.570274601179115,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1154
    },
    {
      "epoch": 1.2890625,
      "grad_norm": 8.369056932164737,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1155
    },
    {
      "epoch": 1.2901785714285714,
      "grad_norm": 6.596977175743173,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1156
    },
    {
      "epoch": 1.2912946428571428,
      "grad_norm": 8.989287574747005,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1157
    },
    {
      "epoch": 1.2924107142857144,
      "grad_norm": 9.411917641431922,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1158
    },
    {
      "epoch": 1.2935267857142856,
      "grad_norm": 8.896835711962666,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1159
    },
    {
      "epoch": 1.2946428571428572,
      "grad_norm": 8.209105704049929,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1160
    },
    {
      "epoch": 1.2957589285714286,
      "grad_norm": 9.410771645945152,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1161
    },
    {
      "epoch": 1.296875,
      "grad_norm": 8.624665570177559,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1162
    },
    {
      "epoch": 1.2979910714285714,
      "grad_norm": 8.286585118761423,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1163
    },
    {
      "epoch": 1.2991071428571428,
      "grad_norm": 10.851284576159845,
      "learning_rate": 2e-05,
      "loss": 1.3281,
      "step": 1164
    },
    {
      "epoch": 1.3002232142857144,
      "grad_norm": 9.135330665471441,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1165
    },
    {
      "epoch": 1.3013392857142856,
      "grad_norm": 10.167618742023384,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1166
    },
    {
      "epoch": 1.3024553571428572,
      "grad_norm": 9.620639593277623,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1167
    },
    {
      "epoch": 1.3035714285714286,
      "grad_norm": 8.03741849248842,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1168
    },
    {
      "epoch": 1.3046875,
      "grad_norm": 9.867957511031967,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1169
    },
    {
      "epoch": 1.3058035714285714,
      "grad_norm": 7.717896910066155,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1170
    },
    {
      "epoch": 1.3069196428571428,
      "grad_norm": 9.777731918416134,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1171
    },
    {
      "epoch": 1.3080357142857144,
      "grad_norm": 11.117498258752406,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1172
    },
    {
      "epoch": 1.3091517857142856,
      "grad_norm": 7.4903544784898255,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1173
    },
    {
      "epoch": 1.3102678571428572,
      "grad_norm": 10.974716339687923,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1174
    },
    {
      "epoch": 1.3113839285714286,
      "grad_norm": 10.135457743163899,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1175
    },
    {
      "epoch": 1.3125,
      "grad_norm": 8.268364851943613,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1176
    },
    {
      "epoch": 1.3136160714285714,
      "grad_norm": 10.235575709805547,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 1177
    },
    {
      "epoch": 1.3147321428571428,
      "grad_norm": 8.71103065038037,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1178
    },
    {
      "epoch": 1.3158482142857144,
      "grad_norm": 9.261195602404078,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1179
    },
    {
      "epoch": 1.3169642857142856,
      "grad_norm": 7.545785946999469,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1180
    },
    {
      "epoch": 1.3180803571428572,
      "grad_norm": 9.102385022991381,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1181
    },
    {
      "epoch": 1.3191964285714286,
      "grad_norm": 9.264027113282925,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1182
    },
    {
      "epoch": 1.3203125,
      "grad_norm": 9.384896620436999,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1183
    },
    {
      "epoch": 1.3214285714285714,
      "grad_norm": 8.262065278480032,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1184
    },
    {
      "epoch": 1.3225446428571428,
      "grad_norm": 10.396835398587372,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1185
    },
    {
      "epoch": 1.3236607142857144,
      "grad_norm": 8.72527311956647,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1186
    },
    {
      "epoch": 1.3247767857142856,
      "grad_norm": 8.720779476449058,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1187
    },
    {
      "epoch": 1.3258928571428572,
      "grad_norm": 9.397950899720895,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1188
    },
    {
      "epoch": 1.3270089285714286,
      "grad_norm": 7.895769998445729,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1189
    },
    {
      "epoch": 1.328125,
      "grad_norm": 8.635018496329652,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1190
    },
    {
      "epoch": 1.3292410714285714,
      "grad_norm": 10.627723842876042,
      "learning_rate": 2e-05,
      "loss": 1.5,
      "step": 1191
    },
    {
      "epoch": 1.3303571428571428,
      "grad_norm": 7.509472199583603,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1192
    },
    {
      "epoch": 1.3314732142857144,
      "grad_norm": 8.972348640164384,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 1193
    },
    {
      "epoch": 1.3325892857142856,
      "grad_norm": 8.963677823419317,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1194
    },
    {
      "epoch": 1.3337053571428572,
      "grad_norm": 10.913900657621017,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1195
    },
    {
      "epoch": 1.3348214285714286,
      "grad_norm": 9.393144920900719,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1196
    },
    {
      "epoch": 1.3359375,
      "grad_norm": 8.116816043728063,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1197
    },
    {
      "epoch": 1.3370535714285714,
      "grad_norm": 9.595421232775031,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1198
    },
    {
      "epoch": 1.3381696428571428,
      "grad_norm": 9.685649730639401,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1199
    },
    {
      "epoch": 1.3392857142857144,
      "grad_norm": 7.4569866475420055,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1200
    },
    {
      "epoch": 1.3404017857142856,
      "grad_norm": 7.592941596952568,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 1201
    },
    {
      "epoch": 1.3415178571428572,
      "grad_norm": 8.343161606729224,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1202
    },
    {
      "epoch": 1.3426339285714286,
      "grad_norm": 9.556678114992287,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1203
    },
    {
      "epoch": 1.34375,
      "grad_norm": 9.99194397519716,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1204
    },
    {
      "epoch": 1.3448660714285714,
      "grad_norm": 7.989208631286538,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 1205
    },
    {
      "epoch": 1.3459821428571428,
      "grad_norm": 10.940848690752347,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 1206
    },
    {
      "epoch": 1.3470982142857144,
      "grad_norm": 7.6599173637343245,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1207
    },
    {
      "epoch": 1.3482142857142856,
      "grad_norm": 9.420871136343617,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1208
    },
    {
      "epoch": 1.3493303571428572,
      "grad_norm": 8.775636075711487,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1209
    },
    {
      "epoch": 1.3504464285714286,
      "grad_norm": 8.808656183646793,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1210
    },
    {
      "epoch": 1.3515625,
      "grad_norm": 8.790718208540653,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1211
    },
    {
      "epoch": 1.3526785714285714,
      "grad_norm": 8.289810898612133,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1212
    },
    {
      "epoch": 1.3537946428571428,
      "grad_norm": 8.829937020826023,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1213
    },
    {
      "epoch": 1.3549107142857144,
      "grad_norm": 9.256337076269721,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1214
    },
    {
      "epoch": 1.3560267857142856,
      "grad_norm": 7.506571999498742,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1215
    },
    {
      "epoch": 1.3571428571428572,
      "grad_norm": 8.619595250551384,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1216
    },
    {
      "epoch": 1.3582589285714286,
      "grad_norm": 9.250413782198942,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1217
    },
    {
      "epoch": 1.359375,
      "grad_norm": 9.50373195543006,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1218
    },
    {
      "epoch": 1.3604910714285714,
      "grad_norm": 9.731338355062372,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1219
    },
    {
      "epoch": 1.3616071428571428,
      "grad_norm": 10.251342469808453,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1220
    },
    {
      "epoch": 1.3627232142857144,
      "grad_norm": 9.304201879122632,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1221
    },
    {
      "epoch": 1.3638392857142856,
      "grad_norm": 7.3023154491070565,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1222
    },
    {
      "epoch": 1.3649553571428572,
      "grad_norm": 7.986516415040767,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1223
    },
    {
      "epoch": 1.3660714285714286,
      "grad_norm": 9.159230807520133,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1224
    },
    {
      "epoch": 1.3671875,
      "grad_norm": 9.798305113949336,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1225
    },
    {
      "epoch": 1.3683035714285714,
      "grad_norm": 8.31630395716757,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1226
    },
    {
      "epoch": 1.3694196428571428,
      "grad_norm": 10.03912732212497,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1227
    },
    {
      "epoch": 1.3705357142857144,
      "grad_norm": 9.400539486534997,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1228
    },
    {
      "epoch": 1.3716517857142856,
      "grad_norm": 11.839738639190529,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 1229
    },
    {
      "epoch": 1.3727678571428572,
      "grad_norm": 7.999537099565585,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1230
    },
    {
      "epoch": 1.3738839285714286,
      "grad_norm": 8.216767983951215,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 1231
    },
    {
      "epoch": 1.375,
      "grad_norm": 11.444326064395453,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1232
    },
    {
      "epoch": 1.3761160714285714,
      "grad_norm": 8.097724406118463,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1233
    },
    {
      "epoch": 1.3772321428571428,
      "grad_norm": 12.600222637947544,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 1234
    },
    {
      "epoch": 1.3783482142857144,
      "grad_norm": 9.968216890134098,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 1235
    },
    {
      "epoch": 1.3794642857142856,
      "grad_norm": 9.847527960927193,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1236
    },
    {
      "epoch": 1.3805803571428572,
      "grad_norm": 9.108000633081765,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1237
    },
    {
      "epoch": 1.3816964285714286,
      "grad_norm": 7.884622817270073,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1238
    },
    {
      "epoch": 1.3828125,
      "grad_norm": 8.601721366863462,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1239
    },
    {
      "epoch": 1.3839285714285714,
      "grad_norm": 8.492218878592322,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1240
    },
    {
      "epoch": 1.3850446428571428,
      "grad_norm": 8.23919244548802,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1241
    },
    {
      "epoch": 1.3861607142857144,
      "grad_norm": 8.19658816488533,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1242
    },
    {
      "epoch": 1.3872767857142856,
      "grad_norm": 9.780506147593325,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1243
    },
    {
      "epoch": 1.3883928571428572,
      "grad_norm": 7.689079101093529,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1244
    },
    {
      "epoch": 1.3895089285714286,
      "grad_norm": 11.405380297675658,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1245
    },
    {
      "epoch": 1.390625,
      "grad_norm": 7.877677450575858,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1246
    },
    {
      "epoch": 1.3917410714285714,
      "grad_norm": 10.142678111313343,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1247
    },
    {
      "epoch": 1.3928571428571428,
      "grad_norm": 8.84911245118999,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1248
    },
    {
      "epoch": 1.3939732142857144,
      "grad_norm": 8.349508257739474,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1249
    },
    {
      "epoch": 1.3950892857142856,
      "grad_norm": 10.953375164944068,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1250
    },
    {
      "epoch": 1.3962053571428572,
      "grad_norm": 8.57949834005786,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1251
    },
    {
      "epoch": 1.3973214285714286,
      "grad_norm": 8.635171186114034,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1252
    },
    {
      "epoch": 1.3984375,
      "grad_norm": 10.207946251753595,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1253
    },
    {
      "epoch": 1.3995535714285714,
      "grad_norm": 8.656249736232539,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1254
    },
    {
      "epoch": 1.4006696428571428,
      "grad_norm": 8.555930372543912,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1255
    },
    {
      "epoch": 1.4017857142857144,
      "grad_norm": 8.443056612790487,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1256
    },
    {
      "epoch": 1.4029017857142856,
      "grad_norm": 7.654865151270271,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1257
    },
    {
      "epoch": 1.4040178571428572,
      "grad_norm": 9.254546395630177,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1258
    },
    {
      "epoch": 1.4051339285714286,
      "grad_norm": 10.150048178692769,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1259
    },
    {
      "epoch": 1.40625,
      "grad_norm": 8.203115966267552,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1260
    },
    {
      "epoch": 1.4073660714285714,
      "grad_norm": 9.08234696514549,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1261
    },
    {
      "epoch": 1.4084821428571428,
      "grad_norm": 10.420010674522723,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 1262
    },
    {
      "epoch": 1.4095982142857144,
      "grad_norm": 9.094539098411033,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1263
    },
    {
      "epoch": 1.4107142857142856,
      "grad_norm": 7.826211792493097,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1264
    },
    {
      "epoch": 1.4118303571428572,
      "grad_norm": 8.711283084903403,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1265
    },
    {
      "epoch": 1.4129464285714286,
      "grad_norm": 8.780770832805022,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1266
    },
    {
      "epoch": 1.4140625,
      "grad_norm": 5.643139211801992,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1267
    },
    {
      "epoch": 1.4151785714285714,
      "grad_norm": 7.1558335492974035,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1268
    },
    {
      "epoch": 1.4162946428571428,
      "grad_norm": 9.682229773748837,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 1269
    },
    {
      "epoch": 1.4174107142857144,
      "grad_norm": 8.06674522474829,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1270
    },
    {
      "epoch": 1.4185267857142856,
      "grad_norm": 8.364376229573383,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 1271
    },
    {
      "epoch": 1.4196428571428572,
      "grad_norm": 11.160626357556595,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1272
    },
    {
      "epoch": 1.4207589285714286,
      "grad_norm": 9.200783905082456,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1273
    },
    {
      "epoch": 1.421875,
      "grad_norm": 9.237998344330064,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1274
    },
    {
      "epoch": 1.4229910714285714,
      "grad_norm": 8.273756030099419,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1275
    },
    {
      "epoch": 1.4241071428571428,
      "grad_norm": 9.403904737488276,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1276
    },
    {
      "epoch": 1.4252232142857144,
      "grad_norm": 10.84949710995534,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1277
    },
    {
      "epoch": 1.4263392857142856,
      "grad_norm": 9.858320103105127,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1278
    },
    {
      "epoch": 1.4274553571428572,
      "grad_norm": 9.002876494422388,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 1279
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 10.36907855545062,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 1280
    },
    {
      "epoch": 1.4296875,
      "grad_norm": 8.220803375027813,
      "learning_rate": 2e-05,
      "loss": 1.4453,
      "step": 1281
    },
    {
      "epoch": 1.4308035714285714,
      "grad_norm": 10.891313878053658,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1282
    },
    {
      "epoch": 1.4319196428571428,
      "grad_norm": 9.59590762846664,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1283
    },
    {
      "epoch": 1.4330357142857144,
      "grad_norm": 11.437379488165508,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1284
    },
    {
      "epoch": 1.4341517857142856,
      "grad_norm": 9.666270198219761,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1285
    },
    {
      "epoch": 1.4352678571428572,
      "grad_norm": 9.827495539701932,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1286
    },
    {
      "epoch": 1.4363839285714286,
      "grad_norm": 9.257557918073589,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1287
    },
    {
      "epoch": 1.4375,
      "grad_norm": 8.294507959748042,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1288
    },
    {
      "epoch": 1.4386160714285714,
      "grad_norm": 9.006891008766962,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1289
    },
    {
      "epoch": 1.4397321428571428,
      "grad_norm": 8.488075850145647,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1290
    },
    {
      "epoch": 1.4408482142857144,
      "grad_norm": 10.835565471337143,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1291
    },
    {
      "epoch": 1.4419642857142856,
      "grad_norm": 10.77221035426753,
      "learning_rate": 2e-05,
      "loss": 1.4688,
      "step": 1292
    },
    {
      "epoch": 1.4430803571428572,
      "grad_norm": 11.151705526983818,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1293
    },
    {
      "epoch": 1.4441964285714286,
      "grad_norm": 9.98939825466068,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1294
    },
    {
      "epoch": 1.4453125,
      "grad_norm": 9.088514441344408,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1295
    },
    {
      "epoch": 1.4464285714285714,
      "grad_norm": 9.431136138281046,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1296
    },
    {
      "epoch": 1.4475446428571428,
      "grad_norm": 8.813406012793843,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 1297
    },
    {
      "epoch": 1.4486607142857144,
      "grad_norm": 7.321410948779888,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1298
    },
    {
      "epoch": 1.4497767857142856,
      "grad_norm": 9.347812457949315,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1299
    },
    {
      "epoch": 1.4508928571428572,
      "grad_norm": 7.051124288726215,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1300
    },
    {
      "epoch": 1.4520089285714286,
      "grad_norm": 6.587555223954526,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 1301
    },
    {
      "epoch": 1.453125,
      "grad_norm": 10.705552328043911,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1302
    },
    {
      "epoch": 1.4542410714285714,
      "grad_norm": 9.38049761572144,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1303
    },
    {
      "epoch": 1.4553571428571428,
      "grad_norm": 8.610715199580302,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1304
    },
    {
      "epoch": 1.4564732142857144,
      "grad_norm": 10.505908591298297,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1305
    },
    {
      "epoch": 1.4575892857142856,
      "grad_norm": 8.075456699354536,
      "learning_rate": 2e-05,
      "loss": 1.4219,
      "step": 1306
    },
    {
      "epoch": 1.4587053571428572,
      "grad_norm": 9.506790352065122,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1307
    },
    {
      "epoch": 1.4598214285714286,
      "grad_norm": 10.179823854286406,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1308
    },
    {
      "epoch": 1.4609375,
      "grad_norm": 10.95907634093396,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1309
    },
    {
      "epoch": 1.4620535714285714,
      "grad_norm": 11.350374338087741,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1310
    },
    {
      "epoch": 1.4631696428571428,
      "grad_norm": 8.98414415203287,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1311
    },
    {
      "epoch": 1.4642857142857144,
      "grad_norm": 12.248351815012791,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1312
    },
    {
      "epoch": 1.4654017857142856,
      "grad_norm": 9.782110975643725,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1313
    },
    {
      "epoch": 1.4665178571428572,
      "grad_norm": 9.637522556761839,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1314
    },
    {
      "epoch": 1.4676339285714286,
      "grad_norm": 10.17849804720928,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1315
    },
    {
      "epoch": 1.46875,
      "grad_norm": 10.220372139736691,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 1316
    },
    {
      "epoch": 1.4698660714285714,
      "grad_norm": 8.77208299382895,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1317
    },
    {
      "epoch": 1.4709821428571428,
      "grad_norm": 8.734981600484762,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 1318
    },
    {
      "epoch": 1.4720982142857144,
      "grad_norm": 10.619466330326123,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1319
    },
    {
      "epoch": 1.4732142857142856,
      "grad_norm": 8.795938947465466,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1320
    },
    {
      "epoch": 1.4743303571428572,
      "grad_norm": 8.561916476159835,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1321
    },
    {
      "epoch": 1.4754464285714286,
      "grad_norm": 9.910884241465471,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1322
    },
    {
      "epoch": 1.4765625,
      "grad_norm": 9.016103274377754,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1323
    },
    {
      "epoch": 1.4776785714285714,
      "grad_norm": 7.850106937457594,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1324
    },
    {
      "epoch": 1.4787946428571428,
      "grad_norm": 9.954364401055683,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1325
    },
    {
      "epoch": 1.4799107142857144,
      "grad_norm": 9.211949335693518,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1326
    },
    {
      "epoch": 1.4810267857142856,
      "grad_norm": 8.461458204640762,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1327
    },
    {
      "epoch": 1.4821428571428572,
      "grad_norm": 9.88457879600015,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1328
    },
    {
      "epoch": 1.4832589285714286,
      "grad_norm": 9.429982098518263,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1329
    },
    {
      "epoch": 1.484375,
      "grad_norm": 10.46942108365679,
      "learning_rate": 2e-05,
      "loss": 2.8594,
      "step": 1330
    },
    {
      "epoch": 1.4854910714285714,
      "grad_norm": 9.307743933558234,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1331
    },
    {
      "epoch": 1.4866071428571428,
      "grad_norm": 9.775190618209708,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 1332
    },
    {
      "epoch": 1.4877232142857144,
      "grad_norm": 7.94461890454244,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 1333
    },
    {
      "epoch": 1.4888392857142856,
      "grad_norm": 8.181808596298257,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 1334
    },
    {
      "epoch": 1.4899553571428572,
      "grad_norm": 8.577576800665486,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1335
    },
    {
      "epoch": 1.4910714285714286,
      "grad_norm": 6.308229841960129,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1336
    },
    {
      "epoch": 1.4921875,
      "grad_norm": 8.967499981950674,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1337
    },
    {
      "epoch": 1.4933035714285714,
      "grad_norm": 11.869718193808279,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1338
    },
    {
      "epoch": 1.4944196428571428,
      "grad_norm": 8.198769291208936,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1339
    },
    {
      "epoch": 1.4955357142857144,
      "grad_norm": 10.679253612630305,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 1340
    },
    {
      "epoch": 1.4966517857142856,
      "grad_norm": 9.473446615431198,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1341
    },
    {
      "epoch": 1.4977678571428572,
      "grad_norm": 10.156580206499383,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1342
    },
    {
      "epoch": 1.4988839285714286,
      "grad_norm": 8.989884554918486,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 1343
    },
    {
      "epoch": 1.5,
      "grad_norm": 9.400159850685863,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1344
    },
    {
      "epoch": 1.5011160714285714,
      "grad_norm": 7.613643791477073,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1345
    },
    {
      "epoch": 1.5022321428571428,
      "grad_norm": 8.316291270754624,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1346
    },
    {
      "epoch": 1.5033482142857144,
      "grad_norm": 9.136223699139029,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1347
    },
    {
      "epoch": 1.5044642857142856,
      "grad_norm": 8.714568576850318,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1348
    },
    {
      "epoch": 1.5055803571428572,
      "grad_norm": 9.315276465784633,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1349
    },
    {
      "epoch": 1.5066964285714286,
      "grad_norm": 6.036560483436557,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1350
    },
    {
      "epoch": 1.5078125,
      "grad_norm": 8.786591322202597,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1351
    },
    {
      "epoch": 1.5089285714285714,
      "grad_norm": 8.923725409661635,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1352
    },
    {
      "epoch": 1.5100446428571428,
      "grad_norm": 8.44215884090243,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1353
    },
    {
      "epoch": 1.5111607142857144,
      "grad_norm": 13.668143226424583,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 1354
    },
    {
      "epoch": 1.5122767857142856,
      "grad_norm": 8.872955853150549,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1355
    },
    {
      "epoch": 1.5133928571428572,
      "grad_norm": 8.51784721577343,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1356
    },
    {
      "epoch": 1.5145089285714286,
      "grad_norm": 10.178868859185512,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1357
    },
    {
      "epoch": 1.515625,
      "grad_norm": 6.830645718879109,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1358
    },
    {
      "epoch": 1.5167410714285714,
      "grad_norm": 9.44156260089097,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1359
    },
    {
      "epoch": 1.5178571428571428,
      "grad_norm": 9.794584733988376,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1360
    },
    {
      "epoch": 1.5189732142857144,
      "grad_norm": 6.649240390053075,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1361
    },
    {
      "epoch": 1.5200892857142856,
      "grad_norm": 5.9710737111926475,
      "learning_rate": 2e-05,
      "loss": 0.9648,
      "step": 1362
    },
    {
      "epoch": 1.5212053571428572,
      "grad_norm": 8.598490124505231,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1363
    },
    {
      "epoch": 1.5223214285714286,
      "grad_norm": 8.352090215515267,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1364
    },
    {
      "epoch": 1.5234375,
      "grad_norm": 9.720000480660136,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1365
    },
    {
      "epoch": 1.5245535714285714,
      "grad_norm": 8.983418450357972,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1366
    },
    {
      "epoch": 1.5256696428571428,
      "grad_norm": 9.69182131037961,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 1367
    },
    {
      "epoch": 1.5267857142857144,
      "grad_norm": 17.4536515251492,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1368
    },
    {
      "epoch": 1.5279017857142856,
      "grad_norm": 10.064386675694546,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 1369
    },
    {
      "epoch": 1.5290178571428572,
      "grad_norm": 7.626036909476946,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1370
    },
    {
      "epoch": 1.5301339285714286,
      "grad_norm": 8.547319871516027,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1371
    },
    {
      "epoch": 1.53125,
      "grad_norm": 10.290068210221953,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1372
    },
    {
      "epoch": 1.5323660714285714,
      "grad_norm": 10.519494395821514,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1373
    },
    {
      "epoch": 1.5334821428571428,
      "grad_norm": 9.723586090309666,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 1374
    },
    {
      "epoch": 1.5345982142857144,
      "grad_norm": 9.272419792891116,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1375
    },
    {
      "epoch": 1.5357142857142856,
      "grad_norm": 8.28458315819196,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 1376
    },
    {
      "epoch": 1.5368303571428572,
      "grad_norm": 9.566451166854504,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1377
    },
    {
      "epoch": 1.5379464285714286,
      "grad_norm": 6.477627828146688,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1378
    },
    {
      "epoch": 1.5390625,
      "grad_norm": 10.047897531427251,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1379
    },
    {
      "epoch": 1.5401785714285714,
      "grad_norm": 8.45973267097152,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1380
    },
    {
      "epoch": 1.5412946428571428,
      "grad_norm": 10.18591831912735,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1381
    },
    {
      "epoch": 1.5424107142857144,
      "grad_norm": 9.615141765825463,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1382
    },
    {
      "epoch": 1.5435267857142856,
      "grad_norm": 8.196316758217664,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 1383
    },
    {
      "epoch": 1.5446428571428572,
      "grad_norm": 9.348686007890919,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 1384
    },
    {
      "epoch": 1.5457589285714286,
      "grad_norm": 10.400151563814301,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1385
    },
    {
      "epoch": 1.546875,
      "grad_norm": 8.614021748596835,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1386
    },
    {
      "epoch": 1.5479910714285714,
      "grad_norm": 9.8648604500996,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 1387
    },
    {
      "epoch": 1.5491071428571428,
      "grad_norm": 8.662676330009363,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1388
    },
    {
      "epoch": 1.5502232142857144,
      "grad_norm": 8.704383371648056,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1389
    },
    {
      "epoch": 1.5513392857142856,
      "grad_norm": 8.867178361649243,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1390
    },
    {
      "epoch": 1.5524553571428572,
      "grad_norm": 8.847060171037617,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1391
    },
    {
      "epoch": 1.5535714285714286,
      "grad_norm": 7.151112385174206,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 1392
    },
    {
      "epoch": 1.5546875,
      "grad_norm": 8.96752773500939,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1393
    },
    {
      "epoch": 1.5558035714285714,
      "grad_norm": 9.485938892971276,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1394
    },
    {
      "epoch": 1.5569196428571428,
      "grad_norm": 9.029006611612331,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1395
    },
    {
      "epoch": 1.5580357142857144,
      "grad_norm": 9.547506900517366,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1396
    },
    {
      "epoch": 1.5591517857142856,
      "grad_norm": 10.300458744365,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1397
    },
    {
      "epoch": 1.5602678571428572,
      "grad_norm": 8.249641092226316,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1398
    },
    {
      "epoch": 1.5613839285714286,
      "grad_norm": 7.61290028950055,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1399
    },
    {
      "epoch": 1.5625,
      "grad_norm": 10.76733539896417,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1400
    },
    {
      "epoch": 1.5636160714285714,
      "grad_norm": 9.09189696387239,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1401
    },
    {
      "epoch": 1.5647321428571428,
      "grad_norm": 9.422831315441632,
      "learning_rate": 2e-05,
      "loss": 1.4375,
      "step": 1402
    },
    {
      "epoch": 1.5658482142857144,
      "grad_norm": 8.723724478977571,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1403
    },
    {
      "epoch": 1.5669642857142856,
      "grad_norm": 7.512133054215442,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1404
    },
    {
      "epoch": 1.5680803571428572,
      "grad_norm": 8.787479864749605,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1405
    },
    {
      "epoch": 1.5691964285714286,
      "grad_norm": 7.665087878903814,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1406
    },
    {
      "epoch": 1.5703125,
      "grad_norm": 10.491540601187404,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 1407
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 7.101735868165235,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1408
    },
    {
      "epoch": 1.5725446428571428,
      "grad_norm": 9.30588501022114,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 1409
    },
    {
      "epoch": 1.5736607142857144,
      "grad_norm": 11.0615284145666,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1410
    },
    {
      "epoch": 1.5747767857142856,
      "grad_norm": 9.397955897353127,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 1411
    },
    {
      "epoch": 1.5758928571428572,
      "grad_norm": 9.121632928060553,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1412
    },
    {
      "epoch": 1.5770089285714286,
      "grad_norm": 12.420809074694578,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1413
    },
    {
      "epoch": 1.578125,
      "grad_norm": 8.831113735422129,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1414
    },
    {
      "epoch": 1.5792410714285714,
      "grad_norm": 9.891142653562204,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 1415
    },
    {
      "epoch": 1.5803571428571428,
      "grad_norm": 9.482972724890704,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 1416
    },
    {
      "epoch": 1.5814732142857144,
      "grad_norm": 8.158664783720093,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1417
    },
    {
      "epoch": 1.5825892857142856,
      "grad_norm": 11.369955019646758,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1418
    },
    {
      "epoch": 1.5837053571428572,
      "grad_norm": 9.60203483873775,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1419
    },
    {
      "epoch": 1.5848214285714286,
      "grad_norm": 11.081047783881354,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1420
    },
    {
      "epoch": 1.5859375,
      "grad_norm": 10.584082052344462,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 1421
    },
    {
      "epoch": 1.5870535714285714,
      "grad_norm": 9.083302460358395,
      "learning_rate": 2e-05,
      "loss": 2.8438,
      "step": 1422
    },
    {
      "epoch": 1.5881696428571428,
      "grad_norm": 9.299721662728967,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1423
    },
    {
      "epoch": 1.5892857142857144,
      "grad_norm": 10.434119617236153,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1424
    },
    {
      "epoch": 1.5904017857142856,
      "grad_norm": 8.279427493733941,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1425
    },
    {
      "epoch": 1.5915178571428572,
      "grad_norm": 9.010498417662703,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1426
    },
    {
      "epoch": 1.5926339285714286,
      "grad_norm": 9.716984132111378,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1427
    },
    {
      "epoch": 1.59375,
      "grad_norm": 8.487157790408737,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1428
    },
    {
      "epoch": 1.5948660714285714,
      "grad_norm": 8.408484866806726,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1429
    },
    {
      "epoch": 1.5959821428571428,
      "grad_norm": 11.830329826981554,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1430
    },
    {
      "epoch": 1.5970982142857144,
      "grad_norm": 9.349285227785224,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1431
    },
    {
      "epoch": 1.5982142857142856,
      "grad_norm": 9.438477080280949,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1432
    },
    {
      "epoch": 1.5993303571428572,
      "grad_norm": 8.558669662965295,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1433
    },
    {
      "epoch": 1.6004464285714286,
      "grad_norm": 9.27347033122972,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1434
    },
    {
      "epoch": 1.6015625,
      "grad_norm": 9.034405340885417,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1435
    },
    {
      "epoch": 1.6026785714285714,
      "grad_norm": 10.417668835864772,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1436
    },
    {
      "epoch": 1.6037946428571428,
      "grad_norm": 10.954778964048971,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1437
    },
    {
      "epoch": 1.6049107142857144,
      "grad_norm": 8.262059728526577,
      "learning_rate": 2e-05,
      "loss": 2.8594,
      "step": 1438
    },
    {
      "epoch": 1.6060267857142856,
      "grad_norm": 8.152251275325046,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1439
    },
    {
      "epoch": 1.6071428571428572,
      "grad_norm": 10.201402789725414,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 1440
    },
    {
      "epoch": 1.6082589285714286,
      "grad_norm": 10.674213719399992,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1441
    },
    {
      "epoch": 1.609375,
      "grad_norm": 10.406447891545909,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1442
    },
    {
      "epoch": 1.6104910714285714,
      "grad_norm": 9.839225536353041,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 1443
    },
    {
      "epoch": 1.6116071428571428,
      "grad_norm": 9.146105676663144,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 1444
    },
    {
      "epoch": 1.6127232142857144,
      "grad_norm": 7.399758047591472,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1445
    },
    {
      "epoch": 1.6138392857142856,
      "grad_norm": 8.39525697340261,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1446
    },
    {
      "epoch": 1.6149553571428572,
      "grad_norm": 9.335204404936587,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1447
    },
    {
      "epoch": 1.6160714285714286,
      "grad_norm": 8.742659366869468,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1448
    },
    {
      "epoch": 1.6171875,
      "grad_norm": 8.687213062894054,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1449
    },
    {
      "epoch": 1.6183035714285714,
      "grad_norm": 10.520121734282036,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1450
    },
    {
      "epoch": 1.6194196428571428,
      "grad_norm": 10.022705680397964,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1451
    },
    {
      "epoch": 1.6205357142857144,
      "grad_norm": 12.738272070105687,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1452
    },
    {
      "epoch": 1.6216517857142856,
      "grad_norm": 8.025051960351693,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 1453
    },
    {
      "epoch": 1.6227678571428572,
      "grad_norm": 7.232327378541648,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1454
    },
    {
      "epoch": 1.6238839285714286,
      "grad_norm": 9.001494871591028,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1455
    },
    {
      "epoch": 1.625,
      "grad_norm": 7.873937614254038,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1456
    },
    {
      "epoch": 1.6261160714285714,
      "grad_norm": 10.804973658571797,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 1457
    },
    {
      "epoch": 1.6272321428571428,
      "grad_norm": 9.296202554886131,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1458
    },
    {
      "epoch": 1.6283482142857144,
      "grad_norm": 9.172746619827555,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1459
    },
    {
      "epoch": 1.6294642857142856,
      "grad_norm": 10.827993653419353,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1460
    },
    {
      "epoch": 1.6305803571428572,
      "grad_norm": 10.18924581128832,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 1461
    },
    {
      "epoch": 1.6316964285714286,
      "grad_norm": 9.86582123332929,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1462
    },
    {
      "epoch": 1.6328125,
      "grad_norm": 7.7278268731600575,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1463
    },
    {
      "epoch": 1.6339285714285714,
      "grad_norm": 10.540990019458526,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1464
    },
    {
      "epoch": 1.6350446428571428,
      "grad_norm": 9.88449852242838,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1465
    },
    {
      "epoch": 1.6361607142857144,
      "grad_norm": 10.84407272738642,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 1466
    },
    {
      "epoch": 1.6372767857142856,
      "grad_norm": 9.583963783484819,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 1467
    },
    {
      "epoch": 1.6383928571428572,
      "grad_norm": 10.195317508265969,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1468
    },
    {
      "epoch": 1.6395089285714286,
      "grad_norm": 10.393428961421769,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1469
    },
    {
      "epoch": 1.640625,
      "grad_norm": 9.257461479246551,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1470
    },
    {
      "epoch": 1.6417410714285714,
      "grad_norm": 8.81021040365599,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1471
    },
    {
      "epoch": 1.6428571428571428,
      "grad_norm": 9.50218978026321,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1472
    },
    {
      "epoch": 1.6439732142857144,
      "grad_norm": 11.75096703828102,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1473
    },
    {
      "epoch": 1.6450892857142856,
      "grad_norm": 9.411958872615486,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1474
    },
    {
      "epoch": 1.6462053571428572,
      "grad_norm": 10.760741682943815,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1475
    },
    {
      "epoch": 1.6473214285714286,
      "grad_norm": 10.20473688165343,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1476
    },
    {
      "epoch": 1.6484375,
      "grad_norm": 9.976419189783183,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1477
    },
    {
      "epoch": 1.6495535714285714,
      "grad_norm": 8.149696286413501,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1478
    },
    {
      "epoch": 1.6506696428571428,
      "grad_norm": 10.505897991229702,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1479
    },
    {
      "epoch": 1.6517857142857144,
      "grad_norm": 6.9229638268818166,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 1480
    },
    {
      "epoch": 1.6529017857142856,
      "grad_norm": 9.329856541391585,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1481
    },
    {
      "epoch": 1.6540178571428572,
      "grad_norm": 9.163345135553094,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 1482
    },
    {
      "epoch": 1.6551339285714286,
      "grad_norm": 9.66013585130318,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1483
    },
    {
      "epoch": 1.65625,
      "grad_norm": 10.08574560912204,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1484
    },
    {
      "epoch": 1.6573660714285714,
      "grad_norm": 10.468492628865842,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1485
    },
    {
      "epoch": 1.6584821428571428,
      "grad_norm": 10.674015434915315,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 1486
    },
    {
      "epoch": 1.6595982142857144,
      "grad_norm": 8.58887065082251,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1487
    },
    {
      "epoch": 1.6607142857142856,
      "grad_norm": 8.73652306720952,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1488
    },
    {
      "epoch": 1.6618303571428572,
      "grad_norm": 8.831044986374536,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1489
    },
    {
      "epoch": 1.6629464285714286,
      "grad_norm": 10.044332822231855,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1490
    },
    {
      "epoch": 1.6640625,
      "grad_norm": 8.51243712444697,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 1491
    },
    {
      "epoch": 1.6651785714285714,
      "grad_norm": 10.107345226301298,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 1492
    },
    {
      "epoch": 1.6662946428571428,
      "grad_norm": 8.337971001323886,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1493
    },
    {
      "epoch": 1.6674107142857144,
      "grad_norm": 11.303626951832044,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 1494
    },
    {
      "epoch": 1.6685267857142856,
      "grad_norm": 9.315530569125,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1495
    },
    {
      "epoch": 1.6696428571428572,
      "grad_norm": 10.16649067990225,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1496
    },
    {
      "epoch": 1.6707589285714286,
      "grad_norm": 9.897336062227508,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 1497
    },
    {
      "epoch": 1.671875,
      "grad_norm": 10.455875599436014,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1498
    },
    {
      "epoch": 1.6729910714285714,
      "grad_norm": 8.284090277682271,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1499
    },
    {
      "epoch": 1.6741071428571428,
      "grad_norm": 10.152193948154004,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1500
    },
    {
      "epoch": 1.6752232142857144,
      "grad_norm": 10.621433481989495,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1501
    },
    {
      "epoch": 1.6763392857142856,
      "grad_norm": 8.085968044919502,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 1502
    },
    {
      "epoch": 1.6774553571428572,
      "grad_norm": 9.464612862215388,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1503
    },
    {
      "epoch": 1.6785714285714286,
      "grad_norm": 9.587155534553935,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1504
    },
    {
      "epoch": 1.6796875,
      "grad_norm": 9.55863892106362,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1505
    },
    {
      "epoch": 1.6808035714285714,
      "grad_norm": 9.814868069696766,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1506
    },
    {
      "epoch": 1.6819196428571428,
      "grad_norm": 6.8593764240852995,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1507
    },
    {
      "epoch": 1.6830357142857144,
      "grad_norm": 11.711669492900151,
      "learning_rate": 2e-05,
      "loss": 1.3047,
      "step": 1508
    },
    {
      "epoch": 1.6841517857142856,
      "grad_norm": 10.300507137427546,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1509
    },
    {
      "epoch": 1.6852678571428572,
      "grad_norm": 8.670511781808676,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 1510
    },
    {
      "epoch": 1.6863839285714286,
      "grad_norm": 9.657758409710015,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1511
    },
    {
      "epoch": 1.6875,
      "grad_norm": 7.00623982356805,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 1512
    },
    {
      "epoch": 1.6886160714285714,
      "grad_norm": 9.639097240398668,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1513
    },
    {
      "epoch": 1.6897321428571428,
      "grad_norm": 8.059257663936911,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1514
    },
    {
      "epoch": 1.6908482142857144,
      "grad_norm": 8.53953069742539,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1515
    },
    {
      "epoch": 1.6919642857142856,
      "grad_norm": 10.870223205714398,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1516
    },
    {
      "epoch": 1.6930803571428572,
      "grad_norm": 10.094956024373237,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1517
    },
    {
      "epoch": 1.6941964285714286,
      "grad_norm": 9.156690745762047,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 1518
    },
    {
      "epoch": 1.6953125,
      "grad_norm": 11.76138804696912,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 1519
    },
    {
      "epoch": 1.6964285714285714,
      "grad_norm": 10.659342655778657,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1520
    },
    {
      "epoch": 1.6975446428571428,
      "grad_norm": 7.573792116913332,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1521
    },
    {
      "epoch": 1.6986607142857144,
      "grad_norm": 11.295936274022013,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1522
    },
    {
      "epoch": 1.6997767857142856,
      "grad_norm": 9.802649937758591,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1523
    },
    {
      "epoch": 1.7008928571428572,
      "grad_norm": 11.71234458031626,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1524
    },
    {
      "epoch": 1.7020089285714286,
      "grad_norm": 10.090231904086346,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 1525
    },
    {
      "epoch": 1.703125,
      "grad_norm": 9.999926385254822,
      "learning_rate": 2e-05,
      "loss": 1.4688,
      "step": 1526
    },
    {
      "epoch": 1.7042410714285714,
      "grad_norm": 11.425452430809928,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1527
    },
    {
      "epoch": 1.7053571428571428,
      "grad_norm": 8.634454485530753,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1528
    },
    {
      "epoch": 1.7064732142857144,
      "grad_norm": 11.992402490053337,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1529
    },
    {
      "epoch": 1.7075892857142856,
      "grad_norm": 7.28372679412276,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 1530
    },
    {
      "epoch": 1.7087053571428572,
      "grad_norm": 10.162881705652747,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1531
    },
    {
      "epoch": 1.7098214285714286,
      "grad_norm": 10.0830685330925,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 1532
    },
    {
      "epoch": 1.7109375,
      "grad_norm": 7.506713039975602,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1533
    },
    {
      "epoch": 1.7120535714285714,
      "grad_norm": 10.312520459610095,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1534
    },
    {
      "epoch": 1.7131696428571428,
      "grad_norm": 9.337195485980004,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1535
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 11.523490158078772,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1536
    },
    {
      "epoch": 1.7154017857142856,
      "grad_norm": 10.658520244675378,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1537
    },
    {
      "epoch": 1.7165178571428572,
      "grad_norm": 9.524333337365045,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1538
    },
    {
      "epoch": 1.7176339285714286,
      "grad_norm": 11.138438811925717,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1539
    },
    {
      "epoch": 1.71875,
      "grad_norm": 7.903162416322438,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1540
    },
    {
      "epoch": 1.7198660714285714,
      "grad_norm": 9.370548263502164,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1541
    },
    {
      "epoch": 1.7209821428571428,
      "grad_norm": 7.813952242089537,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1542
    },
    {
      "epoch": 1.7220982142857144,
      "grad_norm": 11.50809232986242,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 1543
    },
    {
      "epoch": 1.7232142857142856,
      "grad_norm": 11.028865569688707,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1544
    },
    {
      "epoch": 1.7243303571428572,
      "grad_norm": 10.762200184960374,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1545
    },
    {
      "epoch": 1.7254464285714286,
      "grad_norm": 10.080920989546247,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1546
    },
    {
      "epoch": 1.7265625,
      "grad_norm": 10.136918817808874,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1547
    },
    {
      "epoch": 1.7276785714285714,
      "grad_norm": 8.711546305747957,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1548
    },
    {
      "epoch": 1.7287946428571428,
      "grad_norm": 11.217259501513423,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1549
    },
    {
      "epoch": 1.7299107142857144,
      "grad_norm": 7.8866690967634785,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1550
    },
    {
      "epoch": 1.7310267857142856,
      "grad_norm": 8.799561212378615,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1551
    },
    {
      "epoch": 1.7321428571428572,
      "grad_norm": 10.767253536996297,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1552
    },
    {
      "epoch": 1.7332589285714286,
      "grad_norm": 9.659188770261075,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1553
    },
    {
      "epoch": 1.734375,
      "grad_norm": 8.757451585655382,
      "learning_rate": 2e-05,
      "loss": 2.9844,
      "step": 1554
    },
    {
      "epoch": 1.7354910714285714,
      "grad_norm": 9.895290637928069,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 1555
    },
    {
      "epoch": 1.7366071428571428,
      "grad_norm": 10.317908956550177,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 1556
    },
    {
      "epoch": 1.7377232142857144,
      "grad_norm": 8.892185696846486,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1557
    },
    {
      "epoch": 1.7388392857142856,
      "grad_norm": 9.211938507234029,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1558
    },
    {
      "epoch": 1.7399553571428572,
      "grad_norm": 9.137725741947497,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1559
    },
    {
      "epoch": 1.7410714285714286,
      "grad_norm": 9.113203899692163,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1560
    },
    {
      "epoch": 1.7421875,
      "grad_norm": 9.963271403425138,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 1561
    },
    {
      "epoch": 1.7433035714285714,
      "grad_norm": 10.077116382282732,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1562
    },
    {
      "epoch": 1.7444196428571428,
      "grad_norm": 10.340914282603212,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 1563
    },
    {
      "epoch": 1.7455357142857144,
      "grad_norm": 9.838463356756959,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1564
    },
    {
      "epoch": 1.7466517857142856,
      "grad_norm": 9.587612705431498,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1565
    },
    {
      "epoch": 1.7477678571428572,
      "grad_norm": 10.430997376531623,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1566
    },
    {
      "epoch": 1.7488839285714286,
      "grad_norm": 10.991644091424158,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 1567
    },
    {
      "epoch": 1.75,
      "grad_norm": 10.304030102912575,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1568
    },
    {
      "epoch": 1.7511160714285714,
      "grad_norm": 8.488900692014889,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1569
    },
    {
      "epoch": 1.7522321428571428,
      "grad_norm": 10.09457223386765,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1570
    },
    {
      "epoch": 1.7533482142857144,
      "grad_norm": 10.86070895304293,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1571
    },
    {
      "epoch": 1.7544642857142856,
      "grad_norm": 9.742796692856764,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1572
    },
    {
      "epoch": 1.7555803571428572,
      "grad_norm": 8.989504521144076,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1573
    },
    {
      "epoch": 1.7566964285714286,
      "grad_norm": 8.88135505758998,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1574
    },
    {
      "epoch": 1.7578125,
      "grad_norm": 9.429579710957274,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1575
    },
    {
      "epoch": 1.7589285714285714,
      "grad_norm": 10.549343782424321,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1576
    },
    {
      "epoch": 1.7600446428571428,
      "grad_norm": 9.590110306827963,
      "learning_rate": 2e-05,
      "loss": 2.9062,
      "step": 1577
    },
    {
      "epoch": 1.7611607142857144,
      "grad_norm": 10.307408006120234,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1578
    },
    {
      "epoch": 1.7622767857142856,
      "grad_norm": 9.382544586659419,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1579
    },
    {
      "epoch": 1.7633928571428572,
      "grad_norm": 8.31246743621136,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 1580
    },
    {
      "epoch": 1.7645089285714286,
      "grad_norm": 7.586334378462209,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 1581
    },
    {
      "epoch": 1.765625,
      "grad_norm": 9.975486409583873,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1582
    },
    {
      "epoch": 1.7667410714285714,
      "grad_norm": 9.928501767543032,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 1583
    },
    {
      "epoch": 1.7678571428571428,
      "grad_norm": 10.172553199484938,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1584
    },
    {
      "epoch": 1.7689732142857144,
      "grad_norm": 10.572718002898002,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1585
    },
    {
      "epoch": 1.7700892857142856,
      "grad_norm": 7.811589141223647,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1586
    },
    {
      "epoch": 1.7712053571428572,
      "grad_norm": 9.76108995198616,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1587
    },
    {
      "epoch": 1.7723214285714286,
      "grad_norm": 9.966853323064576,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 1588
    },
    {
      "epoch": 1.7734375,
      "grad_norm": 10.833503303564294,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1589
    },
    {
      "epoch": 1.7745535714285714,
      "grad_norm": 11.901549944038779,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1590
    },
    {
      "epoch": 1.7756696428571428,
      "grad_norm": 12.20660603368522,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1591
    },
    {
      "epoch": 1.7767857142857144,
      "grad_norm": 11.339669107650025,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1592
    },
    {
      "epoch": 1.7779017857142856,
      "grad_norm": 9.802099355616575,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1593
    },
    {
      "epoch": 1.7790178571428572,
      "grad_norm": 9.44264324914429,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 1594
    },
    {
      "epoch": 1.7801339285714286,
      "grad_norm": 10.031245697306366,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1595
    },
    {
      "epoch": 1.78125,
      "grad_norm": 9.25010545643392,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 1596
    },
    {
      "epoch": 1.7823660714285714,
      "grad_norm": 8.077338729620717,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 1597
    },
    {
      "epoch": 1.7834821428571428,
      "grad_norm": 10.833625823613541,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1598
    },
    {
      "epoch": 1.7845982142857144,
      "grad_norm": 9.80398216438199,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1599
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 9.405221969865373,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 1600
    },
    {
      "epoch": 1.7868303571428572,
      "grad_norm": 9.859562311183602,
      "learning_rate": 2e-05,
      "loss": 1.4453,
      "step": 1601
    },
    {
      "epoch": 1.7879464285714286,
      "grad_norm": 9.86173919198722,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1602
    },
    {
      "epoch": 1.7890625,
      "grad_norm": 8.926609556476459,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1603
    },
    {
      "epoch": 1.7901785714285714,
      "grad_norm": 10.260868742385913,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 1604
    },
    {
      "epoch": 1.7912946428571428,
      "grad_norm": 10.632305267948183,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 1605
    },
    {
      "epoch": 1.7924107142857144,
      "grad_norm": 9.97627931091052,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 1606
    },
    {
      "epoch": 1.7935267857142856,
      "grad_norm": 10.755730928141354,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1607
    },
    {
      "epoch": 1.7946428571428572,
      "grad_norm": 8.78854722961166,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1608
    },
    {
      "epoch": 1.7957589285714286,
      "grad_norm": 10.535091925806332,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1609
    },
    {
      "epoch": 1.796875,
      "grad_norm": 7.885350011758516,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1610
    },
    {
      "epoch": 1.7979910714285714,
      "grad_norm": 10.733561986650319,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 1611
    },
    {
      "epoch": 1.7991071428571428,
      "grad_norm": 8.602645399337955,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1612
    },
    {
      "epoch": 1.8002232142857144,
      "grad_norm": 9.549953121297268,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1613
    },
    {
      "epoch": 1.8013392857142856,
      "grad_norm": 7.157198633711598,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1614
    },
    {
      "epoch": 1.8024553571428572,
      "grad_norm": 9.67929283449173,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 1615
    },
    {
      "epoch": 1.8035714285714286,
      "grad_norm": 8.413273812862302,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1616
    },
    {
      "epoch": 1.8046875,
      "grad_norm": 9.486725041923808,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1617
    },
    {
      "epoch": 1.8058035714285714,
      "grad_norm": 9.144409799672129,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1618
    },
    {
      "epoch": 1.8069196428571428,
      "grad_norm": 8.459607603892005,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1619
    },
    {
      "epoch": 1.8080357142857144,
      "grad_norm": 9.376265414525813,
      "learning_rate": 2e-05,
      "loss": 3.0,
      "step": 1620
    },
    {
      "epoch": 1.8091517857142856,
      "grad_norm": 11.03019296249102,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 1621
    },
    {
      "epoch": 1.8102678571428572,
      "grad_norm": 11.076514011170323,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1622
    },
    {
      "epoch": 1.8113839285714286,
      "grad_norm": 8.601728474691932,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1623
    },
    {
      "epoch": 1.8125,
      "grad_norm": 11.391650974670567,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1624
    },
    {
      "epoch": 1.8136160714285714,
      "grad_norm": 8.432808375740258,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1625
    },
    {
      "epoch": 1.8147321428571428,
      "grad_norm": 7.545789283199662,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1626
    },
    {
      "epoch": 1.8158482142857144,
      "grad_norm": 9.215908668259866,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1627
    },
    {
      "epoch": 1.8169642857142856,
      "grad_norm": 12.782585447901802,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 1628
    },
    {
      "epoch": 1.8180803571428572,
      "grad_norm": 7.983057343582754,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 1629
    },
    {
      "epoch": 1.8191964285714286,
      "grad_norm": 9.675660780172572,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1630
    },
    {
      "epoch": 1.8203125,
      "grad_norm": 10.530148374025568,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1631
    },
    {
      "epoch": 1.8214285714285714,
      "grad_norm": 8.892547878894627,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1632
    },
    {
      "epoch": 1.8225446428571428,
      "grad_norm": 12.90111891377626,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1633
    },
    {
      "epoch": 1.8236607142857144,
      "grad_norm": 9.039755729415202,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1634
    },
    {
      "epoch": 1.8247767857142856,
      "grad_norm": 10.829027949586697,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1635
    },
    {
      "epoch": 1.8258928571428572,
      "grad_norm": 9.336189165029324,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1636
    },
    {
      "epoch": 1.8270089285714286,
      "grad_norm": 10.02137680706756,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1637
    },
    {
      "epoch": 1.828125,
      "grad_norm": 11.267114921201639,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1638
    },
    {
      "epoch": 1.8292410714285714,
      "grad_norm": 8.515333252448265,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1639
    },
    {
      "epoch": 1.8303571428571428,
      "grad_norm": 8.778107881703919,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1640
    },
    {
      "epoch": 1.8314732142857144,
      "grad_norm": 9.839984574793846,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 1641
    },
    {
      "epoch": 1.8325892857142856,
      "grad_norm": 9.820930772482251,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1642
    },
    {
      "epoch": 1.8337053571428572,
      "grad_norm": 8.283303478377722,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1643
    },
    {
      "epoch": 1.8348214285714286,
      "grad_norm": 7.688819819730179,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1644
    },
    {
      "epoch": 1.8359375,
      "grad_norm": 8.321704039134598,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1645
    },
    {
      "epoch": 1.8370535714285714,
      "grad_norm": 6.792893584150218,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1646
    },
    {
      "epoch": 1.8381696428571428,
      "grad_norm": 8.876865271557234,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1647
    },
    {
      "epoch": 1.8392857142857144,
      "grad_norm": 8.54974677806238,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1648
    },
    {
      "epoch": 1.8404017857142856,
      "grad_norm": 10.295483817485769,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1649
    },
    {
      "epoch": 1.8415178571428572,
      "grad_norm": 7.2595692003141785,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1650
    },
    {
      "epoch": 1.8426339285714286,
      "grad_norm": 8.735477876043511,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1651
    },
    {
      "epoch": 1.84375,
      "grad_norm": 10.134718894848323,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1652
    },
    {
      "epoch": 1.8448660714285714,
      "grad_norm": 10.293313578994193,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 1653
    },
    {
      "epoch": 1.8459821428571428,
      "grad_norm": 12.840859422526622,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1654
    },
    {
      "epoch": 1.8470982142857144,
      "grad_norm": 8.87176459709003,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1655
    },
    {
      "epoch": 1.8482142857142856,
      "grad_norm": 11.400481393753363,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1656
    },
    {
      "epoch": 1.8493303571428572,
      "grad_norm": 10.595405763084624,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1657
    },
    {
      "epoch": 1.8504464285714286,
      "grad_norm": 10.218443060028893,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 1658
    },
    {
      "epoch": 1.8515625,
      "grad_norm": 8.66237820427389,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1659
    },
    {
      "epoch": 1.8526785714285714,
      "grad_norm": 9.904146860070998,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 1660
    },
    {
      "epoch": 1.8537946428571428,
      "grad_norm": 10.68181626323851,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1661
    },
    {
      "epoch": 1.8549107142857144,
      "grad_norm": 9.97667739959965,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1662
    },
    {
      "epoch": 1.8560267857142856,
      "grad_norm": 11.658074399588212,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1663
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 9.749817515574192,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 1664
    },
    {
      "epoch": 1.8582589285714286,
      "grad_norm": 10.775523914945083,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1665
    },
    {
      "epoch": 1.859375,
      "grad_norm": 10.378550561696924,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1666
    },
    {
      "epoch": 1.8604910714285714,
      "grad_norm": 9.840983969072857,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1667
    },
    {
      "epoch": 1.8616071428571428,
      "grad_norm": 10.970001274734262,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1668
    },
    {
      "epoch": 1.8627232142857144,
      "grad_norm": 10.618673989889645,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1669
    },
    {
      "epoch": 1.8638392857142856,
      "grad_norm": 11.252967431615822,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1670
    },
    {
      "epoch": 1.8649553571428572,
      "grad_norm": 10.371918863416644,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1671
    },
    {
      "epoch": 1.8660714285714286,
      "grad_norm": 7.8193408474023665,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1672
    },
    {
      "epoch": 1.8671875,
      "grad_norm": 9.411241055129647,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1673
    },
    {
      "epoch": 1.8683035714285714,
      "grad_norm": 9.414966131186825,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1674
    },
    {
      "epoch": 1.8694196428571428,
      "grad_norm": 8.32469282584879,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 1675
    },
    {
      "epoch": 1.8705357142857144,
      "grad_norm": 9.769738852041113,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1676
    },
    {
      "epoch": 1.8716517857142856,
      "grad_norm": 10.209432651712854,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1677
    },
    {
      "epoch": 1.8727678571428572,
      "grad_norm": 9.088950228865613,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1678
    },
    {
      "epoch": 1.8738839285714286,
      "grad_norm": 9.58093991763409,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1679
    },
    {
      "epoch": 1.875,
      "grad_norm": 9.522280796463065,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1680
    },
    {
      "epoch": 1.8761160714285714,
      "grad_norm": 9.27885138737079,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1681
    },
    {
      "epoch": 1.8772321428571428,
      "grad_norm": 12.75419949017588,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 1682
    },
    {
      "epoch": 1.8783482142857144,
      "grad_norm": 10.473041109338052,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1683
    },
    {
      "epoch": 1.8794642857142856,
      "grad_norm": 10.582227072602768,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1684
    },
    {
      "epoch": 1.8805803571428572,
      "grad_norm": 10.698220684490545,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 1685
    },
    {
      "epoch": 1.8816964285714286,
      "grad_norm": 10.227876378298372,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1686
    },
    {
      "epoch": 1.8828125,
      "grad_norm": 9.610139261788499,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1687
    },
    {
      "epoch": 1.8839285714285714,
      "grad_norm": 9.23868300988373,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 1688
    },
    {
      "epoch": 1.8850446428571428,
      "grad_norm": 12.867527041740663,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1689
    },
    {
      "epoch": 1.8861607142857144,
      "grad_norm": 8.425284703708526,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1690
    },
    {
      "epoch": 1.8872767857142856,
      "grad_norm": 9.406551722117985,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1691
    },
    {
      "epoch": 1.8883928571428572,
      "grad_norm": 12.734712279568136,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1692
    },
    {
      "epoch": 1.8895089285714286,
      "grad_norm": 8.61863211435957,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 1693
    },
    {
      "epoch": 1.890625,
      "grad_norm": 10.753697797632718,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 1694
    },
    {
      "epoch": 1.8917410714285714,
      "grad_norm": 7.94847463500134,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1695
    },
    {
      "epoch": 1.8928571428571428,
      "grad_norm": 8.277609361437907,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1696
    },
    {
      "epoch": 1.8939732142857144,
      "grad_norm": 10.98744064222938,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1697
    },
    {
      "epoch": 1.8950892857142856,
      "grad_norm": 9.358059402577629,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1698
    },
    {
      "epoch": 1.8962053571428572,
      "grad_norm": 11.588995942234728,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 1699
    },
    {
      "epoch": 1.8973214285714286,
      "grad_norm": 9.827513328425779,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 1700
    },
    {
      "epoch": 1.8984375,
      "grad_norm": 9.573707389165287,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1701
    },
    {
      "epoch": 1.8995535714285714,
      "grad_norm": 11.078636059926156,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1702
    },
    {
      "epoch": 1.9006696428571428,
      "grad_norm": 7.2402299665792995,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1703
    },
    {
      "epoch": 1.9017857142857144,
      "grad_norm": 7.739196831229188,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1704
    },
    {
      "epoch": 1.9029017857142856,
      "grad_norm": 7.943202105006223,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1705
    },
    {
      "epoch": 1.9040178571428572,
      "grad_norm": 11.612016318625576,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1706
    },
    {
      "epoch": 1.9051339285714286,
      "grad_norm": 9.400878010335711,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1707
    },
    {
      "epoch": 1.90625,
      "grad_norm": 8.480633339746271,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1708
    },
    {
      "epoch": 1.9073660714285714,
      "grad_norm": 7.429167107640977,
      "learning_rate": 2e-05,
      "loss": 3.0469,
      "step": 1709
    },
    {
      "epoch": 1.9084821428571428,
      "grad_norm": 10.216740441497215,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1710
    },
    {
      "epoch": 1.9095982142857144,
      "grad_norm": 9.040280626506672,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1711
    },
    {
      "epoch": 1.9107142857142856,
      "grad_norm": 8.918653834004807,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1712
    },
    {
      "epoch": 1.9118303571428572,
      "grad_norm": 9.707229224109765,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1713
    },
    {
      "epoch": 1.9129464285714286,
      "grad_norm": 7.274243803964139,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1714
    },
    {
      "epoch": 1.9140625,
      "grad_norm": 8.844013867995526,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1715
    },
    {
      "epoch": 1.9151785714285714,
      "grad_norm": 8.463725858352895,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1716
    },
    {
      "epoch": 1.9162946428571428,
      "grad_norm": 10.072204305316681,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1717
    },
    {
      "epoch": 1.9174107142857144,
      "grad_norm": 10.031945033500966,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1718
    },
    {
      "epoch": 1.9185267857142856,
      "grad_norm": 9.233492292993372,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1719
    },
    {
      "epoch": 1.9196428571428572,
      "grad_norm": 7.788690863908381,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 1720
    },
    {
      "epoch": 1.9207589285714286,
      "grad_norm": 9.0063435458262,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1721
    },
    {
      "epoch": 1.921875,
      "grad_norm": 9.781567635874245,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1722
    },
    {
      "epoch": 1.9229910714285714,
      "grad_norm": 9.285743413302525,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1723
    },
    {
      "epoch": 1.9241071428571428,
      "grad_norm": 8.343611700777373,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1724
    },
    {
      "epoch": 1.9252232142857144,
      "grad_norm": 7.782840141031084,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1725
    },
    {
      "epoch": 1.9263392857142856,
      "grad_norm": 9.68028874763218,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 1726
    },
    {
      "epoch": 1.9274553571428572,
      "grad_norm": 11.622674883006043,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1727
    },
    {
      "epoch": 1.9285714285714286,
      "grad_norm": 10.071798265496833,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1728
    },
    {
      "epoch": 1.9296875,
      "grad_norm": 8.499510282748028,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1729
    },
    {
      "epoch": 1.9308035714285714,
      "grad_norm": 9.100388623446856,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1730
    },
    {
      "epoch": 1.9319196428571428,
      "grad_norm": 9.140741682024682,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1731
    },
    {
      "epoch": 1.9330357142857144,
      "grad_norm": 10.256971612583882,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1732
    },
    {
      "epoch": 1.9341517857142856,
      "grad_norm": 9.03840988112905,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1733
    },
    {
      "epoch": 1.9352678571428572,
      "grad_norm": 8.413764807816188,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1734
    },
    {
      "epoch": 1.9363839285714286,
      "grad_norm": 8.603688494282132,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1735
    },
    {
      "epoch": 1.9375,
      "grad_norm": 10.755013558586404,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 1736
    },
    {
      "epoch": 1.9386160714285714,
      "grad_norm": 10.53478208083376,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1737
    },
    {
      "epoch": 1.9397321428571428,
      "grad_norm": 9.618269394900478,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 1738
    },
    {
      "epoch": 1.9408482142857144,
      "grad_norm": 6.517698456710877,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1739
    },
    {
      "epoch": 1.9419642857142856,
      "grad_norm": 11.055972111553466,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1740
    },
    {
      "epoch": 1.9430803571428572,
      "grad_norm": 9.570950200228353,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1741
    },
    {
      "epoch": 1.9441964285714286,
      "grad_norm": 11.57738266523505,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1742
    },
    {
      "epoch": 1.9453125,
      "grad_norm": 11.82463245472156,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 1743
    },
    {
      "epoch": 1.9464285714285714,
      "grad_norm": 9.362474870567445,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1744
    },
    {
      "epoch": 1.9475446428571428,
      "grad_norm": 10.405714959488638,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1745
    },
    {
      "epoch": 1.9486607142857144,
      "grad_norm": 11.050380865691077,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1746
    },
    {
      "epoch": 1.9497767857142856,
      "grad_norm": 9.631923974468885,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1747
    },
    {
      "epoch": 1.9508928571428572,
      "grad_norm": 10.52115156232622,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1748
    },
    {
      "epoch": 1.9520089285714286,
      "grad_norm": 10.094428136831787,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1749
    },
    {
      "epoch": 1.953125,
      "grad_norm": 8.929552831293108,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1750
    },
    {
      "epoch": 1.9542410714285714,
      "grad_norm": 9.449950700103633,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1751
    },
    {
      "epoch": 1.9553571428571428,
      "grad_norm": 12.712116003173792,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1752
    },
    {
      "epoch": 1.9564732142857144,
      "grad_norm": 9.558614992170579,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1753
    },
    {
      "epoch": 1.9575892857142856,
      "grad_norm": 10.291693895589965,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1754
    },
    {
      "epoch": 1.9587053571428572,
      "grad_norm": 9.915628568849538,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1755
    },
    {
      "epoch": 1.9598214285714286,
      "grad_norm": 9.649151999552155,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1756
    },
    {
      "epoch": 1.9609375,
      "grad_norm": 9.4659274101641,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1757
    },
    {
      "epoch": 1.9620535714285714,
      "grad_norm": 8.978353629684182,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1758
    },
    {
      "epoch": 1.9631696428571428,
      "grad_norm": 9.864076764777964,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1759
    },
    {
      "epoch": 1.9642857142857144,
      "grad_norm": 11.619123723767606,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 1760
    },
    {
      "epoch": 1.9654017857142856,
      "grad_norm": 11.238611213800137,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 1761
    },
    {
      "epoch": 1.9665178571428572,
      "grad_norm": 7.036616567696682,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 1762
    },
    {
      "epoch": 1.9676339285714286,
      "grad_norm": 10.211430045734723,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1763
    },
    {
      "epoch": 1.96875,
      "grad_norm": 10.23651228457571,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1764
    },
    {
      "epoch": 1.9698660714285714,
      "grad_norm": 9.845849141032541,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1765
    },
    {
      "epoch": 1.9709821428571428,
      "grad_norm": 8.247950448304659,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1766
    },
    {
      "epoch": 1.9720982142857144,
      "grad_norm": 10.095508445837714,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1767
    },
    {
      "epoch": 1.9732142857142856,
      "grad_norm": 10.082328117176965,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 1768
    },
    {
      "epoch": 1.9743303571428572,
      "grad_norm": 10.698613653180322,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1769
    },
    {
      "epoch": 1.9754464285714286,
      "grad_norm": 10.394831828865932,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1770
    },
    {
      "epoch": 1.9765625,
      "grad_norm": 9.255084086526033,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1771
    },
    {
      "epoch": 1.9776785714285714,
      "grad_norm": 10.411039251310347,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1772
    },
    {
      "epoch": 1.9787946428571428,
      "grad_norm": 7.752771367261172,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1773
    },
    {
      "epoch": 1.9799107142857144,
      "grad_norm": 8.567242487843156,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1774
    },
    {
      "epoch": 1.9810267857142856,
      "grad_norm": 10.54959135736178,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 1775
    },
    {
      "epoch": 1.9821428571428572,
      "grad_norm": 11.134362021177067,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1776
    },
    {
      "epoch": 1.9832589285714286,
      "grad_norm": 11.22117008393948,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1777
    },
    {
      "epoch": 1.984375,
      "grad_norm": 10.29937583208718,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1778
    },
    {
      "epoch": 1.9854910714285714,
      "grad_norm": 10.238587030926682,
      "learning_rate": 2e-05,
      "loss": 1.3438,
      "step": 1779
    },
    {
      "epoch": 1.9866071428571428,
      "grad_norm": 8.37012970597976,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1780
    },
    {
      "epoch": 1.9877232142857144,
      "grad_norm": 9.651044501444947,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1781
    },
    {
      "epoch": 1.9888392857142856,
      "grad_norm": 11.40789594606158,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1782
    },
    {
      "epoch": 1.9899553571428572,
      "grad_norm": 12.091061993545628,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1783
    },
    {
      "epoch": 1.9910714285714286,
      "grad_norm": 9.713484162986596,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1784
    },
    {
      "epoch": 1.9921875,
      "grad_norm": 12.424716400955532,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1785
    },
    {
      "epoch": 1.9933035714285714,
      "grad_norm": 9.68039652522805,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 1786
    },
    {
      "epoch": 1.9944196428571428,
      "grad_norm": 9.857936340554588,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1787
    },
    {
      "epoch": 1.9955357142857144,
      "grad_norm": 7.558957714233424,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1788
    },
    {
      "epoch": 1.9966517857142856,
      "grad_norm": 9.981136234333295,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1789
    },
    {
      "epoch": 1.9977678571428572,
      "grad_norm": 9.150995893452588,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1790
    },
    {
      "epoch": 1.9988839285714286,
      "grad_norm": 12.592409409825034,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 1791
    },
    {
      "epoch": 2.0,
      "grad_norm": 12.106029582937747,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1792
    },
    {
      "epoch": 2.0011160714285716,
      "grad_norm": 9.822381840453332,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 1793
    },
    {
      "epoch": 2.002232142857143,
      "grad_norm": 10.62407213751816,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1794
    },
    {
      "epoch": 2.0033482142857144,
      "grad_norm": 11.414098369402666,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 1795
    },
    {
      "epoch": 2.0044642857142856,
      "grad_norm": 9.417843280923016,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1796
    },
    {
      "epoch": 2.005580357142857,
      "grad_norm": 9.682010469773758,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1797
    },
    {
      "epoch": 2.0066964285714284,
      "grad_norm": 9.673423131085835,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1798
    },
    {
      "epoch": 2.0078125,
      "grad_norm": 7.189324388301042,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1799
    },
    {
      "epoch": 2.0089285714285716,
      "grad_norm": 10.69450164585614,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1800
    },
    {
      "epoch": 2.010044642857143,
      "grad_norm": 10.740585056921319,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1801
    },
    {
      "epoch": 2.0111607142857144,
      "grad_norm": 8.449504584254626,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 1802
    },
    {
      "epoch": 2.0122767857142856,
      "grad_norm": 9.955811845178815,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1803
    },
    {
      "epoch": 2.013392857142857,
      "grad_norm": 8.025226081716134,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1804
    },
    {
      "epoch": 2.0145089285714284,
      "grad_norm": 7.686808516904516,
      "learning_rate": 2e-05,
      "loss": 2.8438,
      "step": 1805
    },
    {
      "epoch": 2.015625,
      "grad_norm": 10.44936808666503,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 1806
    },
    {
      "epoch": 2.0167410714285716,
      "grad_norm": 10.965704228296216,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1807
    },
    {
      "epoch": 2.017857142857143,
      "grad_norm": 8.602731838068705,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 1808
    },
    {
      "epoch": 2.0189732142857144,
      "grad_norm": 10.0963336114372,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1809
    },
    {
      "epoch": 2.0200892857142856,
      "grad_norm": 7.12529286892382,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1810
    },
    {
      "epoch": 2.021205357142857,
      "grad_norm": 5.783915754674972,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1811
    },
    {
      "epoch": 2.0223214285714284,
      "grad_norm": 9.863389531068933,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 1812
    },
    {
      "epoch": 2.0234375,
      "grad_norm": 10.763520427049228,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1813
    },
    {
      "epoch": 2.0245535714285716,
      "grad_norm": 8.884870784982228,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 1814
    },
    {
      "epoch": 2.025669642857143,
      "grad_norm": 10.613392043977612,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1815
    },
    {
      "epoch": 2.0267857142857144,
      "grad_norm": 12.223340916092805,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 1816
    },
    {
      "epoch": 2.0279017857142856,
      "grad_norm": 9.677760220060964,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 1817
    },
    {
      "epoch": 2.029017857142857,
      "grad_norm": 11.031131776033064,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1818
    },
    {
      "epoch": 2.0301339285714284,
      "grad_norm": 9.917917453498792,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1819
    },
    {
      "epoch": 2.03125,
      "grad_norm": 8.17139003703668,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1820
    },
    {
      "epoch": 2.0323660714285716,
      "grad_norm": 11.4351267265312,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1821
    },
    {
      "epoch": 2.033482142857143,
      "grad_norm": 9.532509666613965,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 1822
    },
    {
      "epoch": 2.0345982142857144,
      "grad_norm": 10.283072930255194,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1823
    },
    {
      "epoch": 2.0357142857142856,
      "grad_norm": 9.563267660570784,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1824
    },
    {
      "epoch": 2.036830357142857,
      "grad_norm": 10.63621675310337,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1825
    },
    {
      "epoch": 2.0379464285714284,
      "grad_norm": 8.51533806333254,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1826
    },
    {
      "epoch": 2.0390625,
      "grad_norm": 9.93382464577833,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 1827
    },
    {
      "epoch": 2.0401785714285716,
      "grad_norm": 11.047063705033333,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 1828
    },
    {
      "epoch": 2.041294642857143,
      "grad_norm": 10.290520148999438,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1829
    },
    {
      "epoch": 2.0424107142857144,
      "grad_norm": 11.203809260069441,
      "learning_rate": 2e-05,
      "loss": 1.4297,
      "step": 1830
    },
    {
      "epoch": 2.0435267857142856,
      "grad_norm": 9.300467188579914,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1831
    },
    {
      "epoch": 2.044642857142857,
      "grad_norm": 11.60959884560448,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1832
    },
    {
      "epoch": 2.0457589285714284,
      "grad_norm": 12.669042329853884,
      "learning_rate": 2e-05,
      "loss": 1.4141,
      "step": 1833
    },
    {
      "epoch": 2.046875,
      "grad_norm": 11.893444292949868,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1834
    },
    {
      "epoch": 2.0479910714285716,
      "grad_norm": 10.21003190934246,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 1835
    },
    {
      "epoch": 2.049107142857143,
      "grad_norm": 11.054057473044807,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 1836
    },
    {
      "epoch": 2.0502232142857144,
      "grad_norm": 9.936586634270272,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1837
    },
    {
      "epoch": 2.0513392857142856,
      "grad_norm": 10.49567126523937,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1838
    },
    {
      "epoch": 2.052455357142857,
      "grad_norm": 11.152291875646014,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 1839
    },
    {
      "epoch": 2.0535714285714284,
      "grad_norm": 8.992109243292832,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1840
    },
    {
      "epoch": 2.0546875,
      "grad_norm": 9.467820766794299,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1841
    },
    {
      "epoch": 2.0558035714285716,
      "grad_norm": 10.664345566132452,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 1842
    },
    {
      "epoch": 2.056919642857143,
      "grad_norm": 9.824854268591123,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1843
    },
    {
      "epoch": 2.0580357142857144,
      "grad_norm": 11.194506811410005,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1844
    },
    {
      "epoch": 2.0591517857142856,
      "grad_norm": 13.234425678751087,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1845
    },
    {
      "epoch": 2.060267857142857,
      "grad_norm": 8.865144330498813,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1846
    },
    {
      "epoch": 2.0613839285714284,
      "grad_norm": 9.868498913237852,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 1847
    },
    {
      "epoch": 2.0625,
      "grad_norm": 10.803484928972079,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 1848
    },
    {
      "epoch": 2.0636160714285716,
      "grad_norm": 14.71840317166281,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1849
    },
    {
      "epoch": 2.064732142857143,
      "grad_norm": 10.037042585093925,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1850
    },
    {
      "epoch": 2.0658482142857144,
      "grad_norm": 10.915046988663374,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 1851
    },
    {
      "epoch": 2.0669642857142856,
      "grad_norm": 9.620970499282413,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1852
    },
    {
      "epoch": 2.068080357142857,
      "grad_norm": 9.886574887187018,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1853
    },
    {
      "epoch": 2.0691964285714284,
      "grad_norm": 11.05224589976215,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1854
    },
    {
      "epoch": 2.0703125,
      "grad_norm": 12.179967621577163,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1855
    },
    {
      "epoch": 2.0714285714285716,
      "grad_norm": 10.66183899156096,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1856
    },
    {
      "epoch": 2.072544642857143,
      "grad_norm": 12.331381824474947,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 1857
    },
    {
      "epoch": 2.0736607142857144,
      "grad_norm": 9.021253517223402,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1858
    },
    {
      "epoch": 2.0747767857142856,
      "grad_norm": 10.18189190929613,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 1859
    },
    {
      "epoch": 2.075892857142857,
      "grad_norm": 11.662393411349573,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 1860
    },
    {
      "epoch": 2.0770089285714284,
      "grad_norm": 8.911098195595184,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1861
    },
    {
      "epoch": 2.078125,
      "grad_norm": 7.690741307578299,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1862
    },
    {
      "epoch": 2.0792410714285716,
      "grad_norm": 10.708743027567031,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1863
    },
    {
      "epoch": 2.080357142857143,
      "grad_norm": 9.288123003776684,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1864
    },
    {
      "epoch": 2.0814732142857144,
      "grad_norm": 11.400867572479228,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1865
    },
    {
      "epoch": 2.0825892857142856,
      "grad_norm": 9.829210014827218,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1866
    },
    {
      "epoch": 2.083705357142857,
      "grad_norm": 9.912964950154098,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1867
    },
    {
      "epoch": 2.0848214285714284,
      "grad_norm": 10.273538073343127,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 1868
    },
    {
      "epoch": 2.0859375,
      "grad_norm": 9.272558451317893,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1869
    },
    {
      "epoch": 2.0870535714285716,
      "grad_norm": 12.656480851242843,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1870
    },
    {
      "epoch": 2.088169642857143,
      "grad_norm": 10.957383433114815,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1871
    },
    {
      "epoch": 2.0892857142857144,
      "grad_norm": 10.724947491049996,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1872
    },
    {
      "epoch": 2.0904017857142856,
      "grad_norm": 10.692337426063814,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 1873
    },
    {
      "epoch": 2.091517857142857,
      "grad_norm": 10.928890896592977,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1874
    },
    {
      "epoch": 2.0926339285714284,
      "grad_norm": 9.81338638681696,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1875
    },
    {
      "epoch": 2.09375,
      "grad_norm": 10.13072936058903,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 1876
    },
    {
      "epoch": 2.0948660714285716,
      "grad_norm": 9.310630599218994,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 1877
    },
    {
      "epoch": 2.095982142857143,
      "grad_norm": 11.960465986760756,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 1878
    },
    {
      "epoch": 2.0970982142857144,
      "grad_norm": 10.695445051799354,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1879
    },
    {
      "epoch": 2.0982142857142856,
      "grad_norm": 12.680938425616072,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 1880
    },
    {
      "epoch": 2.099330357142857,
      "grad_norm": 12.859356113250822,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 1881
    },
    {
      "epoch": 2.1004464285714284,
      "grad_norm": 12.55990629389905,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1882
    },
    {
      "epoch": 2.1015625,
      "grad_norm": 10.234015804038458,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1883
    },
    {
      "epoch": 2.1026785714285716,
      "grad_norm": 13.572905612763591,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1884
    },
    {
      "epoch": 2.103794642857143,
      "grad_norm": 10.90475627322929,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1885
    },
    {
      "epoch": 2.1049107142857144,
      "grad_norm": 9.990208591091326,
      "learning_rate": 2e-05,
      "loss": 1.3516,
      "step": 1886
    },
    {
      "epoch": 2.1060267857142856,
      "grad_norm": 11.992714193786139,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 1887
    },
    {
      "epoch": 2.107142857142857,
      "grad_norm": 11.165328085988172,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1888
    },
    {
      "epoch": 2.1082589285714284,
      "grad_norm": 11.841963684294281,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 1889
    },
    {
      "epoch": 2.109375,
      "grad_norm": 10.27094935826669,
      "learning_rate": 2e-05,
      "loss": 1.3359,
      "step": 1890
    },
    {
      "epoch": 2.1104910714285716,
      "grad_norm": 11.394948283073683,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1891
    },
    {
      "epoch": 2.111607142857143,
      "grad_norm": 10.650632352718203,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 1892
    },
    {
      "epoch": 2.1127232142857144,
      "grad_norm": 11.576645419161006,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1893
    },
    {
      "epoch": 2.1138392857142856,
      "grad_norm": 7.863633769622592,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 1894
    },
    {
      "epoch": 2.114955357142857,
      "grad_norm": 10.124682552426515,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 1895
    },
    {
      "epoch": 2.1160714285714284,
      "grad_norm": 8.223152582458036,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 1896
    },
    {
      "epoch": 2.1171875,
      "grad_norm": 11.595067604546847,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1897
    },
    {
      "epoch": 2.1183035714285716,
      "grad_norm": 10.231216451687006,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1898
    },
    {
      "epoch": 2.119419642857143,
      "grad_norm": 10.27972405018811,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 1899
    },
    {
      "epoch": 2.1205357142857144,
      "grad_norm": 11.268302924657375,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 1900
    },
    {
      "epoch": 2.1216517857142856,
      "grad_norm": 11.180753378591753,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1901
    },
    {
      "epoch": 2.122767857142857,
      "grad_norm": 10.265421963250812,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1902
    },
    {
      "epoch": 2.1238839285714284,
      "grad_norm": 10.950582204685576,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 1903
    },
    {
      "epoch": 2.125,
      "grad_norm": 9.400071723474186,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1904
    },
    {
      "epoch": 2.1261160714285716,
      "grad_norm": 10.194595138650328,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1905
    },
    {
      "epoch": 2.127232142857143,
      "grad_norm": 9.6595477268488,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1906
    },
    {
      "epoch": 2.1283482142857144,
      "grad_norm": 11.780604432219294,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 1907
    },
    {
      "epoch": 2.1294642857142856,
      "grad_norm": 14.399011570816718,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1908
    },
    {
      "epoch": 2.130580357142857,
      "grad_norm": 8.618422140270424,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 1909
    },
    {
      "epoch": 2.1316964285714284,
      "grad_norm": 10.301798772739902,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1910
    },
    {
      "epoch": 2.1328125,
      "grad_norm": 9.939550342859528,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 1911
    },
    {
      "epoch": 2.1339285714285716,
      "grad_norm": 10.894059932732015,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 1912
    },
    {
      "epoch": 2.135044642857143,
      "grad_norm": 12.930374770862796,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 1913
    },
    {
      "epoch": 2.1361607142857144,
      "grad_norm": 10.758208143457543,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1914
    },
    {
      "epoch": 2.1372767857142856,
      "grad_norm": 9.656651349013066,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1915
    },
    {
      "epoch": 2.138392857142857,
      "grad_norm": 9.646026999554406,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1916
    },
    {
      "epoch": 2.1395089285714284,
      "grad_norm": 11.228110359051682,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 1917
    },
    {
      "epoch": 2.140625,
      "grad_norm": 9.549003897647513,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1918
    },
    {
      "epoch": 2.1417410714285716,
      "grad_norm": 9.660636186108203,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1919
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 10.270525922161406,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 1920
    },
    {
      "epoch": 2.1439732142857144,
      "grad_norm": 10.056201749779042,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1921
    },
    {
      "epoch": 2.1450892857142856,
      "grad_norm": 9.222236613413683,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 1922
    },
    {
      "epoch": 2.146205357142857,
      "grad_norm": 9.493701180215389,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1923
    },
    {
      "epoch": 2.1473214285714284,
      "grad_norm": 11.46240345443136,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 1924
    },
    {
      "epoch": 2.1484375,
      "grad_norm": 9.655735043950722,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 1925
    },
    {
      "epoch": 2.1495535714285716,
      "grad_norm": 11.657820131277878,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1926
    },
    {
      "epoch": 2.150669642857143,
      "grad_norm": 9.477715729277525,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 1927
    },
    {
      "epoch": 2.1517857142857144,
      "grad_norm": 9.677115833752564,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 1928
    },
    {
      "epoch": 2.1529017857142856,
      "grad_norm": 12.069180712203723,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 1929
    },
    {
      "epoch": 2.154017857142857,
      "grad_norm": 7.576675980816067,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 1930
    },
    {
      "epoch": 2.1551339285714284,
      "grad_norm": 10.953621245790764,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 1931
    },
    {
      "epoch": 2.15625,
      "grad_norm": 12.233232320710124,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1932
    },
    {
      "epoch": 2.1573660714285716,
      "grad_norm": 10.189326621052682,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1933
    },
    {
      "epoch": 2.158482142857143,
      "grad_norm": 12.029515620310818,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1934
    },
    {
      "epoch": 2.1595982142857144,
      "grad_norm": 8.919752261627439,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1935
    },
    {
      "epoch": 2.1607142857142856,
      "grad_norm": 10.24217992348568,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 1936
    },
    {
      "epoch": 2.161830357142857,
      "grad_norm": 10.831668358439334,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1937
    },
    {
      "epoch": 2.1629464285714284,
      "grad_norm": 11.075661925216254,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 1938
    },
    {
      "epoch": 2.1640625,
      "grad_norm": 9.220238916307084,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1939
    },
    {
      "epoch": 2.1651785714285716,
      "grad_norm": 8.151853354652744,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 1940
    },
    {
      "epoch": 2.166294642857143,
      "grad_norm": 12.451587005433996,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 1941
    },
    {
      "epoch": 2.1674107142857144,
      "grad_norm": 12.352529138356658,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 1942
    },
    {
      "epoch": 2.1685267857142856,
      "grad_norm": 11.606319312050518,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1943
    },
    {
      "epoch": 2.169642857142857,
      "grad_norm": 10.72425601571367,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1944
    },
    {
      "epoch": 2.1707589285714284,
      "grad_norm": 11.170198309149725,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 1945
    },
    {
      "epoch": 2.171875,
      "grad_norm": 10.432250128042517,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 1946
    },
    {
      "epoch": 2.1729910714285716,
      "grad_norm": 8.59487490999088,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1947
    },
    {
      "epoch": 2.174107142857143,
      "grad_norm": 10.60836981019486,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 1948
    },
    {
      "epoch": 2.1752232142857144,
      "grad_norm": 12.596073998649212,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1949
    },
    {
      "epoch": 2.1763392857142856,
      "grad_norm": 9.889170782738399,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 1950
    },
    {
      "epoch": 2.177455357142857,
      "grad_norm": 11.460169746496728,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1951
    },
    {
      "epoch": 2.1785714285714284,
      "grad_norm": 10.672564446783392,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 1952
    },
    {
      "epoch": 2.1796875,
      "grad_norm": 10.252967291611633,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 1953
    },
    {
      "epoch": 2.1808035714285716,
      "grad_norm": 11.110987163307215,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 1954
    },
    {
      "epoch": 2.181919642857143,
      "grad_norm": 12.207441260007517,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 1955
    },
    {
      "epoch": 2.1830357142857144,
      "grad_norm": 12.150715301483483,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 1956
    },
    {
      "epoch": 2.1841517857142856,
      "grad_norm": 12.580552763801764,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1957
    },
    {
      "epoch": 2.185267857142857,
      "grad_norm": 8.855464399494648,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 1958
    },
    {
      "epoch": 2.1863839285714284,
      "grad_norm": 9.048336711126007,
      "learning_rate": 2e-05,
      "loss": 2.7656,
      "step": 1959
    },
    {
      "epoch": 2.1875,
      "grad_norm": 7.826038435202088,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1960
    },
    {
      "epoch": 2.1886160714285716,
      "grad_norm": 11.181462018835989,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 1961
    },
    {
      "epoch": 2.189732142857143,
      "grad_norm": 9.09920823072326,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1962
    },
    {
      "epoch": 2.1908482142857144,
      "grad_norm": 12.631568291117253,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 1963
    },
    {
      "epoch": 2.1919642857142856,
      "grad_norm": 9.231251348101978,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 1964
    },
    {
      "epoch": 2.193080357142857,
      "grad_norm": 10.739606104042199,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 1965
    },
    {
      "epoch": 2.1941964285714284,
      "grad_norm": 11.455519916353941,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 1966
    },
    {
      "epoch": 2.1953125,
      "grad_norm": 7.8850166248288875,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1967
    },
    {
      "epoch": 2.1964285714285716,
      "grad_norm": 10.283494204626173,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 1968
    },
    {
      "epoch": 2.197544642857143,
      "grad_norm": 11.054139449861559,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 1969
    },
    {
      "epoch": 2.1986607142857144,
      "grad_norm": 10.09062152663219,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 1970
    },
    {
      "epoch": 2.1997767857142856,
      "grad_norm": 10.653206596655806,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 1971
    },
    {
      "epoch": 2.200892857142857,
      "grad_norm": 9.750713292467122,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 1972
    },
    {
      "epoch": 2.2020089285714284,
      "grad_norm": 11.715619937022423,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 1973
    },
    {
      "epoch": 2.203125,
      "grad_norm": 8.954988738377452,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 1974
    },
    {
      "epoch": 2.2042410714285716,
      "grad_norm": 11.397366069049982,
      "learning_rate": 2e-05,
      "loss": 1.3516,
      "step": 1975
    },
    {
      "epoch": 2.205357142857143,
      "grad_norm": 10.79339093272249,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 1976
    },
    {
      "epoch": 2.2064732142857144,
      "grad_norm": 11.952708746768078,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 1977
    },
    {
      "epoch": 2.2075892857142856,
      "grad_norm": 11.030092286669804,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 1978
    },
    {
      "epoch": 2.208705357142857,
      "grad_norm": 10.469886011485702,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 1979
    },
    {
      "epoch": 2.2098214285714284,
      "grad_norm": 7.9756332013137365,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 1980
    },
    {
      "epoch": 2.2109375,
      "grad_norm": 14.087488849514308,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1981
    },
    {
      "epoch": 2.2120535714285716,
      "grad_norm": 8.497861823122374,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 1982
    },
    {
      "epoch": 2.213169642857143,
      "grad_norm": 11.596381536488723,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 1983
    },
    {
      "epoch": 2.2142857142857144,
      "grad_norm": 11.613433927971053,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 1984
    },
    {
      "epoch": 2.2154017857142856,
      "grad_norm": 9.784185584533116,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 1985
    },
    {
      "epoch": 2.216517857142857,
      "grad_norm": 10.863503833372429,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1986
    },
    {
      "epoch": 2.2176339285714284,
      "grad_norm": 10.763928756975108,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 1987
    },
    {
      "epoch": 2.21875,
      "grad_norm": 11.37169044782318,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 1988
    },
    {
      "epoch": 2.2198660714285716,
      "grad_norm": 12.507832396345457,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 1989
    },
    {
      "epoch": 2.220982142857143,
      "grad_norm": 9.486009932398241,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 1990
    },
    {
      "epoch": 2.2220982142857144,
      "grad_norm": 11.335809940594569,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 1991
    },
    {
      "epoch": 2.2232142857142856,
      "grad_norm": 10.10704543499021,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 1992
    },
    {
      "epoch": 2.224330357142857,
      "grad_norm": 9.817816937154387,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 1993
    },
    {
      "epoch": 2.2254464285714284,
      "grad_norm": 10.282427976951572,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 1994
    },
    {
      "epoch": 2.2265625,
      "grad_norm": 12.78481827483124,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 1995
    },
    {
      "epoch": 2.2276785714285716,
      "grad_norm": 11.770726750078303,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 1996
    },
    {
      "epoch": 2.228794642857143,
      "grad_norm": 11.832303632975865,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 1997
    },
    {
      "epoch": 2.2299107142857144,
      "grad_norm": 10.853973585532302,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 1998
    },
    {
      "epoch": 2.2310267857142856,
      "grad_norm": 8.8428605504745,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 1999
    },
    {
      "epoch": 2.232142857142857,
      "grad_norm": 11.412407170424641,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 2000
    },
    {
      "epoch": 2.2332589285714284,
      "grad_norm": 10.872836923241948,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 2001
    },
    {
      "epoch": 2.234375,
      "grad_norm": 11.218584703957873,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 2002
    },
    {
      "epoch": 2.2354910714285716,
      "grad_norm": 9.0769921922566,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 2003
    },
    {
      "epoch": 2.236607142857143,
      "grad_norm": 8.961169855864497,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2004
    },
    {
      "epoch": 2.2377232142857144,
      "grad_norm": 11.096698320588528,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 2005
    },
    {
      "epoch": 2.2388392857142856,
      "grad_norm": 12.554571860315711,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 2006
    },
    {
      "epoch": 2.239955357142857,
      "grad_norm": 11.073085069499621,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2007
    },
    {
      "epoch": 2.2410714285714284,
      "grad_norm": 11.9308947524933,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 2008
    },
    {
      "epoch": 2.2421875,
      "grad_norm": 13.228258680686166,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 2009
    },
    {
      "epoch": 2.2433035714285716,
      "grad_norm": 10.637248142472789,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2010
    },
    {
      "epoch": 2.244419642857143,
      "grad_norm": 11.096301536896927,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2011
    },
    {
      "epoch": 2.2455357142857144,
      "grad_norm": 10.160823782655598,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 2012
    },
    {
      "epoch": 2.2466517857142856,
      "grad_norm": 11.502661827173782,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2013
    },
    {
      "epoch": 2.247767857142857,
      "grad_norm": 11.582823539255555,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 2014
    },
    {
      "epoch": 2.2488839285714284,
      "grad_norm": 15.104984814932493,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2015
    },
    {
      "epoch": 2.25,
      "grad_norm": 12.326304829827754,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2016
    },
    {
      "epoch": 2.2511160714285716,
      "grad_norm": 12.597006054022723,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 2017
    },
    {
      "epoch": 2.252232142857143,
      "grad_norm": 9.36661395357238,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 2018
    },
    {
      "epoch": 2.2533482142857144,
      "grad_norm": 13.180324209989777,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2019
    },
    {
      "epoch": 2.2544642857142856,
      "grad_norm": 30.90104758078259,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 2020
    },
    {
      "epoch": 2.255580357142857,
      "grad_norm": 10.210065812794886,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 2021
    },
    {
      "epoch": 2.2566964285714284,
      "grad_norm": 11.316372343137372,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2022
    },
    {
      "epoch": 2.2578125,
      "grad_norm": 10.759830022980818,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2023
    },
    {
      "epoch": 2.2589285714285716,
      "grad_norm": 9.98994115422321,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2024
    },
    {
      "epoch": 2.260044642857143,
      "grad_norm": 11.136871785091833,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2025
    },
    {
      "epoch": 2.2611607142857144,
      "grad_norm": 9.981989255199577,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2026
    },
    {
      "epoch": 2.2622767857142856,
      "grad_norm": 11.97382161868853,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 2027
    },
    {
      "epoch": 2.263392857142857,
      "grad_norm": 8.467569883132139,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 2028
    },
    {
      "epoch": 2.2645089285714284,
      "grad_norm": 11.198182368426238,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2029
    },
    {
      "epoch": 2.265625,
      "grad_norm": 12.755353725735418,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 2030
    },
    {
      "epoch": 2.2667410714285716,
      "grad_norm": 8.984003964199266,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2031
    },
    {
      "epoch": 2.267857142857143,
      "grad_norm": 9.446191099563327,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2032
    },
    {
      "epoch": 2.2689732142857144,
      "grad_norm": 9.584511308639664,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 2033
    },
    {
      "epoch": 2.2700892857142856,
      "grad_norm": 11.447951891773648,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2034
    },
    {
      "epoch": 2.271205357142857,
      "grad_norm": 9.017514805891603,
      "learning_rate": 2e-05,
      "loss": 3.125,
      "step": 2035
    },
    {
      "epoch": 2.2723214285714284,
      "grad_norm": 9.777482141429276,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 2036
    },
    {
      "epoch": 2.2734375,
      "grad_norm": 11.674711863588884,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 2037
    },
    {
      "epoch": 2.2745535714285716,
      "grad_norm": 9.302178436977153,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 2038
    },
    {
      "epoch": 2.275669642857143,
      "grad_norm": 14.39163928977081,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 2039
    },
    {
      "epoch": 2.2767857142857144,
      "grad_norm": 10.78561539092601,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 2040
    },
    {
      "epoch": 2.2779017857142856,
      "grad_norm": 12.473542604378203,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 2041
    },
    {
      "epoch": 2.279017857142857,
      "grad_norm": 11.035012660019877,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 2042
    },
    {
      "epoch": 2.2801339285714284,
      "grad_norm": 10.488036024132546,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2043
    },
    {
      "epoch": 2.28125,
      "grad_norm": 10.721089552189948,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 2044
    },
    {
      "epoch": 2.2823660714285716,
      "grad_norm": 10.059252841189904,
      "learning_rate": 2e-05,
      "loss": 2.7656,
      "step": 2045
    },
    {
      "epoch": 2.283482142857143,
      "grad_norm": 12.23731716754852,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2046
    },
    {
      "epoch": 2.2845982142857144,
      "grad_norm": 9.677817463359654,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 2047
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 6.9133169606578235,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2048
    },
    {
      "epoch": 2.286830357142857,
      "grad_norm": 9.945466059272823,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2049
    },
    {
      "epoch": 2.2879464285714284,
      "grad_norm": 11.169150824515894,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2050
    },
    {
      "epoch": 2.2890625,
      "grad_norm": 9.5009497579593,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2051
    },
    {
      "epoch": 2.2901785714285716,
      "grad_norm": 9.731337117711933,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2052
    },
    {
      "epoch": 2.291294642857143,
      "grad_norm": 9.901090100829354,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 2053
    },
    {
      "epoch": 2.2924107142857144,
      "grad_norm": 10.405331264884817,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2054
    },
    {
      "epoch": 2.2935267857142856,
      "grad_norm": 12.99484377148081,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 2055
    },
    {
      "epoch": 2.294642857142857,
      "grad_norm": 12.279863511299613,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 2056
    },
    {
      "epoch": 2.2957589285714284,
      "grad_norm": 10.884658099012533,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2057
    },
    {
      "epoch": 2.296875,
      "grad_norm": 13.07787821506119,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 2058
    },
    {
      "epoch": 2.2979910714285716,
      "grad_norm": 11.805612470465844,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2059
    },
    {
      "epoch": 2.299107142857143,
      "grad_norm": 9.103216983924355,
      "learning_rate": 2e-05,
      "loss": 1.3906,
      "step": 2060
    },
    {
      "epoch": 2.3002232142857144,
      "grad_norm": 9.320138148332543,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 2061
    },
    {
      "epoch": 2.3013392857142856,
      "grad_norm": 11.91013460839547,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 2062
    },
    {
      "epoch": 2.302455357142857,
      "grad_norm": 10.439740943117881,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 2063
    },
    {
      "epoch": 2.3035714285714284,
      "grad_norm": 11.360833266600595,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 2064
    },
    {
      "epoch": 2.3046875,
      "grad_norm": 11.786235681812014,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2065
    },
    {
      "epoch": 2.3058035714285716,
      "grad_norm": 9.956310209211486,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 2066
    },
    {
      "epoch": 2.306919642857143,
      "grad_norm": 11.921871175114102,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 2067
    },
    {
      "epoch": 2.3080357142857144,
      "grad_norm": 10.089629616352457,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2068
    },
    {
      "epoch": 2.3091517857142856,
      "grad_norm": 10.284477313043732,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 2069
    },
    {
      "epoch": 2.310267857142857,
      "grad_norm": 10.735148486369757,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 2070
    },
    {
      "epoch": 2.3113839285714284,
      "grad_norm": 9.161382519342416,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2071
    },
    {
      "epoch": 2.3125,
      "grad_norm": 10.04921660164699,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2072
    },
    {
      "epoch": 2.3136160714285716,
      "grad_norm": 9.8740335562298,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 2073
    },
    {
      "epoch": 2.314732142857143,
      "grad_norm": 9.068900526284128,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2074
    },
    {
      "epoch": 2.3158482142857144,
      "grad_norm": 11.043175958453066,
      "learning_rate": 2e-05,
      "loss": 1.3672,
      "step": 2075
    },
    {
      "epoch": 2.3169642857142856,
      "grad_norm": 8.96521459087188,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 2076
    },
    {
      "epoch": 2.318080357142857,
      "grad_norm": 14.907489296876808,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 2077
    },
    {
      "epoch": 2.3191964285714284,
      "grad_norm": 12.5342576171337,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2078
    },
    {
      "epoch": 2.3203125,
      "grad_norm": 11.906479155150024,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2079
    },
    {
      "epoch": 2.3214285714285716,
      "grad_norm": 11.223183311285116,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 2080
    },
    {
      "epoch": 2.322544642857143,
      "grad_norm": 12.397705711128467,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 2081
    },
    {
      "epoch": 2.3236607142857144,
      "grad_norm": 10.19067923992187,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2082
    },
    {
      "epoch": 2.3247767857142856,
      "grad_norm": 10.43455044377065,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 2083
    },
    {
      "epoch": 2.325892857142857,
      "grad_norm": 11.688286837364688,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 2084
    },
    {
      "epoch": 2.3270089285714284,
      "grad_norm": 10.954173398982126,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 2085
    },
    {
      "epoch": 2.328125,
      "grad_norm": 10.712679616947396,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 2086
    },
    {
      "epoch": 2.3292410714285716,
      "grad_norm": 11.058769713059492,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2087
    },
    {
      "epoch": 2.330357142857143,
      "grad_norm": 11.824132815270298,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 2088
    },
    {
      "epoch": 2.3314732142857144,
      "grad_norm": 9.597546352184434,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 2089
    },
    {
      "epoch": 2.3325892857142856,
      "grad_norm": 10.429317276738145,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2090
    },
    {
      "epoch": 2.333705357142857,
      "grad_norm": 9.70276760077978,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 2091
    },
    {
      "epoch": 2.3348214285714284,
      "grad_norm": 10.827973939241788,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 2092
    },
    {
      "epoch": 2.3359375,
      "grad_norm": 9.819973931181746,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2093
    },
    {
      "epoch": 2.3370535714285716,
      "grad_norm": 12.25276887323289,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 2094
    },
    {
      "epoch": 2.338169642857143,
      "grad_norm": 10.31488984301303,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2095
    },
    {
      "epoch": 2.3392857142857144,
      "grad_norm": 7.495486876786707,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 2096
    },
    {
      "epoch": 2.3404017857142856,
      "grad_norm": 9.204301590212769,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2097
    },
    {
      "epoch": 2.341517857142857,
      "grad_norm": 10.668719272685149,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2098
    },
    {
      "epoch": 2.3426339285714284,
      "grad_norm": 9.461966238822681,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2099
    },
    {
      "epoch": 2.34375,
      "grad_norm": 13.918856682570611,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 2100
    },
    {
      "epoch": 2.3448660714285716,
      "grad_norm": 9.641260736109349,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 2101
    },
    {
      "epoch": 2.345982142857143,
      "grad_norm": 10.136474859576438,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 2102
    },
    {
      "epoch": 2.3470982142857144,
      "grad_norm": 13.013238757568532,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2103
    },
    {
      "epoch": 2.3482142857142856,
      "grad_norm": 8.605781246010444,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2104
    },
    {
      "epoch": 2.349330357142857,
      "grad_norm": 13.373362717170412,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 2105
    },
    {
      "epoch": 2.3504464285714284,
      "grad_norm": 13.575436505093316,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2106
    },
    {
      "epoch": 2.3515625,
      "grad_norm": 14.467753658472585,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2107
    },
    {
      "epoch": 2.3526785714285716,
      "grad_norm": 11.13691038623425,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2108
    },
    {
      "epoch": 2.353794642857143,
      "grad_norm": 10.830710072581242,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2109
    },
    {
      "epoch": 2.3549107142857144,
      "grad_norm": 13.822814657239189,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2110
    },
    {
      "epoch": 2.3560267857142856,
      "grad_norm": 9.02416852735426,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2111
    },
    {
      "epoch": 2.357142857142857,
      "grad_norm": 11.861676782289942,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2112
    },
    {
      "epoch": 2.3582589285714284,
      "grad_norm": 12.759006223607779,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 2113
    },
    {
      "epoch": 2.359375,
      "grad_norm": 13.043137622820343,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2114
    },
    {
      "epoch": 2.3604910714285716,
      "grad_norm": 10.833040286985598,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 2115
    },
    {
      "epoch": 2.361607142857143,
      "grad_norm": 9.315906032587774,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 2116
    },
    {
      "epoch": 2.3627232142857144,
      "grad_norm": 8.723044892924838,
      "learning_rate": 2e-05,
      "loss": 2.7656,
      "step": 2117
    },
    {
      "epoch": 2.3638392857142856,
      "grad_norm": 11.172268555938803,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 2118
    },
    {
      "epoch": 2.364955357142857,
      "grad_norm": 11.299747805585227,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2119
    },
    {
      "epoch": 2.3660714285714284,
      "grad_norm": 11.031050563157873,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 2120
    },
    {
      "epoch": 2.3671875,
      "grad_norm": 12.406366052118994,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 2121
    },
    {
      "epoch": 2.3683035714285716,
      "grad_norm": 10.269901694370644,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 2122
    },
    {
      "epoch": 2.369419642857143,
      "grad_norm": 11.835776449226675,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 2123
    },
    {
      "epoch": 2.3705357142857144,
      "grad_norm": 12.789111627944873,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 2124
    },
    {
      "epoch": 2.3716517857142856,
      "grad_norm": 11.38119449264324,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 2125
    },
    {
      "epoch": 2.372767857142857,
      "grad_norm": 8.943439228418665,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 2126
    },
    {
      "epoch": 2.3738839285714284,
      "grad_norm": 13.340805397412925,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2127
    },
    {
      "epoch": 2.375,
      "grad_norm": 11.622258755907179,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 2128
    },
    {
      "epoch": 2.3761160714285716,
      "grad_norm": 9.714232737687885,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 2129
    },
    {
      "epoch": 2.377232142857143,
      "grad_norm": 10.512555670991603,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 2130
    },
    {
      "epoch": 2.3783482142857144,
      "grad_norm": 12.443630266480442,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2131
    },
    {
      "epoch": 2.3794642857142856,
      "grad_norm": 10.906488155835062,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 2132
    },
    {
      "epoch": 2.380580357142857,
      "grad_norm": 14.45920748179823,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 2133
    },
    {
      "epoch": 2.3816964285714284,
      "grad_norm": 8.901393781733487,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 2134
    },
    {
      "epoch": 2.3828125,
      "grad_norm": 11.841088929199488,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 2135
    },
    {
      "epoch": 2.3839285714285716,
      "grad_norm": 10.761385193396833,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 2136
    },
    {
      "epoch": 2.385044642857143,
      "grad_norm": 10.863738348465954,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 2137
    },
    {
      "epoch": 2.3861607142857144,
      "grad_norm": 8.070752616241567,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 2138
    },
    {
      "epoch": 2.3872767857142856,
      "grad_norm": 9.300761816698497,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 2139
    },
    {
      "epoch": 2.388392857142857,
      "grad_norm": 12.533966998080547,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 2140
    },
    {
      "epoch": 2.3895089285714284,
      "grad_norm": 14.3602836975328,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2141
    },
    {
      "epoch": 2.390625,
      "grad_norm": 9.731306350103605,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 2142
    },
    {
      "epoch": 2.3917410714285716,
      "grad_norm": 9.336227196760792,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 2143
    },
    {
      "epoch": 2.392857142857143,
      "grad_norm": 13.41480449390836,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 2144
    },
    {
      "epoch": 2.3939732142857144,
      "grad_norm": 9.945705465728171,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 2145
    },
    {
      "epoch": 2.3950892857142856,
      "grad_norm": 10.70543059245785,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 2146
    },
    {
      "epoch": 2.396205357142857,
      "grad_norm": 10.501010549335179,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2147
    },
    {
      "epoch": 2.3973214285714284,
      "grad_norm": 9.949004585193503,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2148
    },
    {
      "epoch": 2.3984375,
      "grad_norm": 11.909552315676564,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2149
    },
    {
      "epoch": 2.3995535714285716,
      "grad_norm": 11.64170940856718,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2150
    },
    {
      "epoch": 2.400669642857143,
      "grad_norm": 14.345343555755418,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 2151
    },
    {
      "epoch": 2.4017857142857144,
      "grad_norm": 10.933611025865327,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 2152
    },
    {
      "epoch": 2.4029017857142856,
      "grad_norm": 11.746190757919845,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 2153
    },
    {
      "epoch": 2.404017857142857,
      "grad_norm": 10.944094491102017,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 2154
    },
    {
      "epoch": 2.4051339285714284,
      "grad_norm": 11.470265630872204,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 2155
    },
    {
      "epoch": 2.40625,
      "grad_norm": 9.757480356447218,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2156
    },
    {
      "epoch": 2.4073660714285716,
      "grad_norm": 12.992250678052539,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 2157
    },
    {
      "epoch": 2.408482142857143,
      "grad_norm": 11.817654282256242,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2158
    },
    {
      "epoch": 2.4095982142857144,
      "grad_norm": 9.714366950200496,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 2159
    },
    {
      "epoch": 2.4107142857142856,
      "grad_norm": 10.811174616701685,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 2160
    },
    {
      "epoch": 2.411830357142857,
      "grad_norm": 11.026848096558085,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 2161
    },
    {
      "epoch": 2.4129464285714284,
      "grad_norm": 12.303707576299066,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 2162
    },
    {
      "epoch": 2.4140625,
      "grad_norm": 11.171810247422464,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 2163
    },
    {
      "epoch": 2.4151785714285716,
      "grad_norm": 11.333234003281222,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2164
    },
    {
      "epoch": 2.416294642857143,
      "grad_norm": 12.628528555764742,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 2165
    },
    {
      "epoch": 2.4174107142857144,
      "grad_norm": 12.30696547496869,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2166
    },
    {
      "epoch": 2.4185267857142856,
      "grad_norm": 11.819913992626557,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 2167
    },
    {
      "epoch": 2.419642857142857,
      "grad_norm": 11.311493998960511,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 2168
    },
    {
      "epoch": 2.4207589285714284,
      "grad_norm": 10.734558151002068,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 2169
    },
    {
      "epoch": 2.421875,
      "grad_norm": 10.898354914277185,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2170
    },
    {
      "epoch": 2.4229910714285716,
      "grad_norm": 10.162596013018183,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2171
    },
    {
      "epoch": 2.424107142857143,
      "grad_norm": 11.939011777431853,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 2172
    },
    {
      "epoch": 2.4252232142857144,
      "grad_norm": 11.549650571635294,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2173
    },
    {
      "epoch": 2.4263392857142856,
      "grad_norm": 14.762418175187335,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 2174
    },
    {
      "epoch": 2.427455357142857,
      "grad_norm": 13.092962612739084,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2175
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 9.338771785537029,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2176
    },
    {
      "epoch": 2.4296875,
      "grad_norm": 9.36945679428785,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 2177
    },
    {
      "epoch": 2.4308035714285716,
      "grad_norm": 11.323686678513445,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 2178
    },
    {
      "epoch": 2.431919642857143,
      "grad_norm": 11.927296303260324,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 2179
    },
    {
      "epoch": 2.4330357142857144,
      "grad_norm": 11.045769536465155,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 2180
    },
    {
      "epoch": 2.4341517857142856,
      "grad_norm": 12.933721504110213,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 2181
    },
    {
      "epoch": 2.435267857142857,
      "grad_norm": 12.069072168879156,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 2182
    },
    {
      "epoch": 2.4363839285714284,
      "grad_norm": 11.803828535671116,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2183
    },
    {
      "epoch": 2.4375,
      "grad_norm": 10.169844663136653,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 2184
    },
    {
      "epoch": 2.4386160714285716,
      "grad_norm": 11.238036435465009,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2185
    },
    {
      "epoch": 2.439732142857143,
      "grad_norm": 9.945007542433544,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 2186
    },
    {
      "epoch": 2.4408482142857144,
      "grad_norm": 11.18588181543639,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2187
    },
    {
      "epoch": 2.4419642857142856,
      "grad_norm": 13.945880480857078,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 2188
    },
    {
      "epoch": 2.443080357142857,
      "grad_norm": 10.12103413397919,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2189
    },
    {
      "epoch": 2.4441964285714284,
      "grad_norm": 9.930914245961091,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2190
    },
    {
      "epoch": 2.4453125,
      "grad_norm": 10.715356171638348,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 2191
    },
    {
      "epoch": 2.4464285714285716,
      "grad_norm": 11.75225887828527,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 2192
    },
    {
      "epoch": 2.447544642857143,
      "grad_norm": 9.353968618514118,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 2193
    },
    {
      "epoch": 2.4486607142857144,
      "grad_norm": 11.146401507102688,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 2194
    },
    {
      "epoch": 2.4497767857142856,
      "grad_norm": 9.41783500465039,
      "learning_rate": 2e-05,
      "loss": 1.5312,
      "step": 2195
    },
    {
      "epoch": 2.450892857142857,
      "grad_norm": 12.267987157887003,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 2196
    },
    {
      "epoch": 2.4520089285714284,
      "grad_norm": 12.157946147681251,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 2197
    },
    {
      "epoch": 2.453125,
      "grad_norm": 7.794158153925989,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2198
    },
    {
      "epoch": 2.4542410714285716,
      "grad_norm": 11.016612798313101,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2199
    },
    {
      "epoch": 2.455357142857143,
      "grad_norm": 10.9891665743122,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2200
    },
    {
      "epoch": 2.4564732142857144,
      "grad_norm": 11.419367476612814,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 2201
    },
    {
      "epoch": 2.4575892857142856,
      "grad_norm": 11.643653421657659,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2202
    },
    {
      "epoch": 2.458705357142857,
      "grad_norm": 10.780887083526368,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 2203
    },
    {
      "epoch": 2.4598214285714284,
      "grad_norm": 11.687051359494026,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2204
    },
    {
      "epoch": 2.4609375,
      "grad_norm": 13.18217300916371,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2205
    },
    {
      "epoch": 2.4620535714285716,
      "grad_norm": 10.756219519930601,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 2206
    },
    {
      "epoch": 2.463169642857143,
      "grad_norm": 9.901010658146813,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2207
    },
    {
      "epoch": 2.4642857142857144,
      "grad_norm": 10.640389114486942,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 2208
    },
    {
      "epoch": 2.4654017857142856,
      "grad_norm": 10.103157621935603,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 2209
    },
    {
      "epoch": 2.466517857142857,
      "grad_norm": 9.960565896061459,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 2210
    },
    {
      "epoch": 2.4676339285714284,
      "grad_norm": 10.450570844680303,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 2211
    },
    {
      "epoch": 2.46875,
      "grad_norm": 11.429703848610414,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 2212
    },
    {
      "epoch": 2.4698660714285716,
      "grad_norm": 10.341674796011212,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2213
    },
    {
      "epoch": 2.470982142857143,
      "grad_norm": 12.810643971929858,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2214
    },
    {
      "epoch": 2.4720982142857144,
      "grad_norm": 9.286999756446718,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 2215
    },
    {
      "epoch": 2.4732142857142856,
      "grad_norm": 11.53725936093152,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 2216
    },
    {
      "epoch": 2.474330357142857,
      "grad_norm": 12.683260254336547,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 2217
    },
    {
      "epoch": 2.4754464285714284,
      "grad_norm": 10.949350795029996,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 2218
    },
    {
      "epoch": 2.4765625,
      "grad_norm": 10.851337818842346,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 2219
    },
    {
      "epoch": 2.4776785714285716,
      "grad_norm": 9.867208693987676,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2220
    },
    {
      "epoch": 2.478794642857143,
      "grad_norm": 10.802019154858701,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 2221
    },
    {
      "epoch": 2.4799107142857144,
      "grad_norm": 13.079855582097432,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2222
    },
    {
      "epoch": 2.4810267857142856,
      "grad_norm": 10.570910407942646,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 2223
    },
    {
      "epoch": 2.482142857142857,
      "grad_norm": 10.977097802743266,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 2224
    },
    {
      "epoch": 2.4832589285714284,
      "grad_norm": 10.872589093534394,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 2225
    },
    {
      "epoch": 2.484375,
      "grad_norm": 11.37017021580431,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2226
    },
    {
      "epoch": 2.4854910714285716,
      "grad_norm": 10.112710779282873,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 2227
    },
    {
      "epoch": 2.486607142857143,
      "grad_norm": 11.001560648507368,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 2228
    },
    {
      "epoch": 2.4877232142857144,
      "grad_norm": 13.014119514301806,
      "learning_rate": 2e-05,
      "loss": 1.4141,
      "step": 2229
    },
    {
      "epoch": 2.4888392857142856,
      "grad_norm": 14.843461430971637,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 2230
    },
    {
      "epoch": 2.489955357142857,
      "grad_norm": 10.304359165031583,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 2231
    },
    {
      "epoch": 2.4910714285714284,
      "grad_norm": 12.284967386418835,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2232
    },
    {
      "epoch": 2.4921875,
      "grad_norm": 10.98271523784298,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2233
    },
    {
      "epoch": 2.4933035714285716,
      "grad_norm": 11.054165607900963,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 2234
    },
    {
      "epoch": 2.494419642857143,
      "grad_norm": 12.426312187936329,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 2235
    },
    {
      "epoch": 2.4955357142857144,
      "grad_norm": 10.719794553022416,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 2236
    },
    {
      "epoch": 2.4966517857142856,
      "grad_norm": 10.037037960953839,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 2237
    },
    {
      "epoch": 2.497767857142857,
      "grad_norm": 19.839852899672803,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 2238
    },
    {
      "epoch": 2.4988839285714284,
      "grad_norm": 10.209831676400048,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2239
    },
    {
      "epoch": 2.5,
      "grad_norm": 11.92100643717555,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 2240
    },
    {
      "epoch": 2.501116071428571,
      "grad_norm": 9.669442480160093,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 2241
    },
    {
      "epoch": 2.502232142857143,
      "grad_norm": 13.227194266123885,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2242
    },
    {
      "epoch": 2.5033482142857144,
      "grad_norm": 12.104151647678478,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 2243
    },
    {
      "epoch": 2.5044642857142856,
      "grad_norm": 12.188065417916574,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 2244
    },
    {
      "epoch": 2.505580357142857,
      "grad_norm": 11.05010399894327,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2245
    },
    {
      "epoch": 2.506696428571429,
      "grad_norm": 12.33597935513424,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 2246
    },
    {
      "epoch": 2.5078125,
      "grad_norm": 10.304969308476979,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 2247
    },
    {
      "epoch": 2.508928571428571,
      "grad_norm": 11.643837197115019,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 2248
    },
    {
      "epoch": 2.510044642857143,
      "grad_norm": 9.847813395551343,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 2249
    },
    {
      "epoch": 2.5111607142857144,
      "grad_norm": 10.265120396878581,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 2250
    },
    {
      "epoch": 2.5122767857142856,
      "grad_norm": 11.528113104375048,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 2251
    },
    {
      "epoch": 2.513392857142857,
      "grad_norm": 11.08752476938704,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2252
    },
    {
      "epoch": 2.514508928571429,
      "grad_norm": 14.014747412914682,
      "learning_rate": 2e-05,
      "loss": 1.3594,
      "step": 2253
    },
    {
      "epoch": 2.515625,
      "grad_norm": 9.08322801397014,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 2254
    },
    {
      "epoch": 2.516741071428571,
      "grad_norm": 13.667397274358141,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 2255
    },
    {
      "epoch": 2.517857142857143,
      "grad_norm": 10.920176869248733,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 2256
    },
    {
      "epoch": 2.5189732142857144,
      "grad_norm": 13.247021723321126,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 2257
    },
    {
      "epoch": 2.5200892857142856,
      "grad_norm": 12.509560711732583,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2258
    },
    {
      "epoch": 2.521205357142857,
      "grad_norm": 10.966842303306052,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 2259
    },
    {
      "epoch": 2.522321428571429,
      "grad_norm": 11.625299661239643,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 2260
    },
    {
      "epoch": 2.5234375,
      "grad_norm": 9.212448022253534,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2261
    },
    {
      "epoch": 2.524553571428571,
      "grad_norm": 14.091097625833518,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2262
    },
    {
      "epoch": 2.525669642857143,
      "grad_norm": 12.027911734548882,
      "learning_rate": 2e-05,
      "loss": 1.2812,
      "step": 2263
    },
    {
      "epoch": 2.5267857142857144,
      "grad_norm": 13.493197231746485,
      "learning_rate": 2e-05,
      "loss": 1.2422,
      "step": 2264
    },
    {
      "epoch": 2.5279017857142856,
      "grad_norm": 12.90098077861417,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 2265
    },
    {
      "epoch": 2.529017857142857,
      "grad_norm": 11.924188930669663,
      "learning_rate": 2e-05,
      "loss": 1.375,
      "step": 2266
    },
    {
      "epoch": 2.530133928571429,
      "grad_norm": 11.670937357151233,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 2267
    },
    {
      "epoch": 2.53125,
      "grad_norm": 7.289830655087912,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 2268
    },
    {
      "epoch": 2.532366071428571,
      "grad_norm": 10.63508068049154,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 2269
    },
    {
      "epoch": 2.533482142857143,
      "grad_norm": 11.222969516444175,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2270
    },
    {
      "epoch": 2.5345982142857144,
      "grad_norm": 10.384708969961627,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 2271
    },
    {
      "epoch": 2.5357142857142856,
      "grad_norm": 10.49885214358794,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 2272
    },
    {
      "epoch": 2.536830357142857,
      "grad_norm": 12.950507531921783,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 2273
    },
    {
      "epoch": 2.537946428571429,
      "grad_norm": 6.921730926227143,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 2274
    },
    {
      "epoch": 2.5390625,
      "grad_norm": 10.16306554952752,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2275
    },
    {
      "epoch": 2.540178571428571,
      "grad_norm": 8.44743109203839,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2276
    },
    {
      "epoch": 2.541294642857143,
      "grad_norm": 11.96462373633288,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 2277
    },
    {
      "epoch": 2.5424107142857144,
      "grad_norm": 10.33499923289304,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2278
    },
    {
      "epoch": 2.5435267857142856,
      "grad_norm": 10.059409212514971,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 2279
    },
    {
      "epoch": 2.544642857142857,
      "grad_norm": 9.568422446324753,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2280
    },
    {
      "epoch": 2.545758928571429,
      "grad_norm": 10.618642029578092,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2281
    },
    {
      "epoch": 2.546875,
      "grad_norm": 10.64902810020591,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2282
    },
    {
      "epoch": 2.547991071428571,
      "grad_norm": 9.807420635523103,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 2283
    },
    {
      "epoch": 2.549107142857143,
      "grad_norm": 8.507207959347483,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2284
    },
    {
      "epoch": 2.5502232142857144,
      "grad_norm": 9.422575844735318,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2285
    },
    {
      "epoch": 2.5513392857142856,
      "grad_norm": 13.750080661867317,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 2286
    },
    {
      "epoch": 2.552455357142857,
      "grad_norm": 8.884689187846142,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 2287
    },
    {
      "epoch": 2.553571428571429,
      "grad_norm": 12.447131858442937,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 2288
    },
    {
      "epoch": 2.5546875,
      "grad_norm": 12.191781990811368,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 2289
    },
    {
      "epoch": 2.555803571428571,
      "grad_norm": 9.625503883501583,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2290
    },
    {
      "epoch": 2.556919642857143,
      "grad_norm": 8.669921198352712,
      "learning_rate": 2e-05,
      "loss": 3.0938,
      "step": 2291
    },
    {
      "epoch": 2.5580357142857144,
      "grad_norm": 13.427239991335503,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 2292
    },
    {
      "epoch": 2.5591517857142856,
      "grad_norm": 11.038975321230637,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2293
    },
    {
      "epoch": 2.560267857142857,
      "grad_norm": 12.770331777882703,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 2294
    },
    {
      "epoch": 2.561383928571429,
      "grad_norm": 11.989766935703289,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2295
    },
    {
      "epoch": 2.5625,
      "grad_norm": 11.65764509043435,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 2296
    },
    {
      "epoch": 2.563616071428571,
      "grad_norm": 12.528001302099337,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2297
    },
    {
      "epoch": 2.564732142857143,
      "grad_norm": 11.311588302913153,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 2298
    },
    {
      "epoch": 2.5658482142857144,
      "grad_norm": 7.697472089162868,
      "learning_rate": 2e-05,
      "loss": 3.0781,
      "step": 2299
    },
    {
      "epoch": 2.5669642857142856,
      "grad_norm": 15.186754659190498,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 2300
    },
    {
      "epoch": 2.568080357142857,
      "grad_norm": 12.381923394180081,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 2301
    },
    {
      "epoch": 2.569196428571429,
      "grad_norm": 12.413519687509659,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 2302
    },
    {
      "epoch": 2.5703125,
      "grad_norm": 12.836090144775392,
      "learning_rate": 2e-05,
      "loss": 1.3984,
      "step": 2303
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 10.309230473510462,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 2304
    },
    {
      "epoch": 2.572544642857143,
      "grad_norm": 7.965664260272311,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 2305
    },
    {
      "epoch": 2.5736607142857144,
      "grad_norm": 14.547384209071913,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2306
    },
    {
      "epoch": 2.5747767857142856,
      "grad_norm": 10.05466875863855,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2307
    },
    {
      "epoch": 2.575892857142857,
      "grad_norm": 13.947408214225801,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 2308
    },
    {
      "epoch": 2.577008928571429,
      "grad_norm": 12.713627669499513,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 2309
    },
    {
      "epoch": 2.578125,
      "grad_norm": 12.031226972370973,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2310
    },
    {
      "epoch": 2.579241071428571,
      "grad_norm": 12.690745954393325,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 2311
    },
    {
      "epoch": 2.580357142857143,
      "grad_norm": 11.401194031893077,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2312
    },
    {
      "epoch": 2.5814732142857144,
      "grad_norm": 11.983154492945443,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 2313
    },
    {
      "epoch": 2.5825892857142856,
      "grad_norm": 10.379637632837746,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2314
    },
    {
      "epoch": 2.583705357142857,
      "grad_norm": 9.772702410131417,
      "learning_rate": 2e-05,
      "loss": 1.2734,
      "step": 2315
    },
    {
      "epoch": 2.584821428571429,
      "grad_norm": 9.594791108971814,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2316
    },
    {
      "epoch": 2.5859375,
      "grad_norm": 10.597986403872245,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2317
    },
    {
      "epoch": 2.587053571428571,
      "grad_norm": 14.335617844893187,
      "learning_rate": 2e-05,
      "loss": 1.4375,
      "step": 2318
    },
    {
      "epoch": 2.588169642857143,
      "grad_norm": 10.63523575758667,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2319
    },
    {
      "epoch": 2.5892857142857144,
      "grad_norm": 9.357453047356445,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 2320
    },
    {
      "epoch": 2.5904017857142856,
      "grad_norm": 11.33322386529217,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2321
    },
    {
      "epoch": 2.591517857142857,
      "grad_norm": 10.694988628241616,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2322
    },
    {
      "epoch": 2.592633928571429,
      "grad_norm": 13.411992532791322,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2323
    },
    {
      "epoch": 2.59375,
      "grad_norm": 10.466463000786641,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2324
    },
    {
      "epoch": 2.594866071428571,
      "grad_norm": 13.649968595078844,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 2325
    },
    {
      "epoch": 2.595982142857143,
      "grad_norm": 13.198912062036436,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 2326
    },
    {
      "epoch": 2.5970982142857144,
      "grad_norm": 13.265388511172779,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 2327
    },
    {
      "epoch": 2.5982142857142856,
      "grad_norm": 12.268315600780651,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 2328
    },
    {
      "epoch": 2.599330357142857,
      "grad_norm": 10.29536880929928,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 2329
    },
    {
      "epoch": 2.600446428571429,
      "grad_norm": 11.893678981649582,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2330
    },
    {
      "epoch": 2.6015625,
      "grad_norm": 11.537457031333451,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 2331
    },
    {
      "epoch": 2.602678571428571,
      "grad_norm": 9.77838027324557,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 2332
    },
    {
      "epoch": 2.603794642857143,
      "grad_norm": 11.093014276151207,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 2333
    },
    {
      "epoch": 2.6049107142857144,
      "grad_norm": 13.527207933989946,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 2334
    },
    {
      "epoch": 2.6060267857142856,
      "grad_norm": 11.37165318118478,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 2335
    },
    {
      "epoch": 2.607142857142857,
      "grad_norm": 11.103832117926464,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2336
    },
    {
      "epoch": 2.608258928571429,
      "grad_norm": 11.494394191329317,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 2337
    },
    {
      "epoch": 2.609375,
      "grad_norm": 10.240710914918377,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 2338
    },
    {
      "epoch": 2.610491071428571,
      "grad_norm": 10.871881643984027,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 2339
    },
    {
      "epoch": 2.611607142857143,
      "grad_norm": 11.864734887840125,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 2340
    },
    {
      "epoch": 2.6127232142857144,
      "grad_norm": 10.847253180790785,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 2341
    },
    {
      "epoch": 2.6138392857142856,
      "grad_norm": 12.084897990420199,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 2342
    },
    {
      "epoch": 2.614955357142857,
      "grad_norm": 10.668502682454557,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2343
    },
    {
      "epoch": 2.616071428571429,
      "grad_norm": 10.176610974261083,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 2344
    },
    {
      "epoch": 2.6171875,
      "grad_norm": 10.372388087974135,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 2345
    },
    {
      "epoch": 2.618303571428571,
      "grad_norm": 12.143973222478598,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2346
    },
    {
      "epoch": 2.619419642857143,
      "grad_norm": 10.714856565802034,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2347
    },
    {
      "epoch": 2.6205357142857144,
      "grad_norm": 11.431748686586682,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 2348
    },
    {
      "epoch": 2.6216517857142856,
      "grad_norm": 10.554587522925825,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 2349
    },
    {
      "epoch": 2.622767857142857,
      "grad_norm": 12.373875252828306,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 2350
    },
    {
      "epoch": 2.623883928571429,
      "grad_norm": 13.17839271584835,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2351
    },
    {
      "epoch": 2.625,
      "grad_norm": 11.214116092956832,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 2352
    },
    {
      "epoch": 2.626116071428571,
      "grad_norm": 11.168286554356907,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2353
    },
    {
      "epoch": 2.627232142857143,
      "grad_norm": 12.035883572612441,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2354
    },
    {
      "epoch": 2.6283482142857144,
      "grad_norm": 10.009702060782784,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 2355
    },
    {
      "epoch": 2.6294642857142856,
      "grad_norm": 9.163997074978306,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2356
    },
    {
      "epoch": 2.630580357142857,
      "grad_norm": 11.686596280211107,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2357
    },
    {
      "epoch": 2.631696428571429,
      "grad_norm": 10.741654515810932,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2358
    },
    {
      "epoch": 2.6328125,
      "grad_norm": 10.404402213732098,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2359
    },
    {
      "epoch": 2.633928571428571,
      "grad_norm": 10.348227594946039,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 2360
    },
    {
      "epoch": 2.635044642857143,
      "grad_norm": 12.209252188343266,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2361
    },
    {
      "epoch": 2.6361607142857144,
      "grad_norm": 11.72298180890957,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 2362
    },
    {
      "epoch": 2.6372767857142856,
      "grad_norm": 11.23365560515621,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2363
    },
    {
      "epoch": 2.638392857142857,
      "grad_norm": 11.916747479153905,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2364
    },
    {
      "epoch": 2.639508928571429,
      "grad_norm": 12.251274660296987,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 2365
    },
    {
      "epoch": 2.640625,
      "grad_norm": 10.326735515778878,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2366
    },
    {
      "epoch": 2.641741071428571,
      "grad_norm": 9.818019906874788,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 2367
    },
    {
      "epoch": 2.642857142857143,
      "grad_norm": 11.59776071326597,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 2368
    },
    {
      "epoch": 2.6439732142857144,
      "grad_norm": 11.290688107490995,
      "learning_rate": 2e-05,
      "loss": 1.4062,
      "step": 2369
    },
    {
      "epoch": 2.6450892857142856,
      "grad_norm": 11.013606968059939,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 2370
    },
    {
      "epoch": 2.646205357142857,
      "grad_norm": 10.925676261214955,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2371
    },
    {
      "epoch": 2.647321428571429,
      "grad_norm": 11.988813702197465,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 2372
    },
    {
      "epoch": 2.6484375,
      "grad_norm": 13.936099792771396,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 2373
    },
    {
      "epoch": 2.649553571428571,
      "grad_norm": 10.709613898022502,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2374
    },
    {
      "epoch": 2.650669642857143,
      "grad_norm": 11.231013911461702,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2375
    },
    {
      "epoch": 2.6517857142857144,
      "grad_norm": 10.78188982609726,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 2376
    },
    {
      "epoch": 2.6529017857142856,
      "grad_norm": 12.194604027842598,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 2377
    },
    {
      "epoch": 2.654017857142857,
      "grad_norm": 14.008399884002518,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 2378
    },
    {
      "epoch": 2.655133928571429,
      "grad_norm": 9.718589427125362,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 2379
    },
    {
      "epoch": 2.65625,
      "grad_norm": 6.284328215359925,
      "learning_rate": 2e-05,
      "loss": 0.8984,
      "step": 2380
    },
    {
      "epoch": 2.657366071428571,
      "grad_norm": 12.35289024701596,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 2381
    },
    {
      "epoch": 2.658482142857143,
      "grad_norm": 10.808201727601107,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 2382
    },
    {
      "epoch": 2.6595982142857144,
      "grad_norm": 16.779630012734817,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 2383
    },
    {
      "epoch": 2.6607142857142856,
      "grad_norm": 11.632813864274967,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 2384
    },
    {
      "epoch": 2.661830357142857,
      "grad_norm": 9.821745611117377,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 2385
    },
    {
      "epoch": 2.662946428571429,
      "grad_norm": 16.27027514067224,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 2386
    },
    {
      "epoch": 2.6640625,
      "grad_norm": 12.654426267489905,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2387
    },
    {
      "epoch": 2.665178571428571,
      "grad_norm": 12.673767686883355,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 2388
    },
    {
      "epoch": 2.666294642857143,
      "grad_norm": 11.304973541516862,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 2389
    },
    {
      "epoch": 2.6674107142857144,
      "grad_norm": 11.84892813383086,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 2390
    },
    {
      "epoch": 2.6685267857142856,
      "grad_norm": 15.518141458845479,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 2391
    },
    {
      "epoch": 2.669642857142857,
      "grad_norm": 7.984208646921261,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 2392
    },
    {
      "epoch": 2.670758928571429,
      "grad_norm": 13.179810496543688,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2393
    },
    {
      "epoch": 2.671875,
      "grad_norm": 15.125522708475472,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 2394
    },
    {
      "epoch": 2.672991071428571,
      "grad_norm": 12.201635150836575,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2395
    },
    {
      "epoch": 2.674107142857143,
      "grad_norm": 10.35377012837767,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2396
    },
    {
      "epoch": 2.6752232142857144,
      "grad_norm": 10.221185017357508,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 2397
    },
    {
      "epoch": 2.6763392857142856,
      "grad_norm": 15.620674501036229,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 2398
    },
    {
      "epoch": 2.677455357142857,
      "grad_norm": 9.896888507214692,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 2399
    },
    {
      "epoch": 2.678571428571429,
      "grad_norm": 12.280768461928764,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 2400
    },
    {
      "epoch": 2.6796875,
      "grad_norm": 11.370106908921743,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 2401
    },
    {
      "epoch": 2.680803571428571,
      "grad_norm": 12.270714088885033,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2402
    },
    {
      "epoch": 2.681919642857143,
      "grad_norm": 15.498881698865688,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2403
    },
    {
      "epoch": 2.6830357142857144,
      "grad_norm": 14.501745484944264,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2404
    },
    {
      "epoch": 2.6841517857142856,
      "grad_norm": 13.683989342722441,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2405
    },
    {
      "epoch": 2.685267857142857,
      "grad_norm": 12.173383516831864,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 2406
    },
    {
      "epoch": 2.686383928571429,
      "grad_norm": 12.694553208742574,
      "learning_rate": 2e-05,
      "loss": 1.4688,
      "step": 2407
    },
    {
      "epoch": 2.6875,
      "grad_norm": 15.86643268265687,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2408
    },
    {
      "epoch": 2.688616071428571,
      "grad_norm": 8.121724008901214,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2409
    },
    {
      "epoch": 2.689732142857143,
      "grad_norm": 10.026210500473407,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2410
    },
    {
      "epoch": 2.6908482142857144,
      "grad_norm": 12.628542566674177,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2411
    },
    {
      "epoch": 2.6919642857142856,
      "grad_norm": 10.257053647029286,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 2412
    },
    {
      "epoch": 2.693080357142857,
      "grad_norm": 14.065641710882918,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 2413
    },
    {
      "epoch": 2.694196428571429,
      "grad_norm": 13.434955210732431,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2414
    },
    {
      "epoch": 2.6953125,
      "grad_norm": 11.540660841467105,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 2415
    },
    {
      "epoch": 2.696428571428571,
      "grad_norm": 11.25474752384969,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 2416
    },
    {
      "epoch": 2.697544642857143,
      "grad_norm": 10.032914144945655,
      "learning_rate": 2e-05,
      "loss": 1.4219,
      "step": 2417
    },
    {
      "epoch": 2.6986607142857144,
      "grad_norm": 9.774144042234012,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2418
    },
    {
      "epoch": 2.6997767857142856,
      "grad_norm": 11.937397667125873,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2419
    },
    {
      "epoch": 2.700892857142857,
      "grad_norm": 13.201649349076071,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 2420
    },
    {
      "epoch": 2.702008928571429,
      "grad_norm": 13.712047749686919,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2421
    },
    {
      "epoch": 2.703125,
      "grad_norm": 12.642295348795459,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2422
    },
    {
      "epoch": 2.704241071428571,
      "grad_norm": 10.592939825539332,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2423
    },
    {
      "epoch": 2.705357142857143,
      "grad_norm": 13.414714756426154,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2424
    },
    {
      "epoch": 2.7064732142857144,
      "grad_norm": 13.464963373839636,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 2425
    },
    {
      "epoch": 2.7075892857142856,
      "grad_norm": 12.14183910137221,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 2426
    },
    {
      "epoch": 2.708705357142857,
      "grad_norm": 13.844306359357324,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2427
    },
    {
      "epoch": 2.709821428571429,
      "grad_norm": 11.906988050656908,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2428
    },
    {
      "epoch": 2.7109375,
      "grad_norm": 11.192064226916877,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 2429
    },
    {
      "epoch": 2.712053571428571,
      "grad_norm": 11.369342161660166,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2430
    },
    {
      "epoch": 2.713169642857143,
      "grad_norm": 10.404411254044911,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 2431
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 8.712392274426122,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 2432
    },
    {
      "epoch": 2.7154017857142856,
      "grad_norm": 13.258042652380784,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 2433
    },
    {
      "epoch": 2.716517857142857,
      "grad_norm": 10.334954055060555,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2434
    },
    {
      "epoch": 2.717633928571429,
      "grad_norm": 10.112667770924102,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2435
    },
    {
      "epoch": 2.71875,
      "grad_norm": 12.019252409534477,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 2436
    },
    {
      "epoch": 2.719866071428571,
      "grad_norm": 11.498861497258979,
      "learning_rate": 2e-05,
      "loss": 1.3359,
      "step": 2437
    },
    {
      "epoch": 2.720982142857143,
      "grad_norm": 9.903707029629123,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2438
    },
    {
      "epoch": 2.7220982142857144,
      "grad_norm": 9.334398184923165,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2439
    },
    {
      "epoch": 2.7232142857142856,
      "grad_norm": 11.909992219756328,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 2440
    },
    {
      "epoch": 2.724330357142857,
      "grad_norm": 14.456294959792691,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 2441
    },
    {
      "epoch": 2.725446428571429,
      "grad_norm": 12.732783139558345,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 2442
    },
    {
      "epoch": 2.7265625,
      "grad_norm": 8.863079581032673,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 2443
    },
    {
      "epoch": 2.727678571428571,
      "grad_norm": 11.038477750399784,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2444
    },
    {
      "epoch": 2.728794642857143,
      "grad_norm": 9.875526831121102,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2445
    },
    {
      "epoch": 2.7299107142857144,
      "grad_norm": 11.099874684189189,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 2446
    },
    {
      "epoch": 2.7310267857142856,
      "grad_norm": 11.835940552456142,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2447
    },
    {
      "epoch": 2.732142857142857,
      "grad_norm": 10.379179742182371,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 2448
    },
    {
      "epoch": 2.733258928571429,
      "grad_norm": 10.681597508608416,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2449
    },
    {
      "epoch": 2.734375,
      "grad_norm": 9.144434333181655,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2450
    },
    {
      "epoch": 2.735491071428571,
      "grad_norm": 13.772694059185543,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2451
    },
    {
      "epoch": 2.736607142857143,
      "grad_norm": 10.704978728776096,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 2452
    },
    {
      "epoch": 2.7377232142857144,
      "grad_norm": 10.599938918231663,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2453
    },
    {
      "epoch": 2.7388392857142856,
      "grad_norm": 9.188016053928282,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2454
    },
    {
      "epoch": 2.739955357142857,
      "grad_norm": 9.829125523356764,
      "learning_rate": 2e-05,
      "loss": 2.9062,
      "step": 2455
    },
    {
      "epoch": 2.741071428571429,
      "grad_norm": 11.419717584544264,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2456
    },
    {
      "epoch": 2.7421875,
      "grad_norm": 13.281086449677643,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2457
    },
    {
      "epoch": 2.743303571428571,
      "grad_norm": 12.770365502814649,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 2458
    },
    {
      "epoch": 2.744419642857143,
      "grad_norm": 11.407299746802066,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 2459
    },
    {
      "epoch": 2.7455357142857144,
      "grad_norm": 11.51730260950938,
      "learning_rate": 2e-05,
      "loss": 1.3438,
      "step": 2460
    },
    {
      "epoch": 2.7466517857142856,
      "grad_norm": 11.326125006660376,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 2461
    },
    {
      "epoch": 2.747767857142857,
      "grad_norm": 9.04103285465826,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2462
    },
    {
      "epoch": 2.748883928571429,
      "grad_norm": 11.425494108570128,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2463
    },
    {
      "epoch": 2.75,
      "grad_norm": 12.7326860138012,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2464
    },
    {
      "epoch": 2.751116071428571,
      "grad_norm": 12.46602693822499,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2465
    },
    {
      "epoch": 2.752232142857143,
      "grad_norm": 11.274387124204033,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2466
    },
    {
      "epoch": 2.7533482142857144,
      "grad_norm": 10.850586312906499,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2467
    },
    {
      "epoch": 2.7544642857142856,
      "grad_norm": 11.859755845271978,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 2468
    },
    {
      "epoch": 2.755580357142857,
      "grad_norm": 14.325884560987081,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 2469
    },
    {
      "epoch": 2.756696428571429,
      "grad_norm": 12.50093864302384,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 2470
    },
    {
      "epoch": 2.7578125,
      "grad_norm": 14.123381485198255,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2471
    },
    {
      "epoch": 2.758928571428571,
      "grad_norm": 13.01530889574613,
      "learning_rate": 2e-05,
      "loss": 1.3594,
      "step": 2472
    },
    {
      "epoch": 2.760044642857143,
      "grad_norm": 13.59775309382317,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 2473
    },
    {
      "epoch": 2.7611607142857144,
      "grad_norm": 9.790230433712887,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2474
    },
    {
      "epoch": 2.7622767857142856,
      "grad_norm": 11.889653358936943,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 2475
    },
    {
      "epoch": 2.763392857142857,
      "grad_norm": 14.779115222640304,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 2476
    },
    {
      "epoch": 2.764508928571429,
      "grad_norm": 15.094050201618936,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 2477
    },
    {
      "epoch": 2.765625,
      "grad_norm": 12.243053148506416,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2478
    },
    {
      "epoch": 2.766741071428571,
      "grad_norm": 10.38825979290797,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 2479
    },
    {
      "epoch": 2.767857142857143,
      "grad_norm": 10.106273653908177,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2480
    },
    {
      "epoch": 2.7689732142857144,
      "grad_norm": 11.661205447194508,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 2481
    },
    {
      "epoch": 2.7700892857142856,
      "grad_norm": 12.018173851970998,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 2482
    },
    {
      "epoch": 2.771205357142857,
      "grad_norm": 13.3801840583933,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 2483
    },
    {
      "epoch": 2.772321428571429,
      "grad_norm": 11.294450381815373,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 2484
    },
    {
      "epoch": 2.7734375,
      "grad_norm": 8.304130217231807,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 2485
    },
    {
      "epoch": 2.774553571428571,
      "grad_norm": 12.627016002958321,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 2486
    },
    {
      "epoch": 2.775669642857143,
      "grad_norm": 10.567310969608064,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 2487
    },
    {
      "epoch": 2.7767857142857144,
      "grad_norm": 11.436060911354035,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 2488
    },
    {
      "epoch": 2.7779017857142856,
      "grad_norm": 12.295157843603734,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 2489
    },
    {
      "epoch": 2.779017857142857,
      "grad_norm": 9.757285140213954,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2490
    },
    {
      "epoch": 2.780133928571429,
      "grad_norm": 11.587439593206636,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2491
    },
    {
      "epoch": 2.78125,
      "grad_norm": 11.38861735611963,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2492
    },
    {
      "epoch": 2.782366071428571,
      "grad_norm": 12.684474358346865,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 2493
    },
    {
      "epoch": 2.783482142857143,
      "grad_norm": 9.822130215665686,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 2494
    },
    {
      "epoch": 2.7845982142857144,
      "grad_norm": 13.112308633733496,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2495
    },
    {
      "epoch": 2.7857142857142856,
      "grad_norm": 10.52801234576107,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 2496
    },
    {
      "epoch": 2.786830357142857,
      "grad_norm": 15.303132506311522,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2497
    },
    {
      "epoch": 2.787946428571429,
      "grad_norm": 12.421859800592156,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2498
    },
    {
      "epoch": 2.7890625,
      "grad_norm": 9.514046317320457,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 2499
    },
    {
      "epoch": 2.790178571428571,
      "grad_norm": 8.59782181507844,
      "learning_rate": 2e-05,
      "loss": 2.9062,
      "step": 2500
    },
    {
      "epoch": 2.791294642857143,
      "grad_norm": 12.091213568927643,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 2501
    },
    {
      "epoch": 2.7924107142857144,
      "grad_norm": 11.054387161374493,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2502
    },
    {
      "epoch": 2.7935267857142856,
      "grad_norm": 7.911211320610618,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2503
    },
    {
      "epoch": 2.794642857142857,
      "grad_norm": 11.636324546007964,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2504
    },
    {
      "epoch": 2.795758928571429,
      "grad_norm": 11.730574000107236,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 2505
    },
    {
      "epoch": 2.796875,
      "grad_norm": 9.790145952707977,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 2506
    },
    {
      "epoch": 2.797991071428571,
      "grad_norm": 10.359925528539573,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2507
    },
    {
      "epoch": 2.799107142857143,
      "grad_norm": 13.075820235030292,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2508
    },
    {
      "epoch": 2.8002232142857144,
      "grad_norm": 10.51435179859387,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 2509
    },
    {
      "epoch": 2.8013392857142856,
      "grad_norm": 11.31348426701803,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2510
    },
    {
      "epoch": 2.802455357142857,
      "grad_norm": 10.997413422134958,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2511
    },
    {
      "epoch": 2.803571428571429,
      "grad_norm": 10.612940016627624,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 2512
    },
    {
      "epoch": 2.8046875,
      "grad_norm": 11.581240183431634,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 2513
    },
    {
      "epoch": 2.805803571428571,
      "grad_norm": 10.140744426970905,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2514
    },
    {
      "epoch": 2.806919642857143,
      "grad_norm": 11.33801222540245,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2515
    },
    {
      "epoch": 2.8080357142857144,
      "grad_norm": 12.642041517704653,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2516
    },
    {
      "epoch": 2.8091517857142856,
      "grad_norm": 12.870515626545513,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2517
    },
    {
      "epoch": 2.810267857142857,
      "grad_norm": 15.327749595217528,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 2518
    },
    {
      "epoch": 2.811383928571429,
      "grad_norm": 11.144763376498803,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 2519
    },
    {
      "epoch": 2.8125,
      "grad_norm": 10.663895044062313,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2520
    },
    {
      "epoch": 2.813616071428571,
      "grad_norm": 10.094868529554436,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2521
    },
    {
      "epoch": 2.814732142857143,
      "grad_norm": 11.48614278243768,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2522
    },
    {
      "epoch": 2.8158482142857144,
      "grad_norm": 11.02285741330371,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2523
    },
    {
      "epoch": 2.8169642857142856,
      "grad_norm": 10.96903098912444,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 2524
    },
    {
      "epoch": 2.818080357142857,
      "grad_norm": 10.370540445759548,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 2525
    },
    {
      "epoch": 2.819196428571429,
      "grad_norm": 12.71475208053476,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2526
    },
    {
      "epoch": 2.8203125,
      "grad_norm": 11.442060590453668,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2527
    },
    {
      "epoch": 2.821428571428571,
      "grad_norm": 11.31199110047234,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 2528
    },
    {
      "epoch": 2.822544642857143,
      "grad_norm": 9.658190503322412,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 2529
    },
    {
      "epoch": 2.8236607142857144,
      "grad_norm": 9.368507746858459,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 2530
    },
    {
      "epoch": 2.8247767857142856,
      "grad_norm": 16.291960262261007,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2531
    },
    {
      "epoch": 2.825892857142857,
      "grad_norm": 10.519499587362043,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 2532
    },
    {
      "epoch": 2.827008928571429,
      "grad_norm": 12.136638593626845,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2533
    },
    {
      "epoch": 2.828125,
      "grad_norm": 12.58198737761405,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 2534
    },
    {
      "epoch": 2.829241071428571,
      "grad_norm": 11.774702154221918,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 2535
    },
    {
      "epoch": 2.830357142857143,
      "grad_norm": 12.947900447233412,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 2536
    },
    {
      "epoch": 2.8314732142857144,
      "grad_norm": 10.888225183907778,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2537
    },
    {
      "epoch": 2.8325892857142856,
      "grad_norm": 11.085032957647144,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2538
    },
    {
      "epoch": 2.833705357142857,
      "grad_norm": 10.51165506572826,
      "learning_rate": 2e-05,
      "loss": 1.4297,
      "step": 2539
    },
    {
      "epoch": 2.834821428571429,
      "grad_norm": 11.716478864483587,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 2540
    },
    {
      "epoch": 2.8359375,
      "grad_norm": 7.37133151356314,
      "learning_rate": 2e-05,
      "loss": 2.8438,
      "step": 2541
    },
    {
      "epoch": 2.837053571428571,
      "grad_norm": 8.093716895575358,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2542
    },
    {
      "epoch": 2.838169642857143,
      "grad_norm": 13.714501293257316,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 2543
    },
    {
      "epoch": 2.8392857142857144,
      "grad_norm": 12.917772889506438,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 2544
    },
    {
      "epoch": 2.8404017857142856,
      "grad_norm": 11.885067872385816,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 2545
    },
    {
      "epoch": 2.841517857142857,
      "grad_norm": 9.572362742055104,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 2546
    },
    {
      "epoch": 2.842633928571429,
      "grad_norm": 10.641528965748103,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 2547
    },
    {
      "epoch": 2.84375,
      "grad_norm": 8.999586129098367,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 2548
    },
    {
      "epoch": 2.844866071428571,
      "grad_norm": 12.703151483134034,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2549
    },
    {
      "epoch": 2.845982142857143,
      "grad_norm": 10.93511203375718,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 2550
    },
    {
      "epoch": 2.8470982142857144,
      "grad_norm": 12.021860573358325,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 2551
    },
    {
      "epoch": 2.8482142857142856,
      "grad_norm": 13.513628482911196,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 2552
    },
    {
      "epoch": 2.849330357142857,
      "grad_norm": 11.160150466386284,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 2553
    },
    {
      "epoch": 2.850446428571429,
      "grad_norm": 9.521912968433895,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 2554
    },
    {
      "epoch": 2.8515625,
      "grad_norm": 15.19867304461591,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2555
    },
    {
      "epoch": 2.852678571428571,
      "grad_norm": 12.074193174850619,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 2556
    },
    {
      "epoch": 2.853794642857143,
      "grad_norm": 10.0900043154577,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2557
    },
    {
      "epoch": 2.8549107142857144,
      "grad_norm": 13.045951557184342,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 2558
    },
    {
      "epoch": 2.8560267857142856,
      "grad_norm": 11.123187776230827,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2559
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 10.547190008341762,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2560
    },
    {
      "epoch": 2.858258928571429,
      "grad_norm": 13.140638524873092,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2561
    },
    {
      "epoch": 2.859375,
      "grad_norm": 13.449255224863732,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 2562
    },
    {
      "epoch": 2.860491071428571,
      "grad_norm": 10.33643052259335,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 2563
    },
    {
      "epoch": 2.861607142857143,
      "grad_norm": 12.372647398287523,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2564
    },
    {
      "epoch": 2.8627232142857144,
      "grad_norm": 12.962828414133623,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 2565
    },
    {
      "epoch": 2.8638392857142856,
      "grad_norm": 13.324589117746445,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 2566
    },
    {
      "epoch": 2.864955357142857,
      "grad_norm": 13.079716934068845,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2567
    },
    {
      "epoch": 2.866071428571429,
      "grad_norm": 8.523029467592144,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2568
    },
    {
      "epoch": 2.8671875,
      "grad_norm": 11.623389770101147,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 2569
    },
    {
      "epoch": 2.868303571428571,
      "grad_norm": 10.45520144959938,
      "learning_rate": 2e-05,
      "loss": 2.7656,
      "step": 2570
    },
    {
      "epoch": 2.869419642857143,
      "grad_norm": 10.938191004715904,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 2571
    },
    {
      "epoch": 2.8705357142857144,
      "grad_norm": 11.210511231755266,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 2572
    },
    {
      "epoch": 2.8716517857142856,
      "grad_norm": 12.264114216186377,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2573
    },
    {
      "epoch": 2.872767857142857,
      "grad_norm": 10.766478574142361,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2574
    },
    {
      "epoch": 2.873883928571429,
      "grad_norm": 10.48791702600287,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2575
    },
    {
      "epoch": 2.875,
      "grad_norm": 12.425415185479944,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2576
    },
    {
      "epoch": 2.876116071428571,
      "grad_norm": 11.445598253425478,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 2577
    },
    {
      "epoch": 2.877232142857143,
      "grad_norm": 10.372361726311171,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2578
    },
    {
      "epoch": 2.8783482142857144,
      "grad_norm": 12.79240637451107,
      "learning_rate": 2e-05,
      "loss": 1.4453,
      "step": 2579
    },
    {
      "epoch": 2.8794642857142856,
      "grad_norm": 9.715971632566465,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 2580
    },
    {
      "epoch": 2.880580357142857,
      "grad_norm": 10.43175220696101,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 2581
    },
    {
      "epoch": 2.881696428571429,
      "grad_norm": 12.114314945423086,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 2582
    },
    {
      "epoch": 2.8828125,
      "grad_norm": 8.721724901828136,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 2583
    },
    {
      "epoch": 2.883928571428571,
      "grad_norm": 11.027287254774432,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2584
    },
    {
      "epoch": 2.885044642857143,
      "grad_norm": 7.6270371999476785,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 2585
    },
    {
      "epoch": 2.8861607142857144,
      "grad_norm": 10.604714045589509,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 2586
    },
    {
      "epoch": 2.8872767857142856,
      "grad_norm": 8.404066607563308,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 2587
    },
    {
      "epoch": 2.888392857142857,
      "grad_norm": 11.477730760495795,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 2588
    },
    {
      "epoch": 2.889508928571429,
      "grad_norm": 14.557766622359592,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2589
    },
    {
      "epoch": 2.890625,
      "grad_norm": 12.692320061424754,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2590
    },
    {
      "epoch": 2.891741071428571,
      "grad_norm": 12.186236299541518,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 2591
    },
    {
      "epoch": 2.892857142857143,
      "grad_norm": 14.496487093272444,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 2592
    },
    {
      "epoch": 2.8939732142857144,
      "grad_norm": 13.026555597602032,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 2593
    },
    {
      "epoch": 2.8950892857142856,
      "grad_norm": 12.792109585514165,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 2594
    },
    {
      "epoch": 2.896205357142857,
      "grad_norm": 13.408015320724964,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 2595
    },
    {
      "epoch": 2.897321428571429,
      "grad_norm": 11.405481487551945,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 2596
    },
    {
      "epoch": 2.8984375,
      "grad_norm": 11.359643891803472,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2597
    },
    {
      "epoch": 2.899553571428571,
      "grad_norm": 13.110354608388672,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 2598
    },
    {
      "epoch": 2.900669642857143,
      "grad_norm": 11.75054204851056,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2599
    },
    {
      "epoch": 2.9017857142857144,
      "grad_norm": 11.834915613705132,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 2600
    },
    {
      "epoch": 2.9029017857142856,
      "grad_norm": 11.564780147957197,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 2601
    },
    {
      "epoch": 2.904017857142857,
      "grad_norm": 11.87744755920186,
      "learning_rate": 2e-05,
      "loss": 1.4375,
      "step": 2602
    },
    {
      "epoch": 2.905133928571429,
      "grad_norm": 12.535045774607873,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 2603
    },
    {
      "epoch": 2.90625,
      "grad_norm": 11.783838351046965,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2604
    },
    {
      "epoch": 2.907366071428571,
      "grad_norm": 12.890993192988251,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 2605
    },
    {
      "epoch": 2.908482142857143,
      "grad_norm": 13.19767953535759,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 2606
    },
    {
      "epoch": 2.9095982142857144,
      "grad_norm": 10.139333652107613,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 2607
    },
    {
      "epoch": 2.9107142857142856,
      "grad_norm": 13.119879299527991,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 2608
    },
    {
      "epoch": 2.911830357142857,
      "grad_norm": 10.605301878435043,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 2609
    },
    {
      "epoch": 2.912946428571429,
      "grad_norm": 11.168609843016538,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2610
    },
    {
      "epoch": 2.9140625,
      "grad_norm": 10.267806184213677,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 2611
    },
    {
      "epoch": 2.915178571428571,
      "grad_norm": 12.289188797779481,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2612
    },
    {
      "epoch": 2.916294642857143,
      "grad_norm": 13.323420502602822,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2613
    },
    {
      "epoch": 2.9174107142857144,
      "grad_norm": 11.377665423061824,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 2614
    },
    {
      "epoch": 2.9185267857142856,
      "grad_norm": 9.509448959189196,
      "learning_rate": 2e-05,
      "loss": 2.8594,
      "step": 2615
    },
    {
      "epoch": 2.919642857142857,
      "grad_norm": 11.350703991987784,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2616
    },
    {
      "epoch": 2.920758928571429,
      "grad_norm": 12.227787388416646,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2617
    },
    {
      "epoch": 2.921875,
      "grad_norm": 14.890691955194464,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2618
    },
    {
      "epoch": 2.922991071428571,
      "grad_norm": 10.240827464934473,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 2619
    },
    {
      "epoch": 2.924107142857143,
      "grad_norm": 10.269732378360139,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 2620
    },
    {
      "epoch": 2.9252232142857144,
      "grad_norm": 11.719904325692799,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2621
    },
    {
      "epoch": 2.9263392857142856,
      "grad_norm": 11.260843152392681,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2622
    },
    {
      "epoch": 2.927455357142857,
      "grad_norm": 12.963298369856881,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 2623
    },
    {
      "epoch": 2.928571428571429,
      "grad_norm": 14.38048338618028,
      "learning_rate": 2e-05,
      "loss": 1.5,
      "step": 2624
    },
    {
      "epoch": 2.9296875,
      "grad_norm": 14.558907913904306,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 2625
    },
    {
      "epoch": 2.930803571428571,
      "grad_norm": 11.89345485134442,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 2626
    },
    {
      "epoch": 2.931919642857143,
      "grad_norm": 10.601541282841751,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 2627
    },
    {
      "epoch": 2.9330357142857144,
      "grad_norm": 12.158099206116201,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 2628
    },
    {
      "epoch": 2.9341517857142856,
      "grad_norm": 14.195878013146254,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2629
    },
    {
      "epoch": 2.935267857142857,
      "grad_norm": 11.405876196178022,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2630
    },
    {
      "epoch": 2.936383928571429,
      "grad_norm": 11.89067232976298,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 2631
    },
    {
      "epoch": 2.9375,
      "grad_norm": 10.632901862107447,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2632
    },
    {
      "epoch": 2.938616071428571,
      "grad_norm": 9.612110002534923,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 2633
    },
    {
      "epoch": 2.939732142857143,
      "grad_norm": 10.93916344224887,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 2634
    },
    {
      "epoch": 2.9408482142857144,
      "grad_norm": 9.995575185303565,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 2635
    },
    {
      "epoch": 2.9419642857142856,
      "grad_norm": 10.764761161456109,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 2636
    },
    {
      "epoch": 2.943080357142857,
      "grad_norm": 12.830291010071269,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2637
    },
    {
      "epoch": 2.944196428571429,
      "grad_norm": 10.736530769851715,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2638
    },
    {
      "epoch": 2.9453125,
      "grad_norm": 11.314541917485675,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2639
    },
    {
      "epoch": 2.946428571428571,
      "grad_norm": 10.810854048242073,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 2640
    },
    {
      "epoch": 2.947544642857143,
      "grad_norm": 9.873891370579448,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2641
    },
    {
      "epoch": 2.9486607142857144,
      "grad_norm": 12.616739148104836,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 2642
    },
    {
      "epoch": 2.9497767857142856,
      "grad_norm": 8.70221734709263,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2643
    },
    {
      "epoch": 2.950892857142857,
      "grad_norm": 11.611269395516745,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 2644
    },
    {
      "epoch": 2.952008928571429,
      "grad_norm": 13.278977557751377,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2645
    },
    {
      "epoch": 2.953125,
      "grad_norm": 9.815641916465252,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 2646
    },
    {
      "epoch": 2.954241071428571,
      "grad_norm": 12.171976494476919,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 2647
    },
    {
      "epoch": 2.955357142857143,
      "grad_norm": 13.852767518706374,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 2648
    },
    {
      "epoch": 2.9564732142857144,
      "grad_norm": 12.388337421735708,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2649
    },
    {
      "epoch": 2.9575892857142856,
      "grad_norm": 11.017341922795564,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 2650
    },
    {
      "epoch": 2.958705357142857,
      "grad_norm": 11.988398582420308,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 2651
    },
    {
      "epoch": 2.959821428571429,
      "grad_norm": 11.018178507637657,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2652
    },
    {
      "epoch": 2.9609375,
      "grad_norm": 12.966319728637426,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2653
    },
    {
      "epoch": 2.962053571428571,
      "grad_norm": 11.98406967527625,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 2654
    },
    {
      "epoch": 2.963169642857143,
      "grad_norm": 9.972853505378948,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 2655
    },
    {
      "epoch": 2.9642857142857144,
      "grad_norm": 9.340843796456818,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2656
    },
    {
      "epoch": 2.9654017857142856,
      "grad_norm": 11.270057491667949,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 2657
    },
    {
      "epoch": 2.966517857142857,
      "grad_norm": 10.270206174660272,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2658
    },
    {
      "epoch": 2.967633928571429,
      "grad_norm": 8.943262813642994,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 2659
    },
    {
      "epoch": 2.96875,
      "grad_norm": 9.667442597438408,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2660
    },
    {
      "epoch": 2.969866071428571,
      "grad_norm": 12.119724862086205,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2661
    },
    {
      "epoch": 2.970982142857143,
      "grad_norm": 10.21906408450911,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 2662
    },
    {
      "epoch": 2.9720982142857144,
      "grad_norm": 11.597233627296909,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 2663
    },
    {
      "epoch": 2.9732142857142856,
      "grad_norm": 12.955967586189047,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 2664
    },
    {
      "epoch": 2.974330357142857,
      "grad_norm": 11.869522864425107,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2665
    },
    {
      "epoch": 2.975446428571429,
      "grad_norm": 12.306379660360863,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 2666
    },
    {
      "epoch": 2.9765625,
      "grad_norm": 10.28394768030496,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 2667
    },
    {
      "epoch": 2.977678571428571,
      "grad_norm": 12.445402785333547,
      "learning_rate": 2e-05,
      "loss": 1.2969,
      "step": 2668
    },
    {
      "epoch": 2.978794642857143,
      "grad_norm": 9.983587968460093,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 2669
    },
    {
      "epoch": 2.9799107142857144,
      "grad_norm": 13.435937539486531,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 2670
    },
    {
      "epoch": 2.9810267857142856,
      "grad_norm": 12.483896112053495,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2671
    },
    {
      "epoch": 2.982142857142857,
      "grad_norm": 13.710935738629798,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 2672
    },
    {
      "epoch": 2.983258928571429,
      "grad_norm": 10.892197915106157,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 2673
    },
    {
      "epoch": 2.984375,
      "grad_norm": 12.495194569185164,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 2674
    },
    {
      "epoch": 2.985491071428571,
      "grad_norm": 12.730013745319383,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2675
    },
    {
      "epoch": 2.986607142857143,
      "grad_norm": 12.879344447904455,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2676
    },
    {
      "epoch": 2.9877232142857144,
      "grad_norm": 12.239357456145703,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2677
    },
    {
      "epoch": 2.9888392857142856,
      "grad_norm": 13.317983830703852,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 2678
    },
    {
      "epoch": 2.989955357142857,
      "grad_norm": 16.796070330495578,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2679
    },
    {
      "epoch": 2.991071428571429,
      "grad_norm": 13.530913662647453,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2680
    },
    {
      "epoch": 2.9921875,
      "grad_norm": 10.675769044184108,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 2681
    },
    {
      "epoch": 2.993303571428571,
      "grad_norm": 11.236572433545453,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 2682
    },
    {
      "epoch": 2.994419642857143,
      "grad_norm": 10.653598860522765,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2683
    },
    {
      "epoch": 2.9955357142857144,
      "grad_norm": 12.923560181622747,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2684
    },
    {
      "epoch": 2.9966517857142856,
      "grad_norm": 11.29317003291239,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 2685
    },
    {
      "epoch": 2.997767857142857,
      "grad_norm": 13.372596722485245,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2686
    },
    {
      "epoch": 2.998883928571429,
      "grad_norm": 12.031044099482362,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 2687
    },
    {
      "epoch": 3.0,
      "grad_norm": 11.113714140007806,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2688
    },
    {
      "epoch": 3.0011160714285716,
      "grad_norm": 11.122573864336047,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 2689
    },
    {
      "epoch": 3.002232142857143,
      "grad_norm": 12.572298877804005,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 2690
    },
    {
      "epoch": 3.0033482142857144,
      "grad_norm": 11.475395639940968,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 2691
    },
    {
      "epoch": 3.0044642857142856,
      "grad_norm": 11.10892725739823,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 2692
    },
    {
      "epoch": 3.005580357142857,
      "grad_norm": 13.069431975929533,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2693
    },
    {
      "epoch": 3.0066964285714284,
      "grad_norm": 14.64914940231879,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2694
    },
    {
      "epoch": 3.0078125,
      "grad_norm": 10.708772944575152,
      "learning_rate": 2e-05,
      "loss": 1.4375,
      "step": 2695
    },
    {
      "epoch": 3.0089285714285716,
      "grad_norm": 12.50423446885205,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2696
    },
    {
      "epoch": 3.010044642857143,
      "grad_norm": 12.76707927113273,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 2697
    },
    {
      "epoch": 3.0111607142857144,
      "grad_norm": 9.607822787625581,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2698
    },
    {
      "epoch": 3.0122767857142856,
      "grad_norm": 11.99405156910744,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 2699
    },
    {
      "epoch": 3.013392857142857,
      "grad_norm": 13.99775695306302,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2700
    },
    {
      "epoch": 3.0145089285714284,
      "grad_norm": 10.238513345500644,
      "learning_rate": 2e-05,
      "loss": 1.4141,
      "step": 2701
    },
    {
      "epoch": 3.015625,
      "grad_norm": 13.426462722419798,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 2702
    },
    {
      "epoch": 3.0167410714285716,
      "grad_norm": 9.233283822958883,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2703
    },
    {
      "epoch": 3.017857142857143,
      "grad_norm": 15.042049591864233,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2704
    },
    {
      "epoch": 3.0189732142857144,
      "grad_norm": 12.513931517732352,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 2705
    },
    {
      "epoch": 3.0200892857142856,
      "grad_norm": 11.686640309870414,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2706
    },
    {
      "epoch": 3.021205357142857,
      "grad_norm": 11.340728066789598,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 2707
    },
    {
      "epoch": 3.0223214285714284,
      "grad_norm": 11.760284342896735,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 2708
    },
    {
      "epoch": 3.0234375,
      "grad_norm": 11.741135586979253,
      "learning_rate": 2e-05,
      "loss": 2.8281,
      "step": 2709
    },
    {
      "epoch": 3.0245535714285716,
      "grad_norm": 9.943372810241911,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2710
    },
    {
      "epoch": 3.025669642857143,
      "grad_norm": 11.201040204305878,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2711
    },
    {
      "epoch": 3.0267857142857144,
      "grad_norm": 13.738192771783641,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 2712
    },
    {
      "epoch": 3.0279017857142856,
      "grad_norm": 8.022923019202388,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 2713
    },
    {
      "epoch": 3.029017857142857,
      "grad_norm": 12.992044341112212,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2714
    },
    {
      "epoch": 3.0301339285714284,
      "grad_norm": 12.507331980845768,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 2715
    },
    {
      "epoch": 3.03125,
      "grad_norm": 11.992618287794356,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2716
    },
    {
      "epoch": 3.0323660714285716,
      "grad_norm": 12.7645960486956,
      "learning_rate": 2e-05,
      "loss": 1.3828,
      "step": 2717
    },
    {
      "epoch": 3.033482142857143,
      "grad_norm": 11.609585873877435,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2718
    },
    {
      "epoch": 3.0345982142857144,
      "grad_norm": 11.933556439286967,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 2719
    },
    {
      "epoch": 3.0357142857142856,
      "grad_norm": 13.325932282191095,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 2720
    },
    {
      "epoch": 3.036830357142857,
      "grad_norm": 12.647953563694928,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2721
    },
    {
      "epoch": 3.0379464285714284,
      "grad_norm": 13.559201726010272,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2722
    },
    {
      "epoch": 3.0390625,
      "grad_norm": 10.124625159542402,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2723
    },
    {
      "epoch": 3.0401785714285716,
      "grad_norm": 9.268681338998405,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 2724
    },
    {
      "epoch": 3.041294642857143,
      "grad_norm": 12.23730380380514,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 2725
    },
    {
      "epoch": 3.0424107142857144,
      "grad_norm": 12.398724801469845,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2726
    },
    {
      "epoch": 3.0435267857142856,
      "grad_norm": 11.914684020044309,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2727
    },
    {
      "epoch": 3.044642857142857,
      "grad_norm": 14.158008937455225,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 2728
    },
    {
      "epoch": 3.0457589285714284,
      "grad_norm": 12.985523612238499,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 2729
    },
    {
      "epoch": 3.046875,
      "grad_norm": 11.63116953607058,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2730
    },
    {
      "epoch": 3.0479910714285716,
      "grad_norm": 13.970929427248759,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 2731
    },
    {
      "epoch": 3.049107142857143,
      "grad_norm": 9.273182237890564,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 2732
    },
    {
      "epoch": 3.0502232142857144,
      "grad_norm": 12.51225779729446,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2733
    },
    {
      "epoch": 3.0513392857142856,
      "grad_norm": 15.004295933657696,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 2734
    },
    {
      "epoch": 3.052455357142857,
      "grad_norm": 12.06657776206386,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 2735
    },
    {
      "epoch": 3.0535714285714284,
      "grad_norm": 13.061754774573478,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 2736
    },
    {
      "epoch": 3.0546875,
      "grad_norm": 13.421194722317129,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 2737
    },
    {
      "epoch": 3.0558035714285716,
      "grad_norm": 14.92689501675607,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2738
    },
    {
      "epoch": 3.056919642857143,
      "grad_norm": 10.18561277465442,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 2739
    },
    {
      "epoch": 3.0580357142857144,
      "grad_norm": 11.085511730024193,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2740
    },
    {
      "epoch": 3.0591517857142856,
      "grad_norm": 12.74794355395808,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 2741
    },
    {
      "epoch": 3.060267857142857,
      "grad_norm": 12.41024306866308,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 2742
    },
    {
      "epoch": 3.0613839285714284,
      "grad_norm": 15.123456852612344,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 2743
    },
    {
      "epoch": 3.0625,
      "grad_norm": 11.415699718752752,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 2744
    },
    {
      "epoch": 3.0636160714285716,
      "grad_norm": 11.731560935309666,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 2745
    },
    {
      "epoch": 3.064732142857143,
      "grad_norm": 11.25878554795752,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2746
    },
    {
      "epoch": 3.0658482142857144,
      "grad_norm": 14.0740492616621,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 2747
    },
    {
      "epoch": 3.0669642857142856,
      "grad_norm": 12.271068335119816,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 2748
    },
    {
      "epoch": 3.068080357142857,
      "grad_norm": 10.925959043193103,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2749
    },
    {
      "epoch": 3.0691964285714284,
      "grad_norm": 9.281849622786863,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 2750
    },
    {
      "epoch": 3.0703125,
      "grad_norm": 12.177184356006077,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 2751
    },
    {
      "epoch": 3.0714285714285716,
      "grad_norm": 15.843746142698933,
      "learning_rate": 2e-05,
      "loss": 1.3047,
      "step": 2752
    },
    {
      "epoch": 3.072544642857143,
      "grad_norm": 11.862199485271205,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2753
    },
    {
      "epoch": 3.0736607142857144,
      "grad_norm": 9.056895830783487,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2754
    },
    {
      "epoch": 3.0747767857142856,
      "grad_norm": 10.584662863079638,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2755
    },
    {
      "epoch": 3.075892857142857,
      "grad_norm": 13.446011873535902,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 2756
    },
    {
      "epoch": 3.0770089285714284,
      "grad_norm": 11.829052707637231,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 2757
    },
    {
      "epoch": 3.078125,
      "grad_norm": 12.369483225280481,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 2758
    },
    {
      "epoch": 3.0792410714285716,
      "grad_norm": 14.168252055948663,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2759
    },
    {
      "epoch": 3.080357142857143,
      "grad_norm": 11.756178035813704,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2760
    },
    {
      "epoch": 3.0814732142857144,
      "grad_norm": 15.290584053685356,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2761
    },
    {
      "epoch": 3.0825892857142856,
      "grad_norm": 12.714472021550248,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 2762
    },
    {
      "epoch": 3.083705357142857,
      "grad_norm": 10.879532559916402,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 2763
    },
    {
      "epoch": 3.0848214285714284,
      "grad_norm": 14.965079321741072,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 2764
    },
    {
      "epoch": 3.0859375,
      "grad_norm": 12.83617725519695,
      "learning_rate": 2e-05,
      "loss": 1.3906,
      "step": 2765
    },
    {
      "epoch": 3.0870535714285716,
      "grad_norm": 14.421187830235388,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 2766
    },
    {
      "epoch": 3.088169642857143,
      "grad_norm": 10.289196467186017,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2767
    },
    {
      "epoch": 3.0892857142857144,
      "grad_norm": 12.271202010615024,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2768
    },
    {
      "epoch": 3.0904017857142856,
      "grad_norm": 11.281960772912436,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 2769
    },
    {
      "epoch": 3.091517857142857,
      "grad_norm": 10.425235719963894,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2770
    },
    {
      "epoch": 3.0926339285714284,
      "grad_norm": 11.46029082598209,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2771
    },
    {
      "epoch": 3.09375,
      "grad_norm": 12.777541013773124,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2772
    },
    {
      "epoch": 3.0948660714285716,
      "grad_norm": 13.111853772395483,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 2773
    },
    {
      "epoch": 3.095982142857143,
      "grad_norm": 14.588961794364367,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 2774
    },
    {
      "epoch": 3.0970982142857144,
      "grad_norm": 14.259133596844544,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2775
    },
    {
      "epoch": 3.0982142857142856,
      "grad_norm": 11.89505742549998,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 2776
    },
    {
      "epoch": 3.099330357142857,
      "grad_norm": 12.665311989046556,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2777
    },
    {
      "epoch": 3.1004464285714284,
      "grad_norm": 13.073937883571682,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2778
    },
    {
      "epoch": 3.1015625,
      "grad_norm": 10.201939248151223,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2779
    },
    {
      "epoch": 3.1026785714285716,
      "grad_norm": 13.525990478102578,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2780
    },
    {
      "epoch": 3.103794642857143,
      "grad_norm": 11.72434161894948,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 2781
    },
    {
      "epoch": 3.1049107142857144,
      "grad_norm": 13.356789009029807,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 2782
    },
    {
      "epoch": 3.1060267857142856,
      "grad_norm": 10.15945947959572,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 2783
    },
    {
      "epoch": 3.107142857142857,
      "grad_norm": 12.513962404526502,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 2784
    },
    {
      "epoch": 3.1082589285714284,
      "grad_norm": 13.800475132952874,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2785
    },
    {
      "epoch": 3.109375,
      "grad_norm": 14.385867771852974,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 2786
    },
    {
      "epoch": 3.1104910714285716,
      "grad_norm": 16.264590279762054,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 2787
    },
    {
      "epoch": 3.111607142857143,
      "grad_norm": 10.688226809551379,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2788
    },
    {
      "epoch": 3.1127232142857144,
      "grad_norm": 12.972869201146443,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2789
    },
    {
      "epoch": 3.1138392857142856,
      "grad_norm": 10.924321283066208,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2790
    },
    {
      "epoch": 3.114955357142857,
      "grad_norm": 16.122514495272434,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2791
    },
    {
      "epoch": 3.1160714285714284,
      "grad_norm": 13.189953962065378,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 2792
    },
    {
      "epoch": 3.1171875,
      "grad_norm": 10.462542848055339,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2793
    },
    {
      "epoch": 3.1183035714285716,
      "grad_norm": 8.681816633989262,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 2794
    },
    {
      "epoch": 3.119419642857143,
      "grad_norm": 11.09236659444967,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 2795
    },
    {
      "epoch": 3.1205357142857144,
      "grad_norm": 10.991733291536267,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2796
    },
    {
      "epoch": 3.1216517857142856,
      "grad_norm": 10.850505724418099,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2797
    },
    {
      "epoch": 3.122767857142857,
      "grad_norm": 12.467681441321412,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 2798
    },
    {
      "epoch": 3.1238839285714284,
      "grad_norm": 14.403104979009814,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 2799
    },
    {
      "epoch": 3.125,
      "grad_norm": 8.068221635649683,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 2800
    },
    {
      "epoch": 3.1261160714285716,
      "grad_norm": 10.693597743741611,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2801
    },
    {
      "epoch": 3.127232142857143,
      "grad_norm": 11.989582776954919,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2802
    },
    {
      "epoch": 3.1283482142857144,
      "grad_norm": 13.125691010495059,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 2803
    },
    {
      "epoch": 3.1294642857142856,
      "grad_norm": 15.193439799050868,
      "learning_rate": 2e-05,
      "loss": 1.3047,
      "step": 2804
    },
    {
      "epoch": 3.130580357142857,
      "grad_norm": 12.333386550521306,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 2805
    },
    {
      "epoch": 3.1316964285714284,
      "grad_norm": 13.444802348474134,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 2806
    },
    {
      "epoch": 3.1328125,
      "grad_norm": 12.35412954105474,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 2807
    },
    {
      "epoch": 3.1339285714285716,
      "grad_norm": 13.682913193612457,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 2808
    },
    {
      "epoch": 3.135044642857143,
      "grad_norm": 10.722721918589059,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2809
    },
    {
      "epoch": 3.1361607142857144,
      "grad_norm": 15.928939416143354,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 2810
    },
    {
      "epoch": 3.1372767857142856,
      "grad_norm": 12.559974984449395,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 2811
    },
    {
      "epoch": 3.138392857142857,
      "grad_norm": 14.488437144677683,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 2812
    },
    {
      "epoch": 3.1395089285714284,
      "grad_norm": 13.497702730463127,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 2813
    },
    {
      "epoch": 3.140625,
      "grad_norm": 11.337572931083177,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2814
    },
    {
      "epoch": 3.1417410714285716,
      "grad_norm": 11.262527111855261,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 2815
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 15.326043793081636,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 2816
    },
    {
      "epoch": 3.1439732142857144,
      "grad_norm": 12.615249982171278,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 2817
    },
    {
      "epoch": 3.1450892857142856,
      "grad_norm": 11.920981789496603,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 2818
    },
    {
      "epoch": 3.146205357142857,
      "grad_norm": 15.24013729502746,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 2819
    },
    {
      "epoch": 3.1473214285714284,
      "grad_norm": 12.575972816669253,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 2820
    },
    {
      "epoch": 3.1484375,
      "grad_norm": 15.280216251648586,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 2821
    },
    {
      "epoch": 3.1495535714285716,
      "grad_norm": 13.596170250046303,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 2822
    },
    {
      "epoch": 3.150669642857143,
      "grad_norm": 12.74479835220221,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 2823
    },
    {
      "epoch": 3.1517857142857144,
      "grad_norm": 11.986250087091966,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2824
    },
    {
      "epoch": 3.1529017857142856,
      "grad_norm": 10.415674208070994,
      "learning_rate": 2e-05,
      "loss": 2.8438,
      "step": 2825
    },
    {
      "epoch": 3.154017857142857,
      "grad_norm": 13.935930608619763,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 2826
    },
    {
      "epoch": 3.1551339285714284,
      "grad_norm": 13.919193343167509,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 2827
    },
    {
      "epoch": 3.15625,
      "grad_norm": 11.308996395017962,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2828
    },
    {
      "epoch": 3.1573660714285716,
      "grad_norm": 9.997951829285778,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2829
    },
    {
      "epoch": 3.158482142857143,
      "grad_norm": 12.793043465650626,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 2830
    },
    {
      "epoch": 3.1595982142857144,
      "grad_norm": 12.523672921044488,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 2831
    },
    {
      "epoch": 3.1607142857142856,
      "grad_norm": 12.85006005544131,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2832
    },
    {
      "epoch": 3.161830357142857,
      "grad_norm": 11.83939569606821,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2833
    },
    {
      "epoch": 3.1629464285714284,
      "grad_norm": 13.566317736874614,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 2834
    },
    {
      "epoch": 3.1640625,
      "grad_norm": 13.19989506524841,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2835
    },
    {
      "epoch": 3.1651785714285716,
      "grad_norm": 13.211463532612754,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 2836
    },
    {
      "epoch": 3.166294642857143,
      "grad_norm": 10.517495297752975,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 2837
    },
    {
      "epoch": 3.1674107142857144,
      "grad_norm": 11.236562869267326,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 2838
    },
    {
      "epoch": 3.1685267857142856,
      "grad_norm": 8.243516714344269,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2839
    },
    {
      "epoch": 3.169642857142857,
      "grad_norm": 14.131730286472893,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 2840
    },
    {
      "epoch": 3.1707589285714284,
      "grad_norm": 11.780577300354958,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2841
    },
    {
      "epoch": 3.171875,
      "grad_norm": 18.212926390525794,
      "learning_rate": 2e-05,
      "loss": 1.3438,
      "step": 2842
    },
    {
      "epoch": 3.1729910714285716,
      "grad_norm": 11.869721970277974,
      "learning_rate": 2e-05,
      "loss": 1.4219,
      "step": 2843
    },
    {
      "epoch": 3.174107142857143,
      "grad_norm": 12.605058686077248,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2844
    },
    {
      "epoch": 3.1752232142857144,
      "grad_norm": 11.70906067583254,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2845
    },
    {
      "epoch": 3.1763392857142856,
      "grad_norm": 13.062661889652757,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2846
    },
    {
      "epoch": 3.177455357142857,
      "grad_norm": 12.550447294487029,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 2847
    },
    {
      "epoch": 3.1785714285714284,
      "grad_norm": 11.222398848313512,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 2848
    },
    {
      "epoch": 3.1796875,
      "grad_norm": 11.948884927148642,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2849
    },
    {
      "epoch": 3.1808035714285716,
      "grad_norm": 11.484118059482492,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2850
    },
    {
      "epoch": 3.181919642857143,
      "grad_norm": 10.790668098290523,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 2851
    },
    {
      "epoch": 3.1830357142857144,
      "grad_norm": 13.827199464288189,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 2852
    },
    {
      "epoch": 3.1841517857142856,
      "grad_norm": 12.464119573012846,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2853
    },
    {
      "epoch": 3.185267857142857,
      "grad_norm": 14.139843708925293,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 2854
    },
    {
      "epoch": 3.1863839285714284,
      "grad_norm": 12.47923100096509,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 2855
    },
    {
      "epoch": 3.1875,
      "grad_norm": 13.063588962492615,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2856
    },
    {
      "epoch": 3.1886160714285716,
      "grad_norm": 12.250282108210536,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 2857
    },
    {
      "epoch": 3.189732142857143,
      "grad_norm": 12.892047428615758,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 2858
    },
    {
      "epoch": 3.1908482142857144,
      "grad_norm": 11.976900845838735,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 2859
    },
    {
      "epoch": 3.1919642857142856,
      "grad_norm": 12.257027594174547,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2860
    },
    {
      "epoch": 3.193080357142857,
      "grad_norm": 13.175010516379157,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 2861
    },
    {
      "epoch": 3.1941964285714284,
      "grad_norm": 12.601443740113067,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 2862
    },
    {
      "epoch": 3.1953125,
      "grad_norm": 13.196281224477422,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 2863
    },
    {
      "epoch": 3.1964285714285716,
      "grad_norm": 13.412608113307192,
      "learning_rate": 2e-05,
      "loss": 1.4688,
      "step": 2864
    },
    {
      "epoch": 3.197544642857143,
      "grad_norm": 12.93972268576847,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 2865
    },
    {
      "epoch": 3.1986607142857144,
      "grad_norm": 13.956324516931623,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 2866
    },
    {
      "epoch": 3.1997767857142856,
      "grad_norm": 12.272301469702622,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2867
    },
    {
      "epoch": 3.200892857142857,
      "grad_norm": 13.758879220530773,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2868
    },
    {
      "epoch": 3.2020089285714284,
      "grad_norm": 7.5793747507804685,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 2869
    },
    {
      "epoch": 3.203125,
      "grad_norm": 11.251454148609657,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 2870
    },
    {
      "epoch": 3.2042410714285716,
      "grad_norm": 14.332515112614082,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 2871
    },
    {
      "epoch": 3.205357142857143,
      "grad_norm": 12.728079778133617,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2872
    },
    {
      "epoch": 3.2064732142857144,
      "grad_norm": 10.632083694192358,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2873
    },
    {
      "epoch": 3.2075892857142856,
      "grad_norm": 16.352310436216317,
      "learning_rate": 2e-05,
      "loss": 1.5,
      "step": 2874
    },
    {
      "epoch": 3.208705357142857,
      "grad_norm": 13.237683507857351,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 2875
    },
    {
      "epoch": 3.2098214285714284,
      "grad_norm": 12.66521304457317,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2876
    },
    {
      "epoch": 3.2109375,
      "grad_norm": 11.683727697495906,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 2877
    },
    {
      "epoch": 3.2120535714285716,
      "grad_norm": 12.576568092318205,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 2878
    },
    {
      "epoch": 3.213169642857143,
      "grad_norm": 14.280457127547916,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 2879
    },
    {
      "epoch": 3.2142857142857144,
      "grad_norm": 14.683393991844259,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 2880
    },
    {
      "epoch": 3.2154017857142856,
      "grad_norm": 12.330426442544686,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 2881
    },
    {
      "epoch": 3.216517857142857,
      "grad_norm": 12.209519484406966,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2882
    },
    {
      "epoch": 3.2176339285714284,
      "grad_norm": 13.090097482927618,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2883
    },
    {
      "epoch": 3.21875,
      "grad_norm": 13.384485974933197,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 2884
    },
    {
      "epoch": 3.2198660714285716,
      "grad_norm": 14.043336915334116,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 2885
    },
    {
      "epoch": 3.220982142857143,
      "grad_norm": 14.05619675573983,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2886
    },
    {
      "epoch": 3.2220982142857144,
      "grad_norm": 10.35796938296214,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 2887
    },
    {
      "epoch": 3.2232142857142856,
      "grad_norm": 10.497919641967417,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2888
    },
    {
      "epoch": 3.224330357142857,
      "grad_norm": 12.564123464004362,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 2889
    },
    {
      "epoch": 3.2254464285714284,
      "grad_norm": 12.619903534977766,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 2890
    },
    {
      "epoch": 3.2265625,
      "grad_norm": 11.798218716506339,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2891
    },
    {
      "epoch": 3.2276785714285716,
      "grad_norm": 11.385885791472734,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 2892
    },
    {
      "epoch": 3.228794642857143,
      "grad_norm": 12.893884765174388,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 2893
    },
    {
      "epoch": 3.2299107142857144,
      "grad_norm": 12.385863097879342,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2894
    },
    {
      "epoch": 3.2310267857142856,
      "grad_norm": 14.017934127634346,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 2895
    },
    {
      "epoch": 3.232142857142857,
      "grad_norm": 11.891020609793426,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 2896
    },
    {
      "epoch": 3.2332589285714284,
      "grad_norm": 11.685596404424528,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2897
    },
    {
      "epoch": 3.234375,
      "grad_norm": 11.483131631219589,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 2898
    },
    {
      "epoch": 3.2354910714285716,
      "grad_norm": 12.742623700347032,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2899
    },
    {
      "epoch": 3.236607142857143,
      "grad_norm": 14.299314220209792,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2900
    },
    {
      "epoch": 3.2377232142857144,
      "grad_norm": 9.607742157352076,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 2901
    },
    {
      "epoch": 3.2388392857142856,
      "grad_norm": 12.036938175753454,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 2902
    },
    {
      "epoch": 3.239955357142857,
      "grad_norm": 11.174383767093106,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 2903
    },
    {
      "epoch": 3.2410714285714284,
      "grad_norm": 12.021027632532478,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2904
    },
    {
      "epoch": 3.2421875,
      "grad_norm": 13.568452694203476,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 2905
    },
    {
      "epoch": 3.2433035714285716,
      "grad_norm": 11.528735601456921,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 2906
    },
    {
      "epoch": 3.244419642857143,
      "grad_norm": 11.73420277994355,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 2907
    },
    {
      "epoch": 3.2455357142857144,
      "grad_norm": 13.098493590841667,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 2908
    },
    {
      "epoch": 3.2466517857142856,
      "grad_norm": 11.117810599726917,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 2909
    },
    {
      "epoch": 3.247767857142857,
      "grad_norm": 12.933247349189292,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 2910
    },
    {
      "epoch": 3.2488839285714284,
      "grad_norm": 17.036518291684555,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 2911
    },
    {
      "epoch": 3.25,
      "grad_norm": 13.42497604275232,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2912
    },
    {
      "epoch": 3.2511160714285716,
      "grad_norm": 13.238801990497894,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 2913
    },
    {
      "epoch": 3.252232142857143,
      "grad_norm": 12.341179460824547,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 2914
    },
    {
      "epoch": 3.2533482142857144,
      "grad_norm": 13.203271706422223,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2915
    },
    {
      "epoch": 3.2544642857142856,
      "grad_norm": 12.95366263962252,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 2916
    },
    {
      "epoch": 3.255580357142857,
      "grad_norm": 15.042188441593739,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2917
    },
    {
      "epoch": 3.2566964285714284,
      "grad_norm": 11.400055098612036,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 2918
    },
    {
      "epoch": 3.2578125,
      "grad_norm": 10.384012975362145,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 2919
    },
    {
      "epoch": 3.2589285714285716,
      "grad_norm": 12.865653384398708,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 2920
    },
    {
      "epoch": 3.260044642857143,
      "grad_norm": 13.862885037344949,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 2921
    },
    {
      "epoch": 3.2611607142857144,
      "grad_norm": 13.95022327658341,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 2922
    },
    {
      "epoch": 3.2622767857142856,
      "grad_norm": 15.671220344396945,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 2923
    },
    {
      "epoch": 3.263392857142857,
      "grad_norm": 12.367810549435214,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 2924
    },
    {
      "epoch": 3.2645089285714284,
      "grad_norm": 14.06402937367798,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 2925
    },
    {
      "epoch": 3.265625,
      "grad_norm": 11.905175175825104,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2926
    },
    {
      "epoch": 3.2667410714285716,
      "grad_norm": 10.56715113617088,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 2927
    },
    {
      "epoch": 3.267857142857143,
      "grad_norm": 13.075209282412613,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 2928
    },
    {
      "epoch": 3.2689732142857144,
      "grad_norm": 11.838682026679088,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 2929
    },
    {
      "epoch": 3.2700892857142856,
      "grad_norm": 13.038603133908904,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 2930
    },
    {
      "epoch": 3.271205357142857,
      "grad_norm": 12.309225719898803,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 2931
    },
    {
      "epoch": 3.2723214285714284,
      "grad_norm": 13.142183631265196,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 2932
    },
    {
      "epoch": 3.2734375,
      "grad_norm": 9.721258561806609,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2933
    },
    {
      "epoch": 3.2745535714285716,
      "grad_norm": 13.272157588610806,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 2934
    },
    {
      "epoch": 3.275669642857143,
      "grad_norm": 12.790828711296411,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 2935
    },
    {
      "epoch": 3.2767857142857144,
      "grad_norm": 10.826096085591555,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2936
    },
    {
      "epoch": 3.2779017857142856,
      "grad_norm": 11.968052477306808,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 2937
    },
    {
      "epoch": 3.279017857142857,
      "grad_norm": 14.221294357450654,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 2938
    },
    {
      "epoch": 3.2801339285714284,
      "grad_norm": 12.440219014137169,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 2939
    },
    {
      "epoch": 3.28125,
      "grad_norm": 13.541266269180129,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2940
    },
    {
      "epoch": 3.2823660714285716,
      "grad_norm": 12.150535088956754,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 2941
    },
    {
      "epoch": 3.283482142857143,
      "grad_norm": 14.79985828923678,
      "learning_rate": 2e-05,
      "loss": 1.4375,
      "step": 2942
    },
    {
      "epoch": 3.2845982142857144,
      "grad_norm": 12.55826677028072,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 2943
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 11.793841973558497,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 2944
    },
    {
      "epoch": 3.286830357142857,
      "grad_norm": 11.611850531738483,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 2945
    },
    {
      "epoch": 3.2879464285714284,
      "grad_norm": 13.707162539290668,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 2946
    },
    {
      "epoch": 3.2890625,
      "grad_norm": 15.0958670356713,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2947
    },
    {
      "epoch": 3.2901785714285716,
      "grad_norm": 13.519697524865078,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 2948
    },
    {
      "epoch": 3.291294642857143,
      "grad_norm": 11.931786712948917,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2949
    },
    {
      "epoch": 3.2924107142857144,
      "grad_norm": 13.328677568472202,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 2950
    },
    {
      "epoch": 3.2935267857142856,
      "grad_norm": 11.411038606759808,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 2951
    },
    {
      "epoch": 3.294642857142857,
      "grad_norm": 13.83637058734307,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 2952
    },
    {
      "epoch": 3.2957589285714284,
      "grad_norm": 12.68086168475639,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2953
    },
    {
      "epoch": 3.296875,
      "grad_norm": 13.502522136787528,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 2954
    },
    {
      "epoch": 3.2979910714285716,
      "grad_norm": 13.089100332735905,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 2955
    },
    {
      "epoch": 3.299107142857143,
      "grad_norm": 11.771204459785531,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 2956
    },
    {
      "epoch": 3.3002232142857144,
      "grad_norm": 12.719916280821014,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2957
    },
    {
      "epoch": 3.3013392857142856,
      "grad_norm": 12.345233502185513,
      "learning_rate": 2e-05,
      "loss": 2.7656,
      "step": 2958
    },
    {
      "epoch": 3.302455357142857,
      "grad_norm": 12.78522935306837,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2959
    },
    {
      "epoch": 3.3035714285714284,
      "grad_norm": 13.26763153097608,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 2960
    },
    {
      "epoch": 3.3046875,
      "grad_norm": 14.52855974466453,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 2961
    },
    {
      "epoch": 3.3058035714285716,
      "grad_norm": 13.280886261774967,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 2962
    },
    {
      "epoch": 3.306919642857143,
      "grad_norm": 8.625460186961158,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 2963
    },
    {
      "epoch": 3.3080357142857144,
      "grad_norm": 13.501121962962385,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 2964
    },
    {
      "epoch": 3.3091517857142856,
      "grad_norm": 14.226323923162491,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2965
    },
    {
      "epoch": 3.310267857142857,
      "grad_norm": 12.687537836935778,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 2966
    },
    {
      "epoch": 3.3113839285714284,
      "grad_norm": 14.325300900725557,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 2967
    },
    {
      "epoch": 3.3125,
      "grad_norm": 10.833390788658281,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 2968
    },
    {
      "epoch": 3.3136160714285716,
      "grad_norm": 11.867655264018934,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 2969
    },
    {
      "epoch": 3.314732142857143,
      "grad_norm": 12.968717776942146,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 2970
    },
    {
      "epoch": 3.3158482142857144,
      "grad_norm": 13.135791358471314,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 2971
    },
    {
      "epoch": 3.3169642857142856,
      "grad_norm": 13.91053898702221,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 2972
    },
    {
      "epoch": 3.318080357142857,
      "grad_norm": 13.982050238309428,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 2973
    },
    {
      "epoch": 3.3191964285714284,
      "grad_norm": 11.093670939010044,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 2974
    },
    {
      "epoch": 3.3203125,
      "grad_norm": 11.481361625497644,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2975
    },
    {
      "epoch": 3.3214285714285716,
      "grad_norm": 13.738433182993592,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 2976
    },
    {
      "epoch": 3.322544642857143,
      "grad_norm": 10.754462473881244,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 2977
    },
    {
      "epoch": 3.3236607142857144,
      "grad_norm": 14.699771622041736,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 2978
    },
    {
      "epoch": 3.3247767857142856,
      "grad_norm": 11.418725059295879,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 2979
    },
    {
      "epoch": 3.325892857142857,
      "grad_norm": 12.787285945092286,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 2980
    },
    {
      "epoch": 3.3270089285714284,
      "grad_norm": 13.298155529669078,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 2981
    },
    {
      "epoch": 3.328125,
      "grad_norm": 10.827917459487127,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 2982
    },
    {
      "epoch": 3.3292410714285716,
      "grad_norm": 10.917327565494709,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 2983
    },
    {
      "epoch": 3.330357142857143,
      "grad_norm": 11.201428724300017,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 2984
    },
    {
      "epoch": 3.3314732142857144,
      "grad_norm": 14.39271311850113,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 2985
    },
    {
      "epoch": 3.3325892857142856,
      "grad_norm": 12.145154680387462,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 2986
    },
    {
      "epoch": 3.333705357142857,
      "grad_norm": 12.281812194579274,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 2987
    },
    {
      "epoch": 3.3348214285714284,
      "grad_norm": 12.260032609781335,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 2988
    },
    {
      "epoch": 3.3359375,
      "grad_norm": 13.830497466285154,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 2989
    },
    {
      "epoch": 3.3370535714285716,
      "grad_norm": 11.731017152819152,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 2990
    },
    {
      "epoch": 3.338169642857143,
      "grad_norm": 13.227581348523055,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 2991
    },
    {
      "epoch": 3.3392857142857144,
      "grad_norm": 12.488953185878572,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 2992
    },
    {
      "epoch": 3.3404017857142856,
      "grad_norm": 10.89014660892843,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 2993
    },
    {
      "epoch": 3.341517857142857,
      "grad_norm": 12.074883455861329,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 2994
    },
    {
      "epoch": 3.3426339285714284,
      "grad_norm": 13.693110598565218,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 2995
    },
    {
      "epoch": 3.34375,
      "grad_norm": 12.149720696098823,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 2996
    },
    {
      "epoch": 3.3448660714285716,
      "grad_norm": 14.309025290901891,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 2997
    },
    {
      "epoch": 3.345982142857143,
      "grad_norm": 10.985904214766087,
      "learning_rate": 2e-05,
      "loss": 2.9219,
      "step": 2998
    },
    {
      "epoch": 3.3470982142857144,
      "grad_norm": 9.233310141532684,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 2999
    },
    {
      "epoch": 3.3482142857142856,
      "grad_norm": 7.091403115756404,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 3000
    },
    {
      "epoch": 3.349330357142857,
      "grad_norm": 14.200194510143595,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 3001
    },
    {
      "epoch": 3.3504464285714284,
      "grad_norm": 11.885001223488235,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 3002
    },
    {
      "epoch": 3.3515625,
      "grad_norm": 11.99191291878613,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 3003
    },
    {
      "epoch": 3.3526785714285716,
      "grad_norm": 12.055653714503062,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 3004
    },
    {
      "epoch": 3.353794642857143,
      "grad_norm": 11.05886118952569,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 3005
    },
    {
      "epoch": 3.3549107142857144,
      "grad_norm": 13.22691157431953,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 3006
    },
    {
      "epoch": 3.3560267857142856,
      "grad_norm": 14.478535899266078,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 3007
    },
    {
      "epoch": 3.357142857142857,
      "grad_norm": 12.93749048375731,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 3008
    },
    {
      "epoch": 3.3582589285714284,
      "grad_norm": 13.501427255386773,
      "learning_rate": 2e-05,
      "loss": 1.5312,
      "step": 3009
    },
    {
      "epoch": 3.359375,
      "grad_norm": 11.663689693883736,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 3010
    },
    {
      "epoch": 3.3604910714285716,
      "grad_norm": 18.883511459834136,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 3011
    },
    {
      "epoch": 3.361607142857143,
      "grad_norm": 12.303159031801746,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 3012
    },
    {
      "epoch": 3.3627232142857144,
      "grad_norm": 13.476089618234122,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 3013
    },
    {
      "epoch": 3.3638392857142856,
      "grad_norm": 13.83151790193646,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 3014
    },
    {
      "epoch": 3.364955357142857,
      "grad_norm": 12.074176425761815,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3015
    },
    {
      "epoch": 3.3660714285714284,
      "grad_norm": 12.020510075979365,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 3016
    },
    {
      "epoch": 3.3671875,
      "grad_norm": 13.370428193613463,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 3017
    },
    {
      "epoch": 3.3683035714285716,
      "grad_norm": 14.30216799252902,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 3018
    },
    {
      "epoch": 3.369419642857143,
      "grad_norm": 13.357031441622071,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 3019
    },
    {
      "epoch": 3.3705357142857144,
      "grad_norm": 13.6073953278731,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 3020
    },
    {
      "epoch": 3.3716517857142856,
      "grad_norm": 12.60134773352236,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 3021
    },
    {
      "epoch": 3.372767857142857,
      "grad_norm": 11.897199557942978,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 3022
    },
    {
      "epoch": 3.3738839285714284,
      "grad_norm": 12.587582360948682,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 3023
    },
    {
      "epoch": 3.375,
      "grad_norm": 13.691400063928935,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 3024
    },
    {
      "epoch": 3.3761160714285716,
      "grad_norm": 12.388337804534194,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 3025
    },
    {
      "epoch": 3.377232142857143,
      "grad_norm": 15.743874211149794,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 3026
    },
    {
      "epoch": 3.3783482142857144,
      "grad_norm": 12.568840469366153,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 3027
    },
    {
      "epoch": 3.3794642857142856,
      "grad_norm": 13.89725582575757,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 3028
    },
    {
      "epoch": 3.380580357142857,
      "grad_norm": 11.923747337677769,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 3029
    },
    {
      "epoch": 3.3816964285714284,
      "grad_norm": 10.746695383850732,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 3030
    },
    {
      "epoch": 3.3828125,
      "grad_norm": 12.677093860697005,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3031
    },
    {
      "epoch": 3.3839285714285716,
      "grad_norm": 12.309290654417026,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3032
    },
    {
      "epoch": 3.385044642857143,
      "grad_norm": 12.6590453545914,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 3033
    },
    {
      "epoch": 3.3861607142857144,
      "grad_norm": 13.403330841067485,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 3034
    },
    {
      "epoch": 3.3872767857142856,
      "grad_norm": 15.502601271853344,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 3035
    },
    {
      "epoch": 3.388392857142857,
      "grad_norm": 11.351834975944643,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 3036
    },
    {
      "epoch": 3.3895089285714284,
      "grad_norm": 12.054380697202227,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 3037
    },
    {
      "epoch": 3.390625,
      "grad_norm": 11.881620836147949,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 3038
    },
    {
      "epoch": 3.3917410714285716,
      "grad_norm": 12.357742776358409,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 3039
    },
    {
      "epoch": 3.392857142857143,
      "grad_norm": 12.778370982300908,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 3040
    },
    {
      "epoch": 3.3939732142857144,
      "grad_norm": 26.554234785273707,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3041
    },
    {
      "epoch": 3.3950892857142856,
      "grad_norm": 14.644075522889565,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 3042
    },
    {
      "epoch": 3.396205357142857,
      "grad_norm": 14.501159485929486,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 3043
    },
    {
      "epoch": 3.3973214285714284,
      "grad_norm": 12.208216650932698,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 3044
    },
    {
      "epoch": 3.3984375,
      "grad_norm": 14.5397499861821,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 3045
    },
    {
      "epoch": 3.3995535714285716,
      "grad_norm": 12.658843450746303,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 3046
    },
    {
      "epoch": 3.400669642857143,
      "grad_norm": 10.999489893111116,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 3047
    },
    {
      "epoch": 3.4017857142857144,
      "grad_norm": 14.809984631085262,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 3048
    },
    {
      "epoch": 3.4029017857142856,
      "grad_norm": 12.927480632485542,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 3049
    },
    {
      "epoch": 3.404017857142857,
      "grad_norm": 16.234444790865947,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 3050
    },
    {
      "epoch": 3.4051339285714284,
      "grad_norm": 11.626387419392087,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 3051
    },
    {
      "epoch": 3.40625,
      "grad_norm": 12.41946847900927,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 3052
    },
    {
      "epoch": 3.4073660714285716,
      "grad_norm": 14.711123849758232,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3053
    },
    {
      "epoch": 3.408482142857143,
      "grad_norm": 15.046572412258273,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 3054
    },
    {
      "epoch": 3.4095982142857144,
      "grad_norm": 13.431243960564299,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 3055
    },
    {
      "epoch": 3.4107142857142856,
      "grad_norm": 12.966049011789504,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3056
    },
    {
      "epoch": 3.411830357142857,
      "grad_norm": 10.825869631134749,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 3057
    },
    {
      "epoch": 3.4129464285714284,
      "grad_norm": 14.030081735936442,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 3058
    },
    {
      "epoch": 3.4140625,
      "grad_norm": 15.407169026266098,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 3059
    },
    {
      "epoch": 3.4151785714285716,
      "grad_norm": 12.412232371748427,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3060
    },
    {
      "epoch": 3.416294642857143,
      "grad_norm": 14.204068313828708,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 3061
    },
    {
      "epoch": 3.4174107142857144,
      "grad_norm": 14.858047461050345,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 3062
    },
    {
      "epoch": 3.4185267857142856,
      "grad_norm": 12.41812230407839,
      "learning_rate": 2e-05,
      "loss": 1.3438,
      "step": 3063
    },
    {
      "epoch": 3.419642857142857,
      "grad_norm": 12.95545928446464,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 3064
    },
    {
      "epoch": 3.4207589285714284,
      "grad_norm": 13.626255371269922,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3065
    },
    {
      "epoch": 3.421875,
      "grad_norm": 12.802578183791054,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 3066
    },
    {
      "epoch": 3.4229910714285716,
      "grad_norm": 13.726590924642164,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 3067
    },
    {
      "epoch": 3.424107142857143,
      "grad_norm": 14.432649208293116,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 3068
    },
    {
      "epoch": 3.4252232142857144,
      "grad_norm": 10.772151605027164,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 3069
    },
    {
      "epoch": 3.4263392857142856,
      "grad_norm": 22.026232048315975,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 3070
    },
    {
      "epoch": 3.427455357142857,
      "grad_norm": 13.697281919798831,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 3071
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 16.412684440809517,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3072
    },
    {
      "epoch": 3.4296875,
      "grad_norm": 13.90928416923469,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 3073
    },
    {
      "epoch": 3.4308035714285716,
      "grad_norm": 13.7151555092492,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 3074
    },
    {
      "epoch": 3.431919642857143,
      "grad_norm": 8.986183120946059,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 3075
    },
    {
      "epoch": 3.4330357142857144,
      "grad_norm": 12.127713060694996,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 3076
    },
    {
      "epoch": 3.4341517857142856,
      "grad_norm": 13.710653664210094,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 3077
    },
    {
      "epoch": 3.435267857142857,
      "grad_norm": 11.878529621064917,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 3078
    },
    {
      "epoch": 3.4363839285714284,
      "grad_norm": 12.202635009612266,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 3079
    },
    {
      "epoch": 3.4375,
      "grad_norm": 11.980079926318213,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 3080
    },
    {
      "epoch": 3.4386160714285716,
      "grad_norm": 14.81907374079493,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 3081
    },
    {
      "epoch": 3.439732142857143,
      "grad_norm": 12.736050528243933,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 3082
    },
    {
      "epoch": 3.4408482142857144,
      "grad_norm": 11.62373490872887,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 3083
    },
    {
      "epoch": 3.4419642857142856,
      "grad_norm": 14.65405957847729,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 3084
    },
    {
      "epoch": 3.443080357142857,
      "grad_norm": 13.860620026299772,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 3085
    },
    {
      "epoch": 3.4441964285714284,
      "grad_norm": 10.622140501358519,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3086
    },
    {
      "epoch": 3.4453125,
      "grad_norm": 13.846471732683778,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 3087
    },
    {
      "epoch": 3.4464285714285716,
      "grad_norm": 13.080950627617476,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 3088
    },
    {
      "epoch": 3.447544642857143,
      "grad_norm": 12.632959837124814,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 3089
    },
    {
      "epoch": 3.4486607142857144,
      "grad_norm": 11.288512627749656,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 3090
    },
    {
      "epoch": 3.4497767857142856,
      "grad_norm": 13.146339655728907,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 3091
    },
    {
      "epoch": 3.450892857142857,
      "grad_norm": 11.98174716366478,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 3092
    },
    {
      "epoch": 3.4520089285714284,
      "grad_norm": 13.592591357522075,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 3093
    },
    {
      "epoch": 3.453125,
      "grad_norm": 12.972282100657921,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 3094
    },
    {
      "epoch": 3.4542410714285716,
      "grad_norm": 13.616025490570548,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 3095
    },
    {
      "epoch": 3.455357142857143,
      "grad_norm": 13.290468018156375,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 3096
    },
    {
      "epoch": 3.4564732142857144,
      "grad_norm": 12.70220646280948,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 3097
    },
    {
      "epoch": 3.4575892857142856,
      "grad_norm": 13.623168658919392,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 3098
    },
    {
      "epoch": 3.458705357142857,
      "grad_norm": 13.345650958284956,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 3099
    },
    {
      "epoch": 3.4598214285714284,
      "grad_norm": 13.267758961584539,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 3100
    },
    {
      "epoch": 3.4609375,
      "grad_norm": 12.602178450279274,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 3101
    },
    {
      "epoch": 3.4620535714285716,
      "grad_norm": 14.164440831064056,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 3102
    },
    {
      "epoch": 3.463169642857143,
      "grad_norm": 15.337828819347948,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 3103
    },
    {
      "epoch": 3.4642857142857144,
      "grad_norm": 12.392098561968387,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 3104
    },
    {
      "epoch": 3.4654017857142856,
      "grad_norm": 12.76686863701763,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 3105
    },
    {
      "epoch": 3.466517857142857,
      "grad_norm": 11.429514260230969,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 3106
    },
    {
      "epoch": 3.4676339285714284,
      "grad_norm": 13.170171074638281,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 3107
    },
    {
      "epoch": 3.46875,
      "grad_norm": 12.179648056053145,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 3108
    },
    {
      "epoch": 3.4698660714285716,
      "grad_norm": 11.759137578448675,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 3109
    },
    {
      "epoch": 3.470982142857143,
      "grad_norm": 12.620595072157931,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3110
    },
    {
      "epoch": 3.4720982142857144,
      "grad_norm": 13.460688007400961,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 3111
    },
    {
      "epoch": 3.4732142857142856,
      "grad_norm": 12.259252601248111,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 3112
    },
    {
      "epoch": 3.474330357142857,
      "grad_norm": 14.351819952267554,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 3113
    },
    {
      "epoch": 3.4754464285714284,
      "grad_norm": 11.533725088531202,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 3114
    },
    {
      "epoch": 3.4765625,
      "grad_norm": 9.035431477943519,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 3115
    },
    {
      "epoch": 3.4776785714285716,
      "grad_norm": 12.26893475184646,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 3116
    },
    {
      "epoch": 3.478794642857143,
      "grad_norm": 10.251757912608678,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 3117
    },
    {
      "epoch": 3.4799107142857144,
      "grad_norm": 10.608783335845393,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 3118
    },
    {
      "epoch": 3.4810267857142856,
      "grad_norm": 12.096135875677575,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 3119
    },
    {
      "epoch": 3.482142857142857,
      "grad_norm": 14.699309645091134,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 3120
    },
    {
      "epoch": 3.4832589285714284,
      "grad_norm": 8.992538278610755,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 3121
    },
    {
      "epoch": 3.484375,
      "grad_norm": 14.94392084743666,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 3122
    },
    {
      "epoch": 3.4854910714285716,
      "grad_norm": 13.79374117070986,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 3123
    },
    {
      "epoch": 3.486607142857143,
      "grad_norm": 13.902370185160029,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 3124
    },
    {
      "epoch": 3.4877232142857144,
      "grad_norm": 14.452467762261161,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3125
    },
    {
      "epoch": 3.4888392857142856,
      "grad_norm": 12.895965919927985,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 3126
    },
    {
      "epoch": 3.489955357142857,
      "grad_norm": 13.570944286913264,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 3127
    },
    {
      "epoch": 3.4910714285714284,
      "grad_norm": 13.636914859770977,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 3128
    },
    {
      "epoch": 3.4921875,
      "grad_norm": 13.357310340927672,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 3129
    },
    {
      "epoch": 3.4933035714285716,
      "grad_norm": 15.752833060489719,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 3130
    },
    {
      "epoch": 3.494419642857143,
      "grad_norm": 13.694345484267805,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 3131
    },
    {
      "epoch": 3.4955357142857144,
      "grad_norm": 16.82299005202381,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 3132
    },
    {
      "epoch": 3.4966517857142856,
      "grad_norm": 13.65869231718609,
      "learning_rate": 2e-05,
      "loss": 1.25,
      "step": 3133
    },
    {
      "epoch": 3.497767857142857,
      "grad_norm": 10.953518430795654,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 3134
    },
    {
      "epoch": 3.4988839285714284,
      "grad_norm": 12.097306789043826,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 3135
    },
    {
      "epoch": 3.5,
      "grad_norm": 11.624665561551,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 3136
    },
    {
      "epoch": 3.501116071428571,
      "grad_norm": 13.017944999566705,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3137
    },
    {
      "epoch": 3.502232142857143,
      "grad_norm": 12.030512996110904,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 3138
    },
    {
      "epoch": 3.5033482142857144,
      "grad_norm": 13.065392862148363,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 3139
    },
    {
      "epoch": 3.5044642857142856,
      "grad_norm": 13.370433746954282,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 3140
    },
    {
      "epoch": 3.505580357142857,
      "grad_norm": 14.459186423646901,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 3141
    },
    {
      "epoch": 3.506696428571429,
      "grad_norm": 13.404893201081048,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 3142
    },
    {
      "epoch": 3.5078125,
      "grad_norm": 14.321428777299548,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 3143
    },
    {
      "epoch": 3.508928571428571,
      "grad_norm": 13.273237568599601,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 3144
    },
    {
      "epoch": 3.510044642857143,
      "grad_norm": 13.556293237305216,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 3145
    },
    {
      "epoch": 3.5111607142857144,
      "grad_norm": 11.705239362353492,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 3146
    },
    {
      "epoch": 3.5122767857142856,
      "grad_norm": 12.951420797393457,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 3147
    },
    {
      "epoch": 3.513392857142857,
      "grad_norm": 18.038253810583605,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 3148
    },
    {
      "epoch": 3.514508928571429,
      "grad_norm": 12.691670712956773,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3149
    },
    {
      "epoch": 3.515625,
      "grad_norm": 11.795882754228982,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 3150
    },
    {
      "epoch": 3.516741071428571,
      "grad_norm": 14.029067141268383,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 3151
    },
    {
      "epoch": 3.517857142857143,
      "grad_norm": 7.212717752449476,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 3152
    },
    {
      "epoch": 3.5189732142857144,
      "grad_norm": 16.648127273271278,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3153
    },
    {
      "epoch": 3.5200892857142856,
      "grad_norm": 11.743717224277523,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3154
    },
    {
      "epoch": 3.521205357142857,
      "grad_norm": 12.249266286085717,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 3155
    },
    {
      "epoch": 3.522321428571429,
      "grad_norm": 10.640334703845548,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 3156
    },
    {
      "epoch": 3.5234375,
      "grad_norm": 12.712038954648962,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 3157
    },
    {
      "epoch": 3.524553571428571,
      "grad_norm": 14.733478015831507,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 3158
    },
    {
      "epoch": 3.525669642857143,
      "grad_norm": 14.31066750963497,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 3159
    },
    {
      "epoch": 3.5267857142857144,
      "grad_norm": 17.31186117916992,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 3160
    },
    {
      "epoch": 3.5279017857142856,
      "grad_norm": 13.162455338572437,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 3161
    },
    {
      "epoch": 3.529017857142857,
      "grad_norm": 14.629039646787628,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 3162
    },
    {
      "epoch": 3.530133928571429,
      "grad_norm": 10.68656759400079,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 3163
    },
    {
      "epoch": 3.53125,
      "grad_norm": 13.23923880440547,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3164
    },
    {
      "epoch": 3.532366071428571,
      "grad_norm": 11.56716302611175,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 3165
    },
    {
      "epoch": 3.533482142857143,
      "grad_norm": 12.366671157301413,
      "learning_rate": 2e-05,
      "loss": 1.3438,
      "step": 3166
    },
    {
      "epoch": 3.5345982142857144,
      "grad_norm": 12.831933964159687,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 3167
    },
    {
      "epoch": 3.5357142857142856,
      "grad_norm": 15.78639425793115,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 3168
    },
    {
      "epoch": 3.536830357142857,
      "grad_norm": 14.048797188784267,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 3169
    },
    {
      "epoch": 3.537946428571429,
      "grad_norm": 11.775680905622343,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 3170
    },
    {
      "epoch": 3.5390625,
      "grad_norm": 12.603853282335931,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 3171
    },
    {
      "epoch": 3.540178571428571,
      "grad_norm": 13.585191227030542,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 3172
    },
    {
      "epoch": 3.541294642857143,
      "grad_norm": 14.493148364936905,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 3173
    },
    {
      "epoch": 3.5424107142857144,
      "grad_norm": 16.98757113633129,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 3174
    },
    {
      "epoch": 3.5435267857142856,
      "grad_norm": 14.37330710071911,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 3175
    },
    {
      "epoch": 3.544642857142857,
      "grad_norm": 13.82833862265973,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 3176
    },
    {
      "epoch": 3.545758928571429,
      "grad_norm": 14.442538370921293,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 3177
    },
    {
      "epoch": 3.546875,
      "grad_norm": 14.772660240182175,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 3178
    },
    {
      "epoch": 3.547991071428571,
      "grad_norm": 14.100742317232333,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 3179
    },
    {
      "epoch": 3.549107142857143,
      "grad_norm": 13.493415466011575,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 3180
    },
    {
      "epoch": 3.5502232142857144,
      "grad_norm": 17.000811757376074,
      "learning_rate": 2e-05,
      "loss": 1.3047,
      "step": 3181
    },
    {
      "epoch": 3.5513392857142856,
      "grad_norm": 15.3027748831128,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 3182
    },
    {
      "epoch": 3.552455357142857,
      "grad_norm": 14.025284705061399,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 3183
    },
    {
      "epoch": 3.553571428571429,
      "grad_norm": 13.859910899866335,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 3184
    },
    {
      "epoch": 3.5546875,
      "grad_norm": 11.020310871619056,
      "learning_rate": 2e-05,
      "loss": 3.0156,
      "step": 3185
    },
    {
      "epoch": 3.555803571428571,
      "grad_norm": 14.244876496233662,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 3186
    },
    {
      "epoch": 3.556919642857143,
      "grad_norm": 13.864590792166902,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 3187
    },
    {
      "epoch": 3.5580357142857144,
      "grad_norm": 12.415885259080278,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 3188
    },
    {
      "epoch": 3.5591517857142856,
      "grad_norm": 13.753387657536598,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 3189
    },
    {
      "epoch": 3.560267857142857,
      "grad_norm": 11.213330803008775,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 3190
    },
    {
      "epoch": 3.561383928571429,
      "grad_norm": 18.514564727649557,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 3191
    },
    {
      "epoch": 3.5625,
      "grad_norm": 14.558145871974778,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 3192
    },
    {
      "epoch": 3.563616071428571,
      "grad_norm": 16.10499462541325,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 3193
    },
    {
      "epoch": 3.564732142857143,
      "grad_norm": 14.586396573031433,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 3194
    },
    {
      "epoch": 3.5658482142857144,
      "grad_norm": 16.18542480561271,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 3195
    },
    {
      "epoch": 3.5669642857142856,
      "grad_norm": 16.17387539170875,
      "learning_rate": 2e-05,
      "loss": 1.5,
      "step": 3196
    },
    {
      "epoch": 3.568080357142857,
      "grad_norm": 10.871193692536384,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 3197
    },
    {
      "epoch": 3.569196428571429,
      "grad_norm": 12.781412459870191,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 3198
    },
    {
      "epoch": 3.5703125,
      "grad_norm": 12.273982009969163,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 3199
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 12.888522521371094,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 3200
    },
    {
      "epoch": 3.572544642857143,
      "grad_norm": 12.707821541831633,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 3201
    },
    {
      "epoch": 3.5736607142857144,
      "grad_norm": 13.83927303855642,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 3202
    },
    {
      "epoch": 3.5747767857142856,
      "grad_norm": 13.509619942241782,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 3203
    },
    {
      "epoch": 3.575892857142857,
      "grad_norm": 13.017653881242996,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 3204
    },
    {
      "epoch": 3.577008928571429,
      "grad_norm": 8.314947207480317,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 3205
    },
    {
      "epoch": 3.578125,
      "grad_norm": 16.184240113181055,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 3206
    },
    {
      "epoch": 3.579241071428571,
      "grad_norm": 13.632556729914752,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3207
    },
    {
      "epoch": 3.580357142857143,
      "grad_norm": 12.536511715581572,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 3208
    },
    {
      "epoch": 3.5814732142857144,
      "grad_norm": 13.185720223335954,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 3209
    },
    {
      "epoch": 3.5825892857142856,
      "grad_norm": 11.712520340170759,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 3210
    },
    {
      "epoch": 3.583705357142857,
      "grad_norm": 13.521605572216895,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 3211
    },
    {
      "epoch": 3.584821428571429,
      "grad_norm": 11.744849869134113,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 3212
    },
    {
      "epoch": 3.5859375,
      "grad_norm": 15.767580912265117,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 3213
    },
    {
      "epoch": 3.587053571428571,
      "grad_norm": 9.258493858954449,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 3214
    },
    {
      "epoch": 3.588169642857143,
      "grad_norm": 14.950009745746906,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 3215
    },
    {
      "epoch": 3.5892857142857144,
      "grad_norm": 13.18598539920433,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 3216
    },
    {
      "epoch": 3.5904017857142856,
      "grad_norm": 13.2744981146761,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 3217
    },
    {
      "epoch": 3.591517857142857,
      "grad_norm": 13.784019089591663,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 3218
    },
    {
      "epoch": 3.592633928571429,
      "grad_norm": 13.337829139518355,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 3219
    },
    {
      "epoch": 3.59375,
      "grad_norm": 14.42408080708832,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 3220
    },
    {
      "epoch": 3.594866071428571,
      "grad_norm": 13.369544192324112,
      "learning_rate": 2e-05,
      "loss": 1.5312,
      "step": 3221
    },
    {
      "epoch": 3.595982142857143,
      "grad_norm": 15.559349234005245,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 3222
    },
    {
      "epoch": 3.5970982142857144,
      "grad_norm": 15.857850186215574,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 3223
    },
    {
      "epoch": 3.5982142857142856,
      "grad_norm": 12.106176805988483,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 3224
    },
    {
      "epoch": 3.599330357142857,
      "grad_norm": 12.680349131059153,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 3225
    },
    {
      "epoch": 3.600446428571429,
      "grad_norm": 14.02480808683973,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 3226
    },
    {
      "epoch": 3.6015625,
      "grad_norm": 14.539136951542131,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 3227
    },
    {
      "epoch": 3.602678571428571,
      "grad_norm": 13.14736195406958,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3228
    },
    {
      "epoch": 3.603794642857143,
      "grad_norm": 12.125501448219492,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 3229
    },
    {
      "epoch": 3.6049107142857144,
      "grad_norm": 12.961244673951828,
      "learning_rate": 2e-05,
      "loss": 1.2969,
      "step": 3230
    },
    {
      "epoch": 3.6060267857142856,
      "grad_norm": 15.006085696075688,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 3231
    },
    {
      "epoch": 3.607142857142857,
      "grad_norm": 16.84634231625927,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 3232
    },
    {
      "epoch": 3.608258928571429,
      "grad_norm": 12.121714780807475,
      "learning_rate": 2e-05,
      "loss": 1.2734,
      "step": 3233
    },
    {
      "epoch": 3.609375,
      "grad_norm": 15.628064724087464,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 3234
    },
    {
      "epoch": 3.610491071428571,
      "grad_norm": 15.518996347897927,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 3235
    },
    {
      "epoch": 3.611607142857143,
      "grad_norm": 9.715603482848824,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 3236
    },
    {
      "epoch": 3.6127232142857144,
      "grad_norm": 14.410166898956062,
      "learning_rate": 2e-05,
      "loss": 1.3906,
      "step": 3237
    },
    {
      "epoch": 3.6138392857142856,
      "grad_norm": 13.954271774015208,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 3238
    },
    {
      "epoch": 3.614955357142857,
      "grad_norm": 13.587379652414212,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 3239
    },
    {
      "epoch": 3.616071428571429,
      "grad_norm": 13.304662811049507,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 3240
    },
    {
      "epoch": 3.6171875,
      "grad_norm": 13.502517464091714,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3241
    },
    {
      "epoch": 3.618303571428571,
      "grad_norm": 14.506974253666439,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 3242
    },
    {
      "epoch": 3.619419642857143,
      "grad_norm": 13.780172250638978,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 3243
    },
    {
      "epoch": 3.6205357142857144,
      "grad_norm": 18.645683713299185,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3244
    },
    {
      "epoch": 3.6216517857142856,
      "grad_norm": 14.532572388969118,
      "learning_rate": 2e-05,
      "loss": 1.5,
      "step": 3245
    },
    {
      "epoch": 3.622767857142857,
      "grad_norm": 10.583848701735489,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 3246
    },
    {
      "epoch": 3.623883928571429,
      "grad_norm": 13.933720347634447,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 3247
    },
    {
      "epoch": 3.625,
      "grad_norm": 16.66823443154975,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 3248
    },
    {
      "epoch": 3.626116071428571,
      "grad_norm": 12.91948826160552,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 3249
    },
    {
      "epoch": 3.627232142857143,
      "grad_norm": 16.10992172026425,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 3250
    },
    {
      "epoch": 3.6283482142857144,
      "grad_norm": 16.21661220371929,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 3251
    },
    {
      "epoch": 3.6294642857142856,
      "grad_norm": 13.430127079217401,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 3252
    },
    {
      "epoch": 3.630580357142857,
      "grad_norm": 15.049035179758267,
      "learning_rate": 2e-05,
      "loss": 1.2812,
      "step": 3253
    },
    {
      "epoch": 3.631696428571429,
      "grad_norm": 14.797414696439468,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 3254
    },
    {
      "epoch": 3.6328125,
      "grad_norm": 13.668286550993033,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 3255
    },
    {
      "epoch": 3.633928571428571,
      "grad_norm": 8.01215529587905,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 3256
    },
    {
      "epoch": 3.635044642857143,
      "grad_norm": 14.426087912042567,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 3257
    },
    {
      "epoch": 3.6361607142857144,
      "grad_norm": 13.700477781547683,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 3258
    },
    {
      "epoch": 3.6372767857142856,
      "grad_norm": 15.283388612758465,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 3259
    },
    {
      "epoch": 3.638392857142857,
      "grad_norm": 10.780118236248596,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 3260
    },
    {
      "epoch": 3.639508928571429,
      "grad_norm": 14.020012541748532,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 3261
    },
    {
      "epoch": 3.640625,
      "grad_norm": 14.853694488887449,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 3262
    },
    {
      "epoch": 3.641741071428571,
      "grad_norm": 14.271415296662637,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 3263
    },
    {
      "epoch": 3.642857142857143,
      "grad_norm": 12.69129872574151,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 3264
    },
    {
      "epoch": 3.6439732142857144,
      "grad_norm": 14.807113198294559,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 3265
    },
    {
      "epoch": 3.6450892857142856,
      "grad_norm": 13.875474060819505,
      "learning_rate": 2e-05,
      "loss": 1.4453,
      "step": 3266
    },
    {
      "epoch": 3.646205357142857,
      "grad_norm": 13.62077656155687,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 3267
    },
    {
      "epoch": 3.647321428571429,
      "grad_norm": 11.04814206497518,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 3268
    },
    {
      "epoch": 3.6484375,
      "grad_norm": 12.212894077073742,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 3269
    },
    {
      "epoch": 3.649553571428571,
      "grad_norm": 15.048524360195067,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 3270
    },
    {
      "epoch": 3.650669642857143,
      "grad_norm": 12.415655521477252,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 3271
    },
    {
      "epoch": 3.6517857142857144,
      "grad_norm": 15.807048995563283,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 3272
    },
    {
      "epoch": 3.6529017857142856,
      "grad_norm": 13.446518036194576,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 3273
    },
    {
      "epoch": 3.654017857142857,
      "grad_norm": 14.71265355117531,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 3274
    },
    {
      "epoch": 3.655133928571429,
      "grad_norm": 13.204174441661873,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 3275
    },
    {
      "epoch": 3.65625,
      "grad_norm": 13.654243028527516,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 3276
    },
    {
      "epoch": 3.657366071428571,
      "grad_norm": 11.800240416559797,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 3277
    },
    {
      "epoch": 3.658482142857143,
      "grad_norm": 12.960760076476634,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 3278
    },
    {
      "epoch": 3.6595982142857144,
      "grad_norm": 11.731886576326568,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 3279
    },
    {
      "epoch": 3.6607142857142856,
      "grad_norm": 14.693739063661983,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3280
    },
    {
      "epoch": 3.661830357142857,
      "grad_norm": 13.662314877200869,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 3281
    },
    {
      "epoch": 3.662946428571429,
      "grad_norm": 12.17156142132884,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 3282
    },
    {
      "epoch": 3.6640625,
      "grad_norm": 12.621650705690032,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 3283
    },
    {
      "epoch": 3.665178571428571,
      "grad_norm": 13.832141137791886,
      "learning_rate": 2e-05,
      "loss": 1.4453,
      "step": 3284
    },
    {
      "epoch": 3.666294642857143,
      "grad_norm": 10.831284227616486,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 3285
    },
    {
      "epoch": 3.6674107142857144,
      "grad_norm": 12.221518547571666,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 3286
    },
    {
      "epoch": 3.6685267857142856,
      "grad_norm": 14.67961483486221,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 3287
    },
    {
      "epoch": 3.669642857142857,
      "grad_norm": 12.650099520984082,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 3288
    },
    {
      "epoch": 3.670758928571429,
      "grad_norm": 13.931183620579267,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 3289
    },
    {
      "epoch": 3.671875,
      "grad_norm": 11.604703533172628,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 3290
    },
    {
      "epoch": 3.672991071428571,
      "grad_norm": 15.175419083337431,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3291
    },
    {
      "epoch": 3.674107142857143,
      "grad_norm": 15.837950349093271,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 3292
    },
    {
      "epoch": 3.6752232142857144,
      "grad_norm": 13.665499884608582,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3293
    },
    {
      "epoch": 3.6763392857142856,
      "grad_norm": 10.21420654536574,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 3294
    },
    {
      "epoch": 3.677455357142857,
      "grad_norm": 12.889744619233507,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 3295
    },
    {
      "epoch": 3.678571428571429,
      "grad_norm": 12.538289468304336,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 3296
    },
    {
      "epoch": 3.6796875,
      "grad_norm": 14.090671957333408,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 3297
    },
    {
      "epoch": 3.680803571428571,
      "grad_norm": 13.581595405176214,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 3298
    },
    {
      "epoch": 3.681919642857143,
      "grad_norm": 13.561056716864536,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 3299
    },
    {
      "epoch": 3.6830357142857144,
      "grad_norm": 12.898881047009871,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 3300
    },
    {
      "epoch": 3.6841517857142856,
      "grad_norm": 11.710469654647898,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 3301
    },
    {
      "epoch": 3.685267857142857,
      "grad_norm": 9.110661298183707,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 3302
    },
    {
      "epoch": 3.686383928571429,
      "grad_norm": 13.350815427354748,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 3303
    },
    {
      "epoch": 3.6875,
      "grad_norm": 11.996630398039063,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 3304
    },
    {
      "epoch": 3.688616071428571,
      "grad_norm": 13.010227916647235,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 3305
    },
    {
      "epoch": 3.689732142857143,
      "grad_norm": 12.257342417533915,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 3306
    },
    {
      "epoch": 3.6908482142857144,
      "grad_norm": 14.747671587130915,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 3307
    },
    {
      "epoch": 3.6919642857142856,
      "grad_norm": 12.812098320415185,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 3308
    },
    {
      "epoch": 3.693080357142857,
      "grad_norm": 12.334182569880817,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 3309
    },
    {
      "epoch": 3.694196428571429,
      "grad_norm": 13.2921776507233,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 3310
    },
    {
      "epoch": 3.6953125,
      "grad_norm": 10.698432031210585,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 3311
    },
    {
      "epoch": 3.696428571428571,
      "grad_norm": 14.268514321280966,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 3312
    },
    {
      "epoch": 3.697544642857143,
      "grad_norm": 9.076708779849078,
      "learning_rate": 2e-05,
      "loss": 2.9219,
      "step": 3313
    },
    {
      "epoch": 3.6986607142857144,
      "grad_norm": 11.800239736207242,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 3314
    },
    {
      "epoch": 3.6997767857142856,
      "grad_norm": 13.575334132981807,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 3315
    },
    {
      "epoch": 3.700892857142857,
      "grad_norm": 14.393572734494262,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3316
    },
    {
      "epoch": 3.702008928571429,
      "grad_norm": 15.83082329641525,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 3317
    },
    {
      "epoch": 3.703125,
      "grad_norm": 16.37877051359071,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 3318
    },
    {
      "epoch": 3.704241071428571,
      "grad_norm": 13.010926138743258,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3319
    },
    {
      "epoch": 3.705357142857143,
      "grad_norm": 17.49007243257802,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 3320
    },
    {
      "epoch": 3.7064732142857144,
      "grad_norm": 15.998023320458453,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 3321
    },
    {
      "epoch": 3.7075892857142856,
      "grad_norm": 12.873736998190187,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 3322
    },
    {
      "epoch": 3.708705357142857,
      "grad_norm": 17.79737508740961,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 3323
    },
    {
      "epoch": 3.709821428571429,
      "grad_norm": 12.623797385554848,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 3324
    },
    {
      "epoch": 3.7109375,
      "grad_norm": 10.931887916093238,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 3325
    },
    {
      "epoch": 3.712053571428571,
      "grad_norm": 11.501503763751074,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 3326
    },
    {
      "epoch": 3.713169642857143,
      "grad_norm": 10.939562751047248,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 3327
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 14.373848404983454,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3328
    },
    {
      "epoch": 3.7154017857142856,
      "grad_norm": 12.680403232429947,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 3329
    },
    {
      "epoch": 3.716517857142857,
      "grad_norm": 12.784136405644798,
      "learning_rate": 2e-05,
      "loss": 1.3672,
      "step": 3330
    },
    {
      "epoch": 3.717633928571429,
      "grad_norm": 12.768057055564558,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 3331
    },
    {
      "epoch": 3.71875,
      "grad_norm": 13.885826242740361,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 3332
    },
    {
      "epoch": 3.719866071428571,
      "grad_norm": 13.226229049531055,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 3333
    },
    {
      "epoch": 3.720982142857143,
      "grad_norm": 13.848364288157205,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 3334
    },
    {
      "epoch": 3.7220982142857144,
      "grad_norm": 15.714707645196746,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 3335
    },
    {
      "epoch": 3.7232142857142856,
      "grad_norm": 14.69768566512684,
      "learning_rate": 2e-05,
      "loss": 1.5,
      "step": 3336
    },
    {
      "epoch": 3.724330357142857,
      "grad_norm": 15.60226370763082,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 3337
    },
    {
      "epoch": 3.725446428571429,
      "grad_norm": 12.97620099308935,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3338
    },
    {
      "epoch": 3.7265625,
      "grad_norm": 15.396722000194245,
      "learning_rate": 2e-05,
      "loss": 1.5156,
      "step": 3339
    },
    {
      "epoch": 3.727678571428571,
      "grad_norm": 10.720505196816521,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 3340
    },
    {
      "epoch": 3.728794642857143,
      "grad_norm": 12.659723261429056,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 3341
    },
    {
      "epoch": 3.7299107142857144,
      "grad_norm": 12.757156617864172,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 3342
    },
    {
      "epoch": 3.7310267857142856,
      "grad_norm": 13.655128565431806,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 3343
    },
    {
      "epoch": 3.732142857142857,
      "grad_norm": 15.37172168674336,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 3344
    },
    {
      "epoch": 3.733258928571429,
      "grad_norm": 14.264855455915383,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 3345
    },
    {
      "epoch": 3.734375,
      "grad_norm": 14.376526090380711,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 3346
    },
    {
      "epoch": 3.735491071428571,
      "grad_norm": 11.355822075120095,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 3347
    },
    {
      "epoch": 3.736607142857143,
      "grad_norm": 12.211556539829651,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 3348
    },
    {
      "epoch": 3.7377232142857144,
      "grad_norm": 15.819565793630694,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 3349
    },
    {
      "epoch": 3.7388392857142856,
      "grad_norm": 14.665076319180253,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 3350
    },
    {
      "epoch": 3.739955357142857,
      "grad_norm": 11.279787033609109,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 3351
    },
    {
      "epoch": 3.741071428571429,
      "grad_norm": 11.8664040283215,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 3352
    },
    {
      "epoch": 3.7421875,
      "grad_norm": 14.000867416079737,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 3353
    },
    {
      "epoch": 3.743303571428571,
      "grad_norm": 9.457290746965354,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 3354
    },
    {
      "epoch": 3.744419642857143,
      "grad_norm": 14.711309168652189,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3355
    },
    {
      "epoch": 3.7455357142857144,
      "grad_norm": 11.557941630512365,
      "learning_rate": 2e-05,
      "loss": 1.3203,
      "step": 3356
    },
    {
      "epoch": 3.7466517857142856,
      "grad_norm": 14.3924536981559,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 3357
    },
    {
      "epoch": 3.747767857142857,
      "grad_norm": 14.437454523489569,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 3358
    },
    {
      "epoch": 3.748883928571429,
      "grad_norm": 13.30964334175026,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 3359
    },
    {
      "epoch": 3.75,
      "grad_norm": 14.334293959654067,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 3360
    },
    {
      "epoch": 3.751116071428571,
      "grad_norm": 14.48611355169826,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 3361
    },
    {
      "epoch": 3.752232142857143,
      "grad_norm": 14.147486412506096,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 3362
    },
    {
      "epoch": 3.7533482142857144,
      "grad_norm": 13.64120397992009,
      "learning_rate": 2e-05,
      "loss": 1.5,
      "step": 3363
    },
    {
      "epoch": 3.7544642857142856,
      "grad_norm": 15.627452925302682,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 3364
    },
    {
      "epoch": 3.755580357142857,
      "grad_norm": 12.776403465428297,
      "learning_rate": 2e-05,
      "loss": 2.7656,
      "step": 3365
    },
    {
      "epoch": 3.756696428571429,
      "grad_norm": 12.673599564514067,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 3366
    },
    {
      "epoch": 3.7578125,
      "grad_norm": 13.270018039914538,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 3367
    },
    {
      "epoch": 3.758928571428571,
      "grad_norm": 14.529953849361416,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 3368
    },
    {
      "epoch": 3.760044642857143,
      "grad_norm": 12.906480763619903,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 3369
    },
    {
      "epoch": 3.7611607142857144,
      "grad_norm": 12.837795892010654,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 3370
    },
    {
      "epoch": 3.7622767857142856,
      "grad_norm": 14.235722493148192,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 3371
    },
    {
      "epoch": 3.763392857142857,
      "grad_norm": 12.108521553669107,
      "learning_rate": 2e-05,
      "loss": 1.4141,
      "step": 3372
    },
    {
      "epoch": 3.764508928571429,
      "grad_norm": 14.546390414811372,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3373
    },
    {
      "epoch": 3.765625,
      "grad_norm": 13.902753881152398,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 3374
    },
    {
      "epoch": 3.766741071428571,
      "grad_norm": 15.143745619375117,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 3375
    },
    {
      "epoch": 3.767857142857143,
      "grad_norm": 13.902097562195452,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 3376
    },
    {
      "epoch": 3.7689732142857144,
      "grad_norm": 13.313172801194355,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 3377
    },
    {
      "epoch": 3.7700892857142856,
      "grad_norm": 11.39123747031315,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 3378
    },
    {
      "epoch": 3.771205357142857,
      "grad_norm": 13.998832464313997,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 3379
    },
    {
      "epoch": 3.772321428571429,
      "grad_norm": 12.18362035008714,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 3380
    },
    {
      "epoch": 3.7734375,
      "grad_norm": 11.22667917237856,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 3381
    },
    {
      "epoch": 3.774553571428571,
      "grad_norm": 12.179063504963144,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 3382
    },
    {
      "epoch": 3.775669642857143,
      "grad_norm": 14.87362806924393,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 3383
    },
    {
      "epoch": 3.7767857142857144,
      "grad_norm": 15.67526666767427,
      "learning_rate": 2e-05,
      "loss": 3.3438,
      "step": 3384
    },
    {
      "epoch": 3.7779017857142856,
      "grad_norm": 14.472135394921185,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 3385
    },
    {
      "epoch": 3.779017857142857,
      "grad_norm": 15.429521544769283,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 3386
    },
    {
      "epoch": 3.780133928571429,
      "grad_norm": 13.264075002363343,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3387
    },
    {
      "epoch": 3.78125,
      "grad_norm": 15.473956563184688,
      "learning_rate": 2e-05,
      "loss": 1.6875,
      "step": 3388
    },
    {
      "epoch": 3.782366071428571,
      "grad_norm": 16.136385257116675,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 3389
    },
    {
      "epoch": 3.783482142857143,
      "grad_norm": 12.893108361238248,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 3390
    },
    {
      "epoch": 3.7845982142857144,
      "grad_norm": 11.685560181765034,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 3391
    },
    {
      "epoch": 3.7857142857142856,
      "grad_norm": 16.917706706867616,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 3392
    },
    {
      "epoch": 3.786830357142857,
      "grad_norm": 18.936422320964443,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 3393
    },
    {
      "epoch": 3.787946428571429,
      "grad_norm": 15.248410630686907,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 3394
    },
    {
      "epoch": 3.7890625,
      "grad_norm": 13.29106315943588,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 3395
    },
    {
      "epoch": 3.790178571428571,
      "grad_norm": 12.935609512948506,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 3396
    },
    {
      "epoch": 3.791294642857143,
      "grad_norm": 14.429477801691116,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 3397
    },
    {
      "epoch": 3.7924107142857144,
      "grad_norm": 14.084537741362906,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 3398
    },
    {
      "epoch": 3.7935267857142856,
      "grad_norm": 14.024601557538531,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 3399
    },
    {
      "epoch": 3.794642857142857,
      "grad_norm": 14.141312044390183,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 3400
    },
    {
      "epoch": 3.795758928571429,
      "grad_norm": 12.82175768498664,
      "learning_rate": 2e-05,
      "loss": 1.2578,
      "step": 3401
    },
    {
      "epoch": 3.796875,
      "grad_norm": 13.525166209449022,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 3402
    },
    {
      "epoch": 3.797991071428571,
      "grad_norm": 12.169531436325265,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 3403
    },
    {
      "epoch": 3.799107142857143,
      "grad_norm": 13.750562623243466,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 3404
    },
    {
      "epoch": 3.8002232142857144,
      "grad_norm": 13.354504432566689,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 3405
    },
    {
      "epoch": 3.8013392857142856,
      "grad_norm": 16.087593192212676,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 3406
    },
    {
      "epoch": 3.802455357142857,
      "grad_norm": 13.786019354147783,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 3407
    },
    {
      "epoch": 3.803571428571429,
      "grad_norm": 13.42104135358825,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 3408
    },
    {
      "epoch": 3.8046875,
      "grad_norm": 14.799533541147353,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 3409
    },
    {
      "epoch": 3.805803571428571,
      "grad_norm": 12.623983265758309,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 3410
    },
    {
      "epoch": 3.806919642857143,
      "grad_norm": 15.31451142852299,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3411
    },
    {
      "epoch": 3.8080357142857144,
      "grad_norm": 13.187069482746518,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 3412
    },
    {
      "epoch": 3.8091517857142856,
      "grad_norm": 12.941290134109224,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 3413
    },
    {
      "epoch": 3.810267857142857,
      "grad_norm": 14.535973123357806,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 3414
    },
    {
      "epoch": 3.811383928571429,
      "grad_norm": 15.746063989432983,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 3415
    },
    {
      "epoch": 3.8125,
      "grad_norm": 14.420097214967337,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 3416
    },
    {
      "epoch": 3.813616071428571,
      "grad_norm": 13.65293031149373,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3417
    },
    {
      "epoch": 3.814732142857143,
      "grad_norm": 14.721964125813683,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 3418
    },
    {
      "epoch": 3.8158482142857144,
      "grad_norm": 15.186561881803048,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 3419
    },
    {
      "epoch": 3.8169642857142856,
      "grad_norm": 9.64552030182666,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 3420
    },
    {
      "epoch": 3.818080357142857,
      "grad_norm": 16.333329291358872,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 3421
    },
    {
      "epoch": 3.819196428571429,
      "grad_norm": 10.57052589740523,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 3422
    },
    {
      "epoch": 3.8203125,
      "grad_norm": 15.787644602249996,
      "learning_rate": 2e-05,
      "loss": 1.3828,
      "step": 3423
    },
    {
      "epoch": 3.821428571428571,
      "grad_norm": 15.622393269706938,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 3424
    },
    {
      "epoch": 3.822544642857143,
      "grad_norm": 14.003095219595158,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 3425
    },
    {
      "epoch": 3.8236607142857144,
      "grad_norm": 11.86093091065442,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 3426
    },
    {
      "epoch": 3.8247767857142856,
      "grad_norm": 15.220018775294994,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 3427
    },
    {
      "epoch": 3.825892857142857,
      "grad_norm": 11.490125792535594,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 3428
    },
    {
      "epoch": 3.827008928571429,
      "grad_norm": 13.788674801344083,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 3429
    },
    {
      "epoch": 3.828125,
      "grad_norm": 16.13274072023643,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 3430
    },
    {
      "epoch": 3.829241071428571,
      "grad_norm": 13.681828871627374,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 3431
    },
    {
      "epoch": 3.830357142857143,
      "grad_norm": 13.811651070157215,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3432
    },
    {
      "epoch": 3.8314732142857144,
      "grad_norm": 12.646457660316425,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 3433
    },
    {
      "epoch": 3.8325892857142856,
      "grad_norm": 15.91963158421473,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 3434
    },
    {
      "epoch": 3.833705357142857,
      "grad_norm": 14.450433195341207,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 3435
    },
    {
      "epoch": 3.834821428571429,
      "grad_norm": 14.333434544917854,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3436
    },
    {
      "epoch": 3.8359375,
      "grad_norm": 13.408277536222668,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 3437
    },
    {
      "epoch": 3.837053571428571,
      "grad_norm": 15.4823698388573,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 3438
    },
    {
      "epoch": 3.838169642857143,
      "grad_norm": 12.218304642728842,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 3439
    },
    {
      "epoch": 3.8392857142857144,
      "grad_norm": 15.896041682127148,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 3440
    },
    {
      "epoch": 3.8404017857142856,
      "grad_norm": 10.37355470049727,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 3441
    },
    {
      "epoch": 3.841517857142857,
      "grad_norm": 13.555147749141563,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 3442
    },
    {
      "epoch": 3.842633928571429,
      "grad_norm": 11.99950690238254,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 3443
    },
    {
      "epoch": 3.84375,
      "grad_norm": 13.062543687752608,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 3444
    },
    {
      "epoch": 3.844866071428571,
      "grad_norm": 13.278593986252814,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 3445
    },
    {
      "epoch": 3.845982142857143,
      "grad_norm": 14.77300430661114,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 3446
    },
    {
      "epoch": 3.8470982142857144,
      "grad_norm": 14.56471605526011,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 3447
    },
    {
      "epoch": 3.8482142857142856,
      "grad_norm": 12.309818111103906,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3448
    },
    {
      "epoch": 3.849330357142857,
      "grad_norm": 12.237597140975858,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 3449
    },
    {
      "epoch": 3.850446428571429,
      "grad_norm": 11.01140438993352,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 3450
    },
    {
      "epoch": 3.8515625,
      "grad_norm": 14.014514903022587,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 3451
    },
    {
      "epoch": 3.852678571428571,
      "grad_norm": 12.99346110548808,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3452
    },
    {
      "epoch": 3.853794642857143,
      "grad_norm": 14.21650761458945,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 3453
    },
    {
      "epoch": 3.8549107142857144,
      "grad_norm": 12.602283095905873,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 3454
    },
    {
      "epoch": 3.8560267857142856,
      "grad_norm": 16.920087451085053,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 3455
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 13.868158396224286,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 3456
    },
    {
      "epoch": 3.858258928571429,
      "grad_norm": 16.087624377923078,
      "learning_rate": 2e-05,
      "loss": 1.2734,
      "step": 3457
    },
    {
      "epoch": 3.859375,
      "grad_norm": 15.817483431433581,
      "learning_rate": 2e-05,
      "loss": 1.2656,
      "step": 3458
    },
    {
      "epoch": 3.860491071428571,
      "grad_norm": 14.109895130545251,
      "learning_rate": 2e-05,
      "loss": 1.4688,
      "step": 3459
    },
    {
      "epoch": 3.861607142857143,
      "grad_norm": 16.75516220705157,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 3460
    },
    {
      "epoch": 3.8627232142857144,
      "grad_norm": 9.190762302779293,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 3461
    },
    {
      "epoch": 3.8638392857142856,
      "grad_norm": 13.439006231332277,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 3462
    },
    {
      "epoch": 3.864955357142857,
      "grad_norm": 17.87055914251305,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 3463
    },
    {
      "epoch": 3.866071428571429,
      "grad_norm": 15.345772832808313,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 3464
    },
    {
      "epoch": 3.8671875,
      "grad_norm": 15.12875292686828,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 3465
    },
    {
      "epoch": 3.868303571428571,
      "grad_norm": 15.211704061545932,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 3466
    },
    {
      "epoch": 3.869419642857143,
      "grad_norm": 17.177493088808237,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3467
    },
    {
      "epoch": 3.8705357142857144,
      "grad_norm": 13.585526572230822,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 3468
    },
    {
      "epoch": 3.8716517857142856,
      "grad_norm": 15.08897898948438,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 3469
    },
    {
      "epoch": 3.872767857142857,
      "grad_norm": 16.619524820331538,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 3470
    },
    {
      "epoch": 3.873883928571429,
      "grad_norm": 13.759371550579251,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3471
    },
    {
      "epoch": 3.875,
      "grad_norm": 15.136380682985997,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 3472
    },
    {
      "epoch": 3.876116071428571,
      "grad_norm": 13.057368332841746,
      "learning_rate": 2e-05,
      "loss": 1.3828,
      "step": 3473
    },
    {
      "epoch": 3.877232142857143,
      "grad_norm": 16.496127807801635,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 3474
    },
    {
      "epoch": 3.8783482142857144,
      "grad_norm": 10.984561264209631,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 3475
    },
    {
      "epoch": 3.8794642857142856,
      "grad_norm": 13.919976359446336,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 3476
    },
    {
      "epoch": 3.880580357142857,
      "grad_norm": 14.52085742720117,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 3477
    },
    {
      "epoch": 3.881696428571429,
      "grad_norm": 13.271950213386017,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 3478
    },
    {
      "epoch": 3.8828125,
      "grad_norm": 13.1807379559291,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3479
    },
    {
      "epoch": 3.883928571428571,
      "grad_norm": 15.178111902796962,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 3480
    },
    {
      "epoch": 3.885044642857143,
      "grad_norm": 12.126644312857298,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 3481
    },
    {
      "epoch": 3.8861607142857144,
      "grad_norm": 14.917307919995352,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 3482
    },
    {
      "epoch": 3.8872767857142856,
      "grad_norm": 14.864569802758197,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3483
    },
    {
      "epoch": 3.888392857142857,
      "grad_norm": 9.65060484059417,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 3484
    },
    {
      "epoch": 3.889508928571429,
      "grad_norm": 16.213271724646635,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 3485
    },
    {
      "epoch": 3.890625,
      "grad_norm": 13.650889287158938,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 3486
    },
    {
      "epoch": 3.891741071428571,
      "grad_norm": 13.525538788028067,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 3487
    },
    {
      "epoch": 3.892857142857143,
      "grad_norm": 13.06011000525075,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 3488
    },
    {
      "epoch": 3.8939732142857144,
      "grad_norm": 11.864834819531156,
      "learning_rate": 2e-05,
      "loss": 1.4688,
      "step": 3489
    },
    {
      "epoch": 3.8950892857142856,
      "grad_norm": 13.719100373895584,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 3490
    },
    {
      "epoch": 3.896205357142857,
      "grad_norm": 13.33069502711494,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 3491
    },
    {
      "epoch": 3.897321428571429,
      "grad_norm": 11.610863140783227,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 3492
    },
    {
      "epoch": 3.8984375,
      "grad_norm": 14.941159927741712,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 3493
    },
    {
      "epoch": 3.899553571428571,
      "grad_norm": 15.154092590915143,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 3494
    },
    {
      "epoch": 3.900669642857143,
      "grad_norm": 14.850228684574903,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 3495
    },
    {
      "epoch": 3.9017857142857144,
      "grad_norm": 12.722472166535503,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 3496
    },
    {
      "epoch": 3.9029017857142856,
      "grad_norm": 15.839585728633157,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 3497
    },
    {
      "epoch": 3.904017857142857,
      "grad_norm": 11.774777509595799,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 3498
    },
    {
      "epoch": 3.905133928571429,
      "grad_norm": 10.144957155620922,
      "learning_rate": 2e-05,
      "loss": 3.0469,
      "step": 3499
    },
    {
      "epoch": 3.90625,
      "grad_norm": 13.321222720877916,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 3500
    },
    {
      "epoch": 3.907366071428571,
      "grad_norm": 11.539821139072558,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 3501
    },
    {
      "epoch": 3.908482142857143,
      "grad_norm": 6.931409809581409,
      "learning_rate": 2e-05,
      "loss": 0.7891,
      "step": 3502
    },
    {
      "epoch": 3.9095982142857144,
      "grad_norm": 15.347614795282274,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 3503
    },
    {
      "epoch": 3.9107142857142856,
      "grad_norm": 11.922850496673405,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 3504
    },
    {
      "epoch": 3.911830357142857,
      "grad_norm": 12.289444451879314,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 3505
    },
    {
      "epoch": 3.912946428571429,
      "grad_norm": 12.408151211420714,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 3506
    },
    {
      "epoch": 3.9140625,
      "grad_norm": 14.050720814487676,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 3507
    },
    {
      "epoch": 3.915178571428571,
      "grad_norm": 15.4196857615851,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 3508
    },
    {
      "epoch": 3.916294642857143,
      "grad_norm": 12.418100914127454,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3509
    },
    {
      "epoch": 3.9174107142857144,
      "grad_norm": 13.993151707913963,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 3510
    },
    {
      "epoch": 3.9185267857142856,
      "grad_norm": 13.837734722451158,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 3511
    },
    {
      "epoch": 3.919642857142857,
      "grad_norm": 18.751975697211588,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3512
    },
    {
      "epoch": 3.920758928571429,
      "grad_norm": 14.309470020772041,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 3513
    },
    {
      "epoch": 3.921875,
      "grad_norm": 12.521143035510928,
      "learning_rate": 2e-05,
      "loss": 1.2188,
      "step": 3514
    },
    {
      "epoch": 3.922991071428571,
      "grad_norm": 14.064652054958072,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 3515
    },
    {
      "epoch": 3.924107142857143,
      "grad_norm": 15.289753473716884,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 3516
    },
    {
      "epoch": 3.9252232142857144,
      "grad_norm": 11.479952561663827,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 3517
    },
    {
      "epoch": 3.9263392857142856,
      "grad_norm": 12.040790309393383,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 3518
    },
    {
      "epoch": 3.927455357142857,
      "grad_norm": 13.413758536357566,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 3519
    },
    {
      "epoch": 3.928571428571429,
      "grad_norm": 15.52198222713568,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 3520
    },
    {
      "epoch": 3.9296875,
      "grad_norm": 13.238056016861469,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 3521
    },
    {
      "epoch": 3.930803571428571,
      "grad_norm": 13.689144042824692,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 3522
    },
    {
      "epoch": 3.931919642857143,
      "grad_norm": 11.392943760903332,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 3523
    },
    {
      "epoch": 3.9330357142857144,
      "grad_norm": 11.398011642428855,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 3524
    },
    {
      "epoch": 3.9341517857142856,
      "grad_norm": 14.656405270541772,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 3525
    },
    {
      "epoch": 3.935267857142857,
      "grad_norm": 14.389600664196333,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 3526
    },
    {
      "epoch": 3.936383928571429,
      "grad_norm": 14.029663405533846,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 3527
    },
    {
      "epoch": 3.9375,
      "grad_norm": 12.469110595302071,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 3528
    },
    {
      "epoch": 3.938616071428571,
      "grad_norm": 14.167016589586341,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 3529
    },
    {
      "epoch": 3.939732142857143,
      "grad_norm": 10.757790109625644,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 3530
    },
    {
      "epoch": 3.9408482142857144,
      "grad_norm": 13.520010007840758,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 3531
    },
    {
      "epoch": 3.9419642857142856,
      "grad_norm": 14.089316959259312,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 3532
    },
    {
      "epoch": 3.943080357142857,
      "grad_norm": 12.559337007548743,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 3533
    },
    {
      "epoch": 3.944196428571429,
      "grad_norm": 11.89442070258183,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 3534
    },
    {
      "epoch": 3.9453125,
      "grad_norm": 14.703217662858622,
      "learning_rate": 2e-05,
      "loss": 1.0156,
      "step": 3535
    },
    {
      "epoch": 3.946428571428571,
      "grad_norm": 12.399219823027384,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 3536
    },
    {
      "epoch": 3.947544642857143,
      "grad_norm": 14.949906008922648,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 3537
    },
    {
      "epoch": 3.9486607142857144,
      "grad_norm": 15.684295157671022,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 3538
    },
    {
      "epoch": 3.9497767857142856,
      "grad_norm": 14.78963989186239,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3539
    },
    {
      "epoch": 3.950892857142857,
      "grad_norm": 13.30250978618606,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3540
    },
    {
      "epoch": 3.952008928571429,
      "grad_norm": 12.784931904233641,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 3541
    },
    {
      "epoch": 3.953125,
      "grad_norm": 14.314531776728835,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 3542
    },
    {
      "epoch": 3.954241071428571,
      "grad_norm": 12.396456032991335,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 3543
    },
    {
      "epoch": 3.955357142857143,
      "grad_norm": 11.102897256790634,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 3544
    },
    {
      "epoch": 3.9564732142857144,
      "grad_norm": 14.560604516819334,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 3545
    },
    {
      "epoch": 3.9575892857142856,
      "grad_norm": 13.474801766625319,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3546
    },
    {
      "epoch": 3.958705357142857,
      "grad_norm": 12.828326749378636,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 3547
    },
    {
      "epoch": 3.959821428571429,
      "grad_norm": 14.63396638711098,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 3548
    },
    {
      "epoch": 3.9609375,
      "grad_norm": 13.301536140485599,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 3549
    },
    {
      "epoch": 3.962053571428571,
      "grad_norm": 15.413712038825523,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 3550
    },
    {
      "epoch": 3.963169642857143,
      "grad_norm": 13.55712102536526,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 3551
    },
    {
      "epoch": 3.9642857142857144,
      "grad_norm": 13.267679453212477,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 3552
    },
    {
      "epoch": 3.9654017857142856,
      "grad_norm": 13.021175727322419,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 3553
    },
    {
      "epoch": 3.966517857142857,
      "grad_norm": 9.819244729312112,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 3554
    },
    {
      "epoch": 3.967633928571429,
      "grad_norm": 15.62742416134684,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 3555
    },
    {
      "epoch": 3.96875,
      "grad_norm": 12.558071953224657,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 3556
    },
    {
      "epoch": 3.969866071428571,
      "grad_norm": 11.750986262715202,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 3557
    },
    {
      "epoch": 3.970982142857143,
      "grad_norm": 14.249264920533674,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 3558
    },
    {
      "epoch": 3.9720982142857144,
      "grad_norm": 13.045638906094384,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 3559
    },
    {
      "epoch": 3.9732142857142856,
      "grad_norm": 12.683635990301347,
      "learning_rate": 2e-05,
      "loss": 1.1094,
      "step": 3560
    },
    {
      "epoch": 3.974330357142857,
      "grad_norm": 11.517646455957273,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 3561
    },
    {
      "epoch": 3.975446428571429,
      "grad_norm": 13.218775518184517,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 3562
    },
    {
      "epoch": 3.9765625,
      "grad_norm": 14.619513931588617,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 3563
    },
    {
      "epoch": 3.977678571428571,
      "grad_norm": 12.38160476606856,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 3564
    },
    {
      "epoch": 3.978794642857143,
      "grad_norm": 14.119669966487082,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 3565
    },
    {
      "epoch": 3.9799107142857144,
      "grad_norm": 13.365708819818261,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 3566
    },
    {
      "epoch": 3.9810267857142856,
      "grad_norm": 13.562659147591454,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 3567
    },
    {
      "epoch": 3.982142857142857,
      "grad_norm": 16.939308048421506,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 3568
    },
    {
      "epoch": 3.983258928571429,
      "grad_norm": 13.933518132800636,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 3569
    },
    {
      "epoch": 3.984375,
      "grad_norm": 11.50987733471302,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 3570
    },
    {
      "epoch": 3.985491071428571,
      "grad_norm": 16.57465085181859,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 3571
    },
    {
      "epoch": 3.986607142857143,
      "grad_norm": 10.660662053765463,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 3572
    },
    {
      "epoch": 3.9877232142857144,
      "grad_norm": 11.21819691637742,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 3573
    },
    {
      "epoch": 3.9888392857142856,
      "grad_norm": 13.54753083934373,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 3574
    },
    {
      "epoch": 3.989955357142857,
      "grad_norm": 12.972096011233527,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 3575
    },
    {
      "epoch": 3.991071428571429,
      "grad_norm": 12.69495351920073,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 3576
    },
    {
      "epoch": 3.9921875,
      "grad_norm": 14.081260349560981,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 3577
    },
    {
      "epoch": 3.993303571428571,
      "grad_norm": 19.53749007863766,
      "learning_rate": 2e-05,
      "loss": 1.4297,
      "step": 3578
    },
    {
      "epoch": 3.994419642857143,
      "grad_norm": 11.654645736958388,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 3579
    },
    {
      "epoch": 3.9955357142857144,
      "grad_norm": 13.474971161329835,
      "learning_rate": 2e-05,
      "loss": 1.2812,
      "step": 3580
    },
    {
      "epoch": 3.9966517857142856,
      "grad_norm": 14.972679803220043,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 3581
    },
    {
      "epoch": 3.997767857142857,
      "grad_norm": 13.870370509173789,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 3582
    },
    {
      "epoch": 3.998883928571429,
      "grad_norm": 12.942776360344263,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 3583
    },
    {
      "epoch": 4.0,
      "grad_norm": 13.591943538581734,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 3584
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 3584,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "total_flos": 69630009802752.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
