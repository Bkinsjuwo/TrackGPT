{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 896,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0011160714285714285,
      "grad_norm": 8.714164174460775,
      "learning_rate": 2e-05,
      "loss": 3.2969,
      "step": 1
    },
    {
      "epoch": 0.002232142857142857,
      "grad_norm": 8.128486322384092,
      "learning_rate": 2e-05,
      "loss": 3.3594,
      "step": 2
    },
    {
      "epoch": 0.0033482142857142855,
      "grad_norm": 11.915371164647565,
      "learning_rate": 2e-05,
      "loss": 3.1719,
      "step": 3
    },
    {
      "epoch": 0.004464285714285714,
      "grad_norm": 11.408181156121259,
      "learning_rate": 2e-05,
      "loss": 3.1719,
      "step": 4
    },
    {
      "epoch": 0.005580357142857143,
      "grad_norm": 7.410388430792192,
      "learning_rate": 2e-05,
      "loss": 3.2656,
      "step": 5
    },
    {
      "epoch": 0.006696428571428571,
      "grad_norm": 10.210658826761904,
      "learning_rate": 2e-05,
      "loss": 3.4844,
      "step": 6
    },
    {
      "epoch": 0.0078125,
      "grad_norm": 6.2256575973149095,
      "learning_rate": 2e-05,
      "loss": 3.5312,
      "step": 7
    },
    {
      "epoch": 0.008928571428571428,
      "grad_norm": 8.226034935319733,
      "learning_rate": 2e-05,
      "loss": 3.0781,
      "step": 8
    },
    {
      "epoch": 0.010044642857142858,
      "grad_norm": 8.017552136568476,
      "learning_rate": 2e-05,
      "loss": 3.4219,
      "step": 9
    },
    {
      "epoch": 0.011160714285714286,
      "grad_norm": 9.580740289235301,
      "learning_rate": 2e-05,
      "loss": 3.6406,
      "step": 10
    },
    {
      "epoch": 0.012276785714285714,
      "grad_norm": 6.580472320930182,
      "learning_rate": 2e-05,
      "loss": 3.7969,
      "step": 11
    },
    {
      "epoch": 0.013392857142857142,
      "grad_norm": 8.950205081738448,
      "learning_rate": 2e-05,
      "loss": 3.7188,
      "step": 12
    },
    {
      "epoch": 0.014508928571428572,
      "grad_norm": 9.249130772894745,
      "learning_rate": 2e-05,
      "loss": 2.9531,
      "step": 13
    },
    {
      "epoch": 0.015625,
      "grad_norm": 12.141656431544279,
      "learning_rate": 2e-05,
      "loss": 3.0781,
      "step": 14
    },
    {
      "epoch": 0.016741071428571428,
      "grad_norm": 33.93048922623467,
      "learning_rate": 2e-05,
      "loss": 3.3125,
      "step": 15
    },
    {
      "epoch": 0.017857142857142856,
      "grad_norm": 9.337238368722058,
      "learning_rate": 2e-05,
      "loss": 3.1562,
      "step": 16
    },
    {
      "epoch": 0.018973214285714284,
      "grad_norm": 11.029510825327677,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 17
    },
    {
      "epoch": 0.020089285714285716,
      "grad_norm": 11.70375218990609,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 18
    },
    {
      "epoch": 0.021205357142857144,
      "grad_norm": 7.214853201256005,
      "learning_rate": 2e-05,
      "loss": 2.9531,
      "step": 19
    },
    {
      "epoch": 0.022321428571428572,
      "grad_norm": 6.79025276963439,
      "learning_rate": 2e-05,
      "loss": 3.0312,
      "step": 20
    },
    {
      "epoch": 0.0234375,
      "grad_norm": 6.24740592895989,
      "learning_rate": 2e-05,
      "loss": 3.1094,
      "step": 21
    },
    {
      "epoch": 0.024553571428571428,
      "grad_norm": 10.375773543428478,
      "learning_rate": 2e-05,
      "loss": 3.3906,
      "step": 22
    },
    {
      "epoch": 0.025669642857142856,
      "grad_norm": 9.753379612530864,
      "learning_rate": 2e-05,
      "loss": 3.3125,
      "step": 23
    },
    {
      "epoch": 0.026785714285714284,
      "grad_norm": 13.487060172979637,
      "learning_rate": 2e-05,
      "loss": 3.125,
      "step": 24
    },
    {
      "epoch": 0.027901785714285716,
      "grad_norm": 10.482803374502716,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 25
    },
    {
      "epoch": 0.029017857142857144,
      "grad_norm": 12.323622600485368,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 26
    },
    {
      "epoch": 0.030133928571428572,
      "grad_norm": 7.732238438481504,
      "learning_rate": 2e-05,
      "loss": 3.6875,
      "step": 27
    },
    {
      "epoch": 0.03125,
      "grad_norm": 14.585271907120244,
      "learning_rate": 2e-05,
      "loss": 3.2656,
      "step": 28
    },
    {
      "epoch": 0.03236607142857143,
      "grad_norm": 10.060856015814013,
      "learning_rate": 2e-05,
      "loss": 3.0156,
      "step": 29
    },
    {
      "epoch": 0.033482142857142856,
      "grad_norm": 10.638996642140716,
      "learning_rate": 2e-05,
      "loss": 2.9062,
      "step": 30
    },
    {
      "epoch": 0.03459821428571429,
      "grad_norm": 5.682474307743042,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 31
    },
    {
      "epoch": 0.03571428571428571,
      "grad_norm": 8.462105780228269,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 32
    },
    {
      "epoch": 0.036830357142857144,
      "grad_norm": 12.9593568887898,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 33
    },
    {
      "epoch": 0.03794642857142857,
      "grad_norm": 9.766670242434621,
      "learning_rate": 2e-05,
      "loss": 3.4375,
      "step": 34
    },
    {
      "epoch": 0.0390625,
      "grad_norm": 8.68065181492996,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 35
    },
    {
      "epoch": 0.04017857142857143,
      "grad_norm": 6.641247307035858,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 36
    },
    {
      "epoch": 0.041294642857142856,
      "grad_norm": 6.861348755022539,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 37
    },
    {
      "epoch": 0.04241071428571429,
      "grad_norm": 5.900988240530282,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 38
    },
    {
      "epoch": 0.04352678571428571,
      "grad_norm": 9.62879180130691,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 39
    },
    {
      "epoch": 0.044642857142857144,
      "grad_norm": 10.069632475943388,
      "learning_rate": 2e-05,
      "loss": 3.0156,
      "step": 40
    },
    {
      "epoch": 0.04575892857142857,
      "grad_norm": 6.399836160670785,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 41
    },
    {
      "epoch": 0.046875,
      "grad_norm": 8.154120603779646,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 42
    },
    {
      "epoch": 0.04799107142857143,
      "grad_norm": 6.540864545099026,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 43
    },
    {
      "epoch": 0.049107142857142856,
      "grad_norm": 6.179297670838394,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 44
    },
    {
      "epoch": 0.05022321428571429,
      "grad_norm": 5.9164193745863045,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 45
    },
    {
      "epoch": 0.05133928571428571,
      "grad_norm": 5.213136646124924,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 46
    },
    {
      "epoch": 0.052455357142857144,
      "grad_norm": 7.581984144528733,
      "learning_rate": 2e-05,
      "loss": 2.8438,
      "step": 47
    },
    {
      "epoch": 0.05357142857142857,
      "grad_norm": 6.2324928560410315,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 48
    },
    {
      "epoch": 0.0546875,
      "grad_norm": 4.661068382685757,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 49
    },
    {
      "epoch": 0.05580357142857143,
      "grad_norm": 5.733918148085847,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 50
    },
    {
      "epoch": 0.056919642857142856,
      "grad_norm": 6.1210780790092345,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 51
    },
    {
      "epoch": 0.05803571428571429,
      "grad_norm": 6.129837412297985,
      "learning_rate": 2e-05,
      "loss": 2.9844,
      "step": 52
    },
    {
      "epoch": 0.05915178571428571,
      "grad_norm": 6.93503430350704,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 53
    },
    {
      "epoch": 0.060267857142857144,
      "grad_norm": 12.691960155294792,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 54
    },
    {
      "epoch": 0.06138392857142857,
      "grad_norm": 5.005629787320814,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 55
    },
    {
      "epoch": 0.0625,
      "grad_norm": 5.651187530245846,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 56
    },
    {
      "epoch": 0.06361607142857142,
      "grad_norm": 4.038033895067932,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 57
    },
    {
      "epoch": 0.06473214285714286,
      "grad_norm": 6.438456401260694,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 58
    },
    {
      "epoch": 0.06584821428571429,
      "grad_norm": 5.745731504927555,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 59
    },
    {
      "epoch": 0.06696428571428571,
      "grad_norm": 6.841252472633887,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 60
    },
    {
      "epoch": 0.06808035714285714,
      "grad_norm": 5.997203136648066,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 61
    },
    {
      "epoch": 0.06919642857142858,
      "grad_norm": 5.587411571079513,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 62
    },
    {
      "epoch": 0.0703125,
      "grad_norm": 6.187783477094426,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 63
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 5.848047770729405,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 64
    },
    {
      "epoch": 0.07254464285714286,
      "grad_norm": 4.364303256188071,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 65
    },
    {
      "epoch": 0.07366071428571429,
      "grad_norm": 5.894046035424198,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 66
    },
    {
      "epoch": 0.07477678571428571,
      "grad_norm": 4.188722537348376,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 67
    },
    {
      "epoch": 0.07589285714285714,
      "grad_norm": 5.338077657329879,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 68
    },
    {
      "epoch": 0.07700892857142858,
      "grad_norm": 6.432186512205561,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 69
    },
    {
      "epoch": 0.078125,
      "grad_norm": 5.355334501508085,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 70
    },
    {
      "epoch": 0.07924107142857142,
      "grad_norm": 6.336103241273201,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 71
    },
    {
      "epoch": 0.08035714285714286,
      "grad_norm": 5.459706147832072,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 72
    },
    {
      "epoch": 0.08147321428571429,
      "grad_norm": 4.1566987094408585,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 73
    },
    {
      "epoch": 0.08258928571428571,
      "grad_norm": 5.42232549814063,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 74
    },
    {
      "epoch": 0.08370535714285714,
      "grad_norm": 4.616704349550249,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 75
    },
    {
      "epoch": 0.08482142857142858,
      "grad_norm": 5.170182091793704,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 76
    },
    {
      "epoch": 0.0859375,
      "grad_norm": 7.5025892903524865,
      "learning_rate": 2e-05,
      "loss": 2.8281,
      "step": 77
    },
    {
      "epoch": 0.08705357142857142,
      "grad_norm": 5.8911760382637555,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 78
    },
    {
      "epoch": 0.08816964285714286,
      "grad_norm": 4.211172336958261,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 79
    },
    {
      "epoch": 0.08928571428571429,
      "grad_norm": 4.876182123666695,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 80
    },
    {
      "epoch": 0.09040178571428571,
      "grad_norm": 4.880607340204337,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 81
    },
    {
      "epoch": 0.09151785714285714,
      "grad_norm": 5.675387226031149,
      "learning_rate": 2e-05,
      "loss": 2.9219,
      "step": 82
    },
    {
      "epoch": 0.09263392857142858,
      "grad_norm": 4.822244926498086,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 83
    },
    {
      "epoch": 0.09375,
      "grad_norm": 5.246993994638488,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 84
    },
    {
      "epoch": 0.09486607142857142,
      "grad_norm": 5.365437669016616,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 85
    },
    {
      "epoch": 0.09598214285714286,
      "grad_norm": 4.693012149991979,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 86
    },
    {
      "epoch": 0.09709821428571429,
      "grad_norm": 5.449327693418202,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 87
    },
    {
      "epoch": 0.09821428571428571,
      "grad_norm": 4.885869174488196,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 88
    },
    {
      "epoch": 0.09933035714285714,
      "grad_norm": 4.919588518513742,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 89
    },
    {
      "epoch": 0.10044642857142858,
      "grad_norm": 5.011515684358079,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 90
    },
    {
      "epoch": 0.1015625,
      "grad_norm": 5.192746392384502,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 91
    },
    {
      "epoch": 0.10267857142857142,
      "grad_norm": 5.744993884964599,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 92
    },
    {
      "epoch": 0.10379464285714286,
      "grad_norm": 3.800407046105842,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 93
    },
    {
      "epoch": 0.10491071428571429,
      "grad_norm": 4.238044802514492,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 94
    },
    {
      "epoch": 0.10602678571428571,
      "grad_norm": 5.07951714597911,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 95
    },
    {
      "epoch": 0.10714285714285714,
      "grad_norm": 4.780198280740559,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 96
    },
    {
      "epoch": 0.10825892857142858,
      "grad_norm": 4.272564684212395,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 97
    },
    {
      "epoch": 0.109375,
      "grad_norm": 4.726894705469157,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 98
    },
    {
      "epoch": 0.11049107142857142,
      "grad_norm": 4.96539974104562,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 99
    },
    {
      "epoch": 0.11160714285714286,
      "grad_norm": 5.043302341718162,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 100
    },
    {
      "epoch": 0.11272321428571429,
      "grad_norm": 4.547819817378021,
      "learning_rate": 2e-05,
      "loss": 2.5156,
      "step": 101
    },
    {
      "epoch": 0.11383928571428571,
      "grad_norm": 5.327671400186427,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 102
    },
    {
      "epoch": 0.11495535714285714,
      "grad_norm": 5.012937991854735,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 103
    },
    {
      "epoch": 0.11607142857142858,
      "grad_norm": 4.340149108887595,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 104
    },
    {
      "epoch": 0.1171875,
      "grad_norm": 4.956152466926903,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 105
    },
    {
      "epoch": 0.11830357142857142,
      "grad_norm": 4.785804944126185,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 106
    },
    {
      "epoch": 0.11941964285714286,
      "grad_norm": 4.431269369599007,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 107
    },
    {
      "epoch": 0.12053571428571429,
      "grad_norm": 4.7913980866313395,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 108
    },
    {
      "epoch": 0.12165178571428571,
      "grad_norm": 5.924198309153987,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 109
    },
    {
      "epoch": 0.12276785714285714,
      "grad_norm": 5.377041684494544,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 110
    },
    {
      "epoch": 0.12388392857142858,
      "grad_norm": 5.358904182996805,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 111
    },
    {
      "epoch": 0.125,
      "grad_norm": 4.979890590026488,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 112
    },
    {
      "epoch": 0.12611607142857142,
      "grad_norm": 5.186079196422482,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 113
    },
    {
      "epoch": 0.12723214285714285,
      "grad_norm": 5.129254376263673,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 114
    },
    {
      "epoch": 0.12834821428571427,
      "grad_norm": 4.767729206516721,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 115
    },
    {
      "epoch": 0.12946428571428573,
      "grad_norm": 5.573094659249564,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 116
    },
    {
      "epoch": 0.13058035714285715,
      "grad_norm": 5.347652091324301,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 117
    },
    {
      "epoch": 0.13169642857142858,
      "grad_norm": 5.421429622860902,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 118
    },
    {
      "epoch": 0.1328125,
      "grad_norm": 4.752026631762671,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 119
    },
    {
      "epoch": 0.13392857142857142,
      "grad_norm": 5.974699339998891,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 120
    },
    {
      "epoch": 0.13504464285714285,
      "grad_norm": 5.381149135432819,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 121
    },
    {
      "epoch": 0.13616071428571427,
      "grad_norm": 5.067710492577204,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 122
    },
    {
      "epoch": 0.13727678571428573,
      "grad_norm": 4.729334265765399,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 123
    },
    {
      "epoch": 0.13839285714285715,
      "grad_norm": 4.630528978500444,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 124
    },
    {
      "epoch": 0.13950892857142858,
      "grad_norm": 4.720231090368908,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 125
    },
    {
      "epoch": 0.140625,
      "grad_norm": 5.864592231775595,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 126
    },
    {
      "epoch": 0.14174107142857142,
      "grad_norm": 5.55009570482248,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 127
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 4.77610964730505,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 128
    },
    {
      "epoch": 0.14397321428571427,
      "grad_norm": 5.353534214215445,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 129
    },
    {
      "epoch": 0.14508928571428573,
      "grad_norm": 6.0802820928348424,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 130
    },
    {
      "epoch": 0.14620535714285715,
      "grad_norm": 6.287590694294681,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 131
    },
    {
      "epoch": 0.14732142857142858,
      "grad_norm": 4.910417236773405,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 132
    },
    {
      "epoch": 0.1484375,
      "grad_norm": 4.464446508360941,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 133
    },
    {
      "epoch": 0.14955357142857142,
      "grad_norm": 5.040095053261613,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 134
    },
    {
      "epoch": 0.15066964285714285,
      "grad_norm": 4.561919707363219,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 135
    },
    {
      "epoch": 0.15178571428571427,
      "grad_norm": 4.733170255165138,
      "learning_rate": 2e-05,
      "loss": 1.6719,
      "step": 136
    },
    {
      "epoch": 0.15290178571428573,
      "grad_norm": 6.65344704806108,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 137
    },
    {
      "epoch": 0.15401785714285715,
      "grad_norm": 5.356209447932266,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 138
    },
    {
      "epoch": 0.15513392857142858,
      "grad_norm": 5.3580387456693055,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 139
    },
    {
      "epoch": 0.15625,
      "grad_norm": 5.569688087950964,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 140
    },
    {
      "epoch": 0.15736607142857142,
      "grad_norm": 5.824046549502556,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 141
    },
    {
      "epoch": 0.15848214285714285,
      "grad_norm": 4.977314491783535,
      "learning_rate": 2e-05,
      "loss": 2.8906,
      "step": 142
    },
    {
      "epoch": 0.15959821428571427,
      "grad_norm": 5.063110574174531,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 143
    },
    {
      "epoch": 0.16071428571428573,
      "grad_norm": 6.339391661897655,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 144
    },
    {
      "epoch": 0.16183035714285715,
      "grad_norm": 4.582333281181453,
      "learning_rate": 2e-05,
      "loss": 2.9688,
      "step": 145
    },
    {
      "epoch": 0.16294642857142858,
      "grad_norm": 5.367377203244943,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 146
    },
    {
      "epoch": 0.1640625,
      "grad_norm": 6.680092903686295,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 147
    },
    {
      "epoch": 0.16517857142857142,
      "grad_norm": 4.472201453539309,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 148
    },
    {
      "epoch": 0.16629464285714285,
      "grad_norm": 4.642459438092517,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 149
    },
    {
      "epoch": 0.16741071428571427,
      "grad_norm": 4.61954425833527,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 150
    },
    {
      "epoch": 0.16852678571428573,
      "grad_norm": 5.9386776620981365,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 151
    },
    {
      "epoch": 0.16964285714285715,
      "grad_norm": 6.084532124228088,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 152
    },
    {
      "epoch": 0.17075892857142858,
      "grad_norm": 4.087681681776893,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 153
    },
    {
      "epoch": 0.171875,
      "grad_norm": 6.224213591288618,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 154
    },
    {
      "epoch": 0.17299107142857142,
      "grad_norm": 4.368275464215769,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 155
    },
    {
      "epoch": 0.17410714285714285,
      "grad_norm": 4.596235125350103,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 156
    },
    {
      "epoch": 0.17522321428571427,
      "grad_norm": 4.104998762834547,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 157
    },
    {
      "epoch": 0.17633928571428573,
      "grad_norm": 4.911848926113566,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 158
    },
    {
      "epoch": 0.17745535714285715,
      "grad_norm": 4.9981036728800925,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 159
    },
    {
      "epoch": 0.17857142857142858,
      "grad_norm": 5.510851467998093,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 160
    },
    {
      "epoch": 0.1796875,
      "grad_norm": 5.139795591294709,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 161
    },
    {
      "epoch": 0.18080357142857142,
      "grad_norm": 4.437112357067863,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 162
    },
    {
      "epoch": 0.18191964285714285,
      "grad_norm": 5.690554318954808,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 163
    },
    {
      "epoch": 0.18303571428571427,
      "grad_norm": 5.34529343117522,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 164
    },
    {
      "epoch": 0.18415178571428573,
      "grad_norm": 4.872261742232438,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 165
    },
    {
      "epoch": 0.18526785714285715,
      "grad_norm": 4.157230479263477,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 166
    },
    {
      "epoch": 0.18638392857142858,
      "grad_norm": 4.349506969868449,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 167
    },
    {
      "epoch": 0.1875,
      "grad_norm": 4.497870585174852,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 168
    },
    {
      "epoch": 0.18861607142857142,
      "grad_norm": 5.917971395374744,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 169
    },
    {
      "epoch": 0.18973214285714285,
      "grad_norm": 5.802441588850042,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 170
    },
    {
      "epoch": 0.19084821428571427,
      "grad_norm": 5.316338204099572,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 171
    },
    {
      "epoch": 0.19196428571428573,
      "grad_norm": 4.6337257817105915,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 172
    },
    {
      "epoch": 0.19308035714285715,
      "grad_norm": 5.544854809139442,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 173
    },
    {
      "epoch": 0.19419642857142858,
      "grad_norm": 5.0812840280191285,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 174
    },
    {
      "epoch": 0.1953125,
      "grad_norm": 5.007017664080652,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 175
    },
    {
      "epoch": 0.19642857142857142,
      "grad_norm": 5.987366589781244,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 176
    },
    {
      "epoch": 0.19754464285714285,
      "grad_norm": 4.721525528452891,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 177
    },
    {
      "epoch": 0.19866071428571427,
      "grad_norm": 5.019483872417742,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 178
    },
    {
      "epoch": 0.19977678571428573,
      "grad_norm": 4.697501312780749,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 179
    },
    {
      "epoch": 0.20089285714285715,
      "grad_norm": 6.046201115248955,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 180
    },
    {
      "epoch": 0.20200892857142858,
      "grad_norm": 5.910415472171007,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 181
    },
    {
      "epoch": 0.203125,
      "grad_norm": 5.473306389087776,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 182
    },
    {
      "epoch": 0.20424107142857142,
      "grad_norm": 4.565682727749405,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 183
    },
    {
      "epoch": 0.20535714285714285,
      "grad_norm": 6.2328227238187806,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 184
    },
    {
      "epoch": 0.20647321428571427,
      "grad_norm": 5.93364014209044,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 185
    },
    {
      "epoch": 0.20758928571428573,
      "grad_norm": 5.318288597782362,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 186
    },
    {
      "epoch": 0.20870535714285715,
      "grad_norm": 9.244094239583756,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 187
    },
    {
      "epoch": 0.20982142857142858,
      "grad_norm": 5.2493517713917885,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 188
    },
    {
      "epoch": 0.2109375,
      "grad_norm": 4.718023847907817,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 189
    },
    {
      "epoch": 0.21205357142857142,
      "grad_norm": 5.1117706451353495,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 190
    },
    {
      "epoch": 0.21316964285714285,
      "grad_norm": 6.5400312346390415,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 191
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 4.622534073536627,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 192
    },
    {
      "epoch": 0.21540178571428573,
      "grad_norm": 4.9784670189788365,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 193
    },
    {
      "epoch": 0.21651785714285715,
      "grad_norm": 5.326543381816517,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 194
    },
    {
      "epoch": 0.21763392857142858,
      "grad_norm": 4.283091419725382,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 195
    },
    {
      "epoch": 0.21875,
      "grad_norm": 4.648454954980838,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 196
    },
    {
      "epoch": 0.21986607142857142,
      "grad_norm": 4.725599795986592,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 197
    },
    {
      "epoch": 0.22098214285714285,
      "grad_norm": 4.9415366236178055,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 198
    },
    {
      "epoch": 0.22209821428571427,
      "grad_norm": 5.543879086202631,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 199
    },
    {
      "epoch": 0.22321428571428573,
      "grad_norm": 4.6114693631699915,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 200
    },
    {
      "epoch": 0.22433035714285715,
      "grad_norm": 4.455777071656654,
      "learning_rate": 2e-05,
      "loss": 2.75,
      "step": 201
    },
    {
      "epoch": 0.22544642857142858,
      "grad_norm": 5.426585916659562,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 202
    },
    {
      "epoch": 0.2265625,
      "grad_norm": 5.37508598796026,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 203
    },
    {
      "epoch": 0.22767857142857142,
      "grad_norm": 4.504463553703966,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 204
    },
    {
      "epoch": 0.22879464285714285,
      "grad_norm": 5.632933312255413,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 205
    },
    {
      "epoch": 0.22991071428571427,
      "grad_norm": 4.820290487959167,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 206
    },
    {
      "epoch": 0.23102678571428573,
      "grad_norm": 5.0591413578384525,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 207
    },
    {
      "epoch": 0.23214285714285715,
      "grad_norm": 4.90720053919676,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 208
    },
    {
      "epoch": 0.23325892857142858,
      "grad_norm": 5.584408987152432,
      "learning_rate": 2e-05,
      "loss": 2.9531,
      "step": 209
    },
    {
      "epoch": 0.234375,
      "grad_norm": 6.483344982186383,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 210
    },
    {
      "epoch": 0.23549107142857142,
      "grad_norm": 4.891662989812903,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 211
    },
    {
      "epoch": 0.23660714285714285,
      "grad_norm": 4.715300941994764,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 212
    },
    {
      "epoch": 0.23772321428571427,
      "grad_norm": 4.917011966533966,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 213
    },
    {
      "epoch": 0.23883928571428573,
      "grad_norm": 5.344035610548399,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 214
    },
    {
      "epoch": 0.23995535714285715,
      "grad_norm": 5.841296825708515,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 215
    },
    {
      "epoch": 0.24107142857142858,
      "grad_norm": 5.567403122420545,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 216
    },
    {
      "epoch": 0.2421875,
      "grad_norm": 7.110422885257681,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 217
    },
    {
      "epoch": 0.24330357142857142,
      "grad_norm": 5.8911103376815355,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 218
    },
    {
      "epoch": 0.24441964285714285,
      "grad_norm": 5.159853234572748,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 219
    },
    {
      "epoch": 0.24553571428571427,
      "grad_norm": 4.25010334015838,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 220
    },
    {
      "epoch": 0.24665178571428573,
      "grad_norm": 5.926599948903188,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 221
    },
    {
      "epoch": 0.24776785714285715,
      "grad_norm": 5.114772930689475,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 222
    },
    {
      "epoch": 0.24888392857142858,
      "grad_norm": 5.616855086548894,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 223
    },
    {
      "epoch": 0.25,
      "grad_norm": 4.884625570370895,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 224
    },
    {
      "epoch": 0.25111607142857145,
      "grad_norm": 6.505135334760109,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 225
    },
    {
      "epoch": 0.25223214285714285,
      "grad_norm": 7.250049962995516,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 226
    },
    {
      "epoch": 0.2533482142857143,
      "grad_norm": 5.640007119784958,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 227
    },
    {
      "epoch": 0.2544642857142857,
      "grad_norm": 4.8684104062266265,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 228
    },
    {
      "epoch": 0.25558035714285715,
      "grad_norm": 5.246299659371866,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 229
    },
    {
      "epoch": 0.25669642857142855,
      "grad_norm": 6.197864698442289,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 230
    },
    {
      "epoch": 0.2578125,
      "grad_norm": 5.156051058484374,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 231
    },
    {
      "epoch": 0.25892857142857145,
      "grad_norm": 5.42501117012951,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 232
    },
    {
      "epoch": 0.26004464285714285,
      "grad_norm": 6.101554902984205,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 233
    },
    {
      "epoch": 0.2611607142857143,
      "grad_norm": 4.673240223255446,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 234
    },
    {
      "epoch": 0.2622767857142857,
      "grad_norm": 5.682365722933665,
      "learning_rate": 2e-05,
      "loss": 2.4219,
      "step": 235
    },
    {
      "epoch": 0.26339285714285715,
      "grad_norm": 5.92508714921797,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 236
    },
    {
      "epoch": 0.26450892857142855,
      "grad_norm": 5.527002715594196,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 237
    },
    {
      "epoch": 0.265625,
      "grad_norm": 4.861116207642927,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 238
    },
    {
      "epoch": 0.26674107142857145,
      "grad_norm": 4.9088171632362085,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 239
    },
    {
      "epoch": 0.26785714285714285,
      "grad_norm": 5.855620912498212,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 240
    },
    {
      "epoch": 0.2689732142857143,
      "grad_norm": 5.0733099057221684,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 241
    },
    {
      "epoch": 0.2700892857142857,
      "grad_norm": 5.989621966709335,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 242
    },
    {
      "epoch": 0.27120535714285715,
      "grad_norm": 4.982177302652287,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 243
    },
    {
      "epoch": 0.27232142857142855,
      "grad_norm": 5.222638222315441,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 244
    },
    {
      "epoch": 0.2734375,
      "grad_norm": 5.154905593698026,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 245
    },
    {
      "epoch": 0.27455357142857145,
      "grad_norm": 6.035031319020624,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 246
    },
    {
      "epoch": 0.27566964285714285,
      "grad_norm": 4.8919104357200816,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 247
    },
    {
      "epoch": 0.2767857142857143,
      "grad_norm": 3.995860097179417,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 248
    },
    {
      "epoch": 0.2779017857142857,
      "grad_norm": 4.607390122853896,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 249
    },
    {
      "epoch": 0.27901785714285715,
      "grad_norm": 4.394756002697058,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 250
    },
    {
      "epoch": 0.28013392857142855,
      "grad_norm": 4.581849334008666,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 251
    },
    {
      "epoch": 0.28125,
      "grad_norm": 4.570974643313947,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 252
    },
    {
      "epoch": 0.28236607142857145,
      "grad_norm": 5.2103735682870616,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 253
    },
    {
      "epoch": 0.28348214285714285,
      "grad_norm": 6.02447386620731,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 254
    },
    {
      "epoch": 0.2845982142857143,
      "grad_norm": 4.995303310974033,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 255
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 7.167986441114688,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 256
    },
    {
      "epoch": 0.28683035714285715,
      "grad_norm": 6.524993932244898,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 257
    },
    {
      "epoch": 0.28794642857142855,
      "grad_norm": 4.578586452874299,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 258
    },
    {
      "epoch": 0.2890625,
      "grad_norm": 5.738987692650044,
      "learning_rate": 2e-05,
      "loss": 1.5234,
      "step": 259
    },
    {
      "epoch": 0.29017857142857145,
      "grad_norm": 5.721515767690259,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 260
    },
    {
      "epoch": 0.29129464285714285,
      "grad_norm": 5.5721865096830046,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 261
    },
    {
      "epoch": 0.2924107142857143,
      "grad_norm": 5.65983154115369,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 262
    },
    {
      "epoch": 0.2935267857142857,
      "grad_norm": 5.837739419844417,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 263
    },
    {
      "epoch": 0.29464285714285715,
      "grad_norm": 6.330631551983623,
      "learning_rate": 2e-05,
      "loss": 3.0312,
      "step": 264
    },
    {
      "epoch": 0.29575892857142855,
      "grad_norm": 5.489079083264709,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 265
    },
    {
      "epoch": 0.296875,
      "grad_norm": 5.480703314702249,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 266
    },
    {
      "epoch": 0.29799107142857145,
      "grad_norm": 5.41364619266832,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 267
    },
    {
      "epoch": 0.29910714285714285,
      "grad_norm": 5.152914532654801,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 268
    },
    {
      "epoch": 0.3002232142857143,
      "grad_norm": 5.470606194602912,
      "learning_rate": 2e-05,
      "loss": 1.7031,
      "step": 269
    },
    {
      "epoch": 0.3013392857142857,
      "grad_norm": 5.889479608238374,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 270
    },
    {
      "epoch": 0.30245535714285715,
      "grad_norm": 6.245747382885006,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 271
    },
    {
      "epoch": 0.30357142857142855,
      "grad_norm": 5.623342948736502,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 272
    },
    {
      "epoch": 0.3046875,
      "grad_norm": 5.90200300337235,
      "learning_rate": 2e-05,
      "loss": 2.8594,
      "step": 273
    },
    {
      "epoch": 0.30580357142857145,
      "grad_norm": 5.194870702425177,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 274
    },
    {
      "epoch": 0.30691964285714285,
      "grad_norm": 5.794914889446456,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 275
    },
    {
      "epoch": 0.3080357142857143,
      "grad_norm": 6.381987639823284,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 276
    },
    {
      "epoch": 0.3091517857142857,
      "grad_norm": 6.1834017563839225,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 277
    },
    {
      "epoch": 0.31026785714285715,
      "grad_norm": 5.170698168348872,
      "learning_rate": 2e-05,
      "loss": 2.9375,
      "step": 278
    },
    {
      "epoch": 0.31138392857142855,
      "grad_norm": 6.17602319354166,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 279
    },
    {
      "epoch": 0.3125,
      "grad_norm": 5.725733454018046,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 280
    },
    {
      "epoch": 0.31361607142857145,
      "grad_norm": 4.930488614452633,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 281
    },
    {
      "epoch": 0.31473214285714285,
      "grad_norm": 5.356077894774006,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 282
    },
    {
      "epoch": 0.3158482142857143,
      "grad_norm": 7.393537275045752,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 283
    },
    {
      "epoch": 0.3169642857142857,
      "grad_norm": 6.313266564480613,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 284
    },
    {
      "epoch": 0.31808035714285715,
      "grad_norm": 6.256324757762068,
      "learning_rate": 2e-05,
      "loss": 2.8906,
      "step": 285
    },
    {
      "epoch": 0.31919642857142855,
      "grad_norm": 6.937892129876937,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 286
    },
    {
      "epoch": 0.3203125,
      "grad_norm": 5.939261007145628,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 287
    },
    {
      "epoch": 0.32142857142857145,
      "grad_norm": 4.970244513880297,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 288
    },
    {
      "epoch": 0.32254464285714285,
      "grad_norm": 6.100740073085076,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 289
    },
    {
      "epoch": 0.3236607142857143,
      "grad_norm": 7.817134691450799,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 290
    },
    {
      "epoch": 0.3247767857142857,
      "grad_norm": 5.9876404696188645,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 291
    },
    {
      "epoch": 0.32589285714285715,
      "grad_norm": 6.7560981222037455,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 292
    },
    {
      "epoch": 0.32700892857142855,
      "grad_norm": 4.8528760131213575,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 293
    },
    {
      "epoch": 0.328125,
      "grad_norm": 6.479218529036824,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 294
    },
    {
      "epoch": 0.32924107142857145,
      "grad_norm": 5.970693461617088,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 295
    },
    {
      "epoch": 0.33035714285714285,
      "grad_norm": 5.985771874748177,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 296
    },
    {
      "epoch": 0.3314732142857143,
      "grad_norm": 7.120535499634246,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 297
    },
    {
      "epoch": 0.3325892857142857,
      "grad_norm": 5.5017586636818585,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 298
    },
    {
      "epoch": 0.33370535714285715,
      "grad_norm": 8.306176640131177,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 299
    },
    {
      "epoch": 0.33482142857142855,
      "grad_norm": 4.681558910307139,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 300
    },
    {
      "epoch": 0.3359375,
      "grad_norm": 6.403087530825947,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 301
    },
    {
      "epoch": 0.33705357142857145,
      "grad_norm": 5.738328659066505,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 302
    },
    {
      "epoch": 0.33816964285714285,
      "grad_norm": 5.202189004026315,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 303
    },
    {
      "epoch": 0.3392857142857143,
      "grad_norm": 5.012230752872492,
      "learning_rate": 2e-05,
      "loss": 1.0859,
      "step": 304
    },
    {
      "epoch": 0.3404017857142857,
      "grad_norm": 5.595198360432458,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 305
    },
    {
      "epoch": 0.34151785714285715,
      "grad_norm": 4.108237430988085,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 306
    },
    {
      "epoch": 0.34263392857142855,
      "grad_norm": 7.445207832035633,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 307
    },
    {
      "epoch": 0.34375,
      "grad_norm": 5.102228760086666,
      "learning_rate": 2e-05,
      "loss": 1.2734,
      "step": 308
    },
    {
      "epoch": 0.34486607142857145,
      "grad_norm": 5.918815693531516,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 309
    },
    {
      "epoch": 0.34598214285714285,
      "grad_norm": 6.017954914081264,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 310
    },
    {
      "epoch": 0.3470982142857143,
      "grad_norm": 6.405339743119702,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 311
    },
    {
      "epoch": 0.3482142857142857,
      "grad_norm": 5.943174245522186,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 312
    },
    {
      "epoch": 0.34933035714285715,
      "grad_norm": 6.14619126533703,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 313
    },
    {
      "epoch": 0.35044642857142855,
      "grad_norm": 4.043394727049005,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 314
    },
    {
      "epoch": 0.3515625,
      "grad_norm": 6.491212784856782,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 315
    },
    {
      "epoch": 0.35267857142857145,
      "grad_norm": 6.122075727934848,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 316
    },
    {
      "epoch": 0.35379464285714285,
      "grad_norm": 6.7001201964117225,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 317
    },
    {
      "epoch": 0.3549107142857143,
      "grad_norm": 5.246304713916152,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 318
    },
    {
      "epoch": 0.3560267857142857,
      "grad_norm": 5.38420903793464,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 319
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 5.468480386425721,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 320
    },
    {
      "epoch": 0.35825892857142855,
      "grad_norm": 6.521058901771744,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 321
    },
    {
      "epoch": 0.359375,
      "grad_norm": 5.998039052021094,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 322
    },
    {
      "epoch": 0.36049107142857145,
      "grad_norm": 5.6877215069394245,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 323
    },
    {
      "epoch": 0.36160714285714285,
      "grad_norm": 5.796934576746362,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 324
    },
    {
      "epoch": 0.3627232142857143,
      "grad_norm": 5.75865595517348,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 325
    },
    {
      "epoch": 0.3638392857142857,
      "grad_norm": 6.259569392547488,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 326
    },
    {
      "epoch": 0.36495535714285715,
      "grad_norm": 6.219423948128885,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 327
    },
    {
      "epoch": 0.36607142857142855,
      "grad_norm": 5.7428639267416335,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 328
    },
    {
      "epoch": 0.3671875,
      "grad_norm": 5.829919905472288,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 329
    },
    {
      "epoch": 0.36830357142857145,
      "grad_norm": 5.836898358141586,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 330
    },
    {
      "epoch": 0.36941964285714285,
      "grad_norm": 6.685005677280061,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 331
    },
    {
      "epoch": 0.3705357142857143,
      "grad_norm": 6.981256958896409,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 332
    },
    {
      "epoch": 0.3716517857142857,
      "grad_norm": 5.260740722332293,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 333
    },
    {
      "epoch": 0.37276785714285715,
      "grad_norm": 6.946577895491614,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 334
    },
    {
      "epoch": 0.37388392857142855,
      "grad_norm": 5.898512311305371,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 335
    },
    {
      "epoch": 0.375,
      "grad_norm": 6.6867196651542,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 336
    },
    {
      "epoch": 0.37611607142857145,
      "grad_norm": 6.2167047098151205,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 337
    },
    {
      "epoch": 0.37723214285714285,
      "grad_norm": 6.481859342806182,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 338
    },
    {
      "epoch": 0.3783482142857143,
      "grad_norm": 6.672480363971137,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 339
    },
    {
      "epoch": 0.3794642857142857,
      "grad_norm": 6.474222763479225,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 340
    },
    {
      "epoch": 0.38058035714285715,
      "grad_norm": 6.865369480979002,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 341
    },
    {
      "epoch": 0.38169642857142855,
      "grad_norm": 5.700214233178537,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 342
    },
    {
      "epoch": 0.3828125,
      "grad_norm": 4.330382195168137,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 343
    },
    {
      "epoch": 0.38392857142857145,
      "grad_norm": 5.990920890336209,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 344
    },
    {
      "epoch": 0.38504464285714285,
      "grad_norm": 7.716361785749903,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 345
    },
    {
      "epoch": 0.3861607142857143,
      "grad_norm": 5.641672105532815,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 346
    },
    {
      "epoch": 0.3872767857142857,
      "grad_norm": 6.247139794342922,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 347
    },
    {
      "epoch": 0.38839285714285715,
      "grad_norm": 4.526405318878593,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 348
    },
    {
      "epoch": 0.38950892857142855,
      "grad_norm": 5.84802183091192,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 349
    },
    {
      "epoch": 0.390625,
      "grad_norm": 6.3075716987046135,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 350
    },
    {
      "epoch": 0.39174107142857145,
      "grad_norm": 5.3962683021827855,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 351
    },
    {
      "epoch": 0.39285714285714285,
      "grad_norm": 6.859895369254169,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 352
    },
    {
      "epoch": 0.3939732142857143,
      "grad_norm": 5.985377035158306,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 353
    },
    {
      "epoch": 0.3950892857142857,
      "grad_norm": 5.410371489631165,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 354
    },
    {
      "epoch": 0.39620535714285715,
      "grad_norm": 6.665188916366339,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 355
    },
    {
      "epoch": 0.39732142857142855,
      "grad_norm": 5.391209519947215,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 356
    },
    {
      "epoch": 0.3984375,
      "grad_norm": 6.001163232692434,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 357
    },
    {
      "epoch": 0.39955357142857145,
      "grad_norm": 6.603579911502081,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 358
    },
    {
      "epoch": 0.40066964285714285,
      "grad_norm": 6.579369133187988,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 359
    },
    {
      "epoch": 0.4017857142857143,
      "grad_norm": 5.545349699122646,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 360
    },
    {
      "epoch": 0.4029017857142857,
      "grad_norm": 6.105307562591906,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 361
    },
    {
      "epoch": 0.40401785714285715,
      "grad_norm": 6.7813003407687695,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 362
    },
    {
      "epoch": 0.40513392857142855,
      "grad_norm": 7.046860244609957,
      "learning_rate": 2e-05,
      "loss": 1.5391,
      "step": 363
    },
    {
      "epoch": 0.40625,
      "grad_norm": 5.426861969461977,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 364
    },
    {
      "epoch": 0.40736607142857145,
      "grad_norm": 6.195293264878027,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 365
    },
    {
      "epoch": 0.40848214285714285,
      "grad_norm": 6.200447050861281,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 366
    },
    {
      "epoch": 0.4095982142857143,
      "grad_norm": 5.595228344398224,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 367
    },
    {
      "epoch": 0.4107142857142857,
      "grad_norm": 5.8432601289481125,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 368
    },
    {
      "epoch": 0.41183035714285715,
      "grad_norm": 6.079638208423657,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 369
    },
    {
      "epoch": 0.41294642857142855,
      "grad_norm": 5.402080857317698,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 370
    },
    {
      "epoch": 0.4140625,
      "grad_norm": 5.373155401106747,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 371
    },
    {
      "epoch": 0.41517857142857145,
      "grad_norm": 5.212789404143644,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 372
    },
    {
      "epoch": 0.41629464285714285,
      "grad_norm": 6.592401396032539,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 373
    },
    {
      "epoch": 0.4174107142857143,
      "grad_norm": 5.444279805970692,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 374
    },
    {
      "epoch": 0.4185267857142857,
      "grad_norm": 6.129844170249731,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 375
    },
    {
      "epoch": 0.41964285714285715,
      "grad_norm": 6.34720215493731,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 376
    },
    {
      "epoch": 0.42075892857142855,
      "grad_norm": 5.276766947606829,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 377
    },
    {
      "epoch": 0.421875,
      "grad_norm": 6.5468063152206115,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 378
    },
    {
      "epoch": 0.42299107142857145,
      "grad_norm": 6.590926383115588,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 379
    },
    {
      "epoch": 0.42410714285714285,
      "grad_norm": 6.499396319898452,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 380
    },
    {
      "epoch": 0.4252232142857143,
      "grad_norm": 6.024348317348521,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 381
    },
    {
      "epoch": 0.4263392857142857,
      "grad_norm": 4.818488782305947,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 382
    },
    {
      "epoch": 0.42745535714285715,
      "grad_norm": 6.097826451283656,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 383
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 5.6921555500568575,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 384
    },
    {
      "epoch": 0.4296875,
      "grad_norm": 7.504548885171974,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 385
    },
    {
      "epoch": 0.43080357142857145,
      "grad_norm": 6.243656641408445,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 386
    },
    {
      "epoch": 0.43191964285714285,
      "grad_norm": 7.082077410520427,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 387
    },
    {
      "epoch": 0.4330357142857143,
      "grad_norm": 6.263167564970838,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 388
    },
    {
      "epoch": 0.4341517857142857,
      "grad_norm": 5.9703564545701795,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 389
    },
    {
      "epoch": 0.43526785714285715,
      "grad_norm": 6.70410265204994,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 390
    },
    {
      "epoch": 0.43638392857142855,
      "grad_norm": 5.855712967355312,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 391
    },
    {
      "epoch": 0.4375,
      "grad_norm": 4.492800181110915,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 392
    },
    {
      "epoch": 0.43861607142857145,
      "grad_norm": 6.5544548419871544,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 393
    },
    {
      "epoch": 0.43973214285714285,
      "grad_norm": 7.247362772010449,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 394
    },
    {
      "epoch": 0.4408482142857143,
      "grad_norm": 5.884066207580705,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 395
    },
    {
      "epoch": 0.4419642857142857,
      "grad_norm": 6.8943334875672315,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 396
    },
    {
      "epoch": 0.44308035714285715,
      "grad_norm": 6.864360297879285,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 397
    },
    {
      "epoch": 0.44419642857142855,
      "grad_norm": 6.140238793237744,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 398
    },
    {
      "epoch": 0.4453125,
      "grad_norm": 5.3291157336995285,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 399
    },
    {
      "epoch": 0.44642857142857145,
      "grad_norm": 5.598887102514742,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 400
    },
    {
      "epoch": 0.44754464285714285,
      "grad_norm": 8.286684196049563,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 401
    },
    {
      "epoch": 0.4486607142857143,
      "grad_norm": 5.788371395199885,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 402
    },
    {
      "epoch": 0.4497767857142857,
      "grad_norm": 5.244714959085075,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 403
    },
    {
      "epoch": 0.45089285714285715,
      "grad_norm": 7.9648950652684505,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 404
    },
    {
      "epoch": 0.45200892857142855,
      "grad_norm": 5.840505536000766,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 405
    },
    {
      "epoch": 0.453125,
      "grad_norm": 5.325114943786698,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 406
    },
    {
      "epoch": 0.45424107142857145,
      "grad_norm": 6.860372358841779,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 407
    },
    {
      "epoch": 0.45535714285714285,
      "grad_norm": 5.283241798012223,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 408
    },
    {
      "epoch": 0.4564732142857143,
      "grad_norm": 6.14959717966234,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 409
    },
    {
      "epoch": 0.4575892857142857,
      "grad_norm": 6.778686519742661,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 410
    },
    {
      "epoch": 0.45870535714285715,
      "grad_norm": 7.232801883524477,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 411
    },
    {
      "epoch": 0.45982142857142855,
      "grad_norm": 6.145644586883981,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 412
    },
    {
      "epoch": 0.4609375,
      "grad_norm": 5.898979644439567,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 413
    },
    {
      "epoch": 0.46205357142857145,
      "grad_norm": 5.569789972706816,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 414
    },
    {
      "epoch": 0.46316964285714285,
      "grad_norm": 5.045999108828418,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 415
    },
    {
      "epoch": 0.4642857142857143,
      "grad_norm": 4.894099241373748,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 416
    },
    {
      "epoch": 0.4654017857142857,
      "grad_norm": 6.449555308330442,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 417
    },
    {
      "epoch": 0.46651785714285715,
      "grad_norm": 6.093921261346725,
      "learning_rate": 2e-05,
      "loss": 2.6562,
      "step": 418
    },
    {
      "epoch": 0.46763392857142855,
      "grad_norm": 5.105227635925224,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 419
    },
    {
      "epoch": 0.46875,
      "grad_norm": 4.195421234821704,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 420
    },
    {
      "epoch": 0.46986607142857145,
      "grad_norm": 5.84584469759069,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 421
    },
    {
      "epoch": 0.47098214285714285,
      "grad_norm": 7.367747184371759,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 422
    },
    {
      "epoch": 0.4720982142857143,
      "grad_norm": 6.056601658643672,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 423
    },
    {
      "epoch": 0.4732142857142857,
      "grad_norm": 6.8024343216931245,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 424
    },
    {
      "epoch": 0.47433035714285715,
      "grad_norm": 7.195987718691981,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 425
    },
    {
      "epoch": 0.47544642857142855,
      "grad_norm": 7.293579492177266,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 426
    },
    {
      "epoch": 0.4765625,
      "grad_norm": 6.210461453096692,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 427
    },
    {
      "epoch": 0.47767857142857145,
      "grad_norm": 7.465061823243178,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 428
    },
    {
      "epoch": 0.47879464285714285,
      "grad_norm": 6.780365154528446,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 429
    },
    {
      "epoch": 0.4799107142857143,
      "grad_norm": 5.973983523759283,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 430
    },
    {
      "epoch": 0.4810267857142857,
      "grad_norm": 7.040120640539666,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 431
    },
    {
      "epoch": 0.48214285714285715,
      "grad_norm": 5.181417951339893,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 432
    },
    {
      "epoch": 0.48325892857142855,
      "grad_norm": 9.015996162493627,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 433
    },
    {
      "epoch": 0.484375,
      "grad_norm": 6.086607805504361,
      "learning_rate": 2e-05,
      "loss": 1.4688,
      "step": 434
    },
    {
      "epoch": 0.48549107142857145,
      "grad_norm": 7.675171980234482,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 435
    },
    {
      "epoch": 0.48660714285714285,
      "grad_norm": 7.196074650717554,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 436
    },
    {
      "epoch": 0.4877232142857143,
      "grad_norm": 7.247741196750318,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 437
    },
    {
      "epoch": 0.4888392857142857,
      "grad_norm": 7.079544525807087,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 438
    },
    {
      "epoch": 0.48995535714285715,
      "grad_norm": 5.821062052032175,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 439
    },
    {
      "epoch": 0.49107142857142855,
      "grad_norm": 7.621067196834796,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 440
    },
    {
      "epoch": 0.4921875,
      "grad_norm": 6.191078994075534,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 441
    },
    {
      "epoch": 0.49330357142857145,
      "grad_norm": 7.656098911630931,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 442
    },
    {
      "epoch": 0.49441964285714285,
      "grad_norm": 6.580625077145156,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 443
    },
    {
      "epoch": 0.4955357142857143,
      "grad_norm": 5.615320082987715,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 444
    },
    {
      "epoch": 0.4966517857142857,
      "grad_norm": 5.540945424055651,
      "learning_rate": 2e-05,
      "loss": 2.8594,
      "step": 445
    },
    {
      "epoch": 0.49776785714285715,
      "grad_norm": 7.179715882567501,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 446
    },
    {
      "epoch": 0.49888392857142855,
      "grad_norm": 6.506292836312854,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 447
    },
    {
      "epoch": 0.5,
      "grad_norm": 10.789402229886935,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 448
    },
    {
      "epoch": 0.5011160714285714,
      "grad_norm": 6.1447604043094195,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 449
    },
    {
      "epoch": 0.5022321428571429,
      "grad_norm": 8.116584067649637,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 450
    },
    {
      "epoch": 0.5033482142857143,
      "grad_norm": 5.982471564908289,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 451
    },
    {
      "epoch": 0.5044642857142857,
      "grad_norm": 7.097930942452871,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 452
    },
    {
      "epoch": 0.5055803571428571,
      "grad_norm": 5.4296084797052,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 453
    },
    {
      "epoch": 0.5066964285714286,
      "grad_norm": 6.182447680161965,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 454
    },
    {
      "epoch": 0.5078125,
      "grad_norm": 6.603200965927261,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 455
    },
    {
      "epoch": 0.5089285714285714,
      "grad_norm": 7.816722907344761,
      "learning_rate": 2e-05,
      "loss": 1.4922,
      "step": 456
    },
    {
      "epoch": 0.5100446428571429,
      "grad_norm": 7.048079859801893,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 457
    },
    {
      "epoch": 0.5111607142857143,
      "grad_norm": 8.243049639406017,
      "learning_rate": 2e-05,
      "loss": 1.4609,
      "step": 458
    },
    {
      "epoch": 0.5122767857142857,
      "grad_norm": 6.659956671556887,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 459
    },
    {
      "epoch": 0.5133928571428571,
      "grad_norm": 6.126250657764653,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 460
    },
    {
      "epoch": 0.5145089285714286,
      "grad_norm": 6.124736043291105,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 461
    },
    {
      "epoch": 0.515625,
      "grad_norm": 6.371795427964414,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 462
    },
    {
      "epoch": 0.5167410714285714,
      "grad_norm": 6.476056901849557,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 463
    },
    {
      "epoch": 0.5178571428571429,
      "grad_norm": 7.232511118933011,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 464
    },
    {
      "epoch": 0.5189732142857143,
      "grad_norm": 17.891802830408736,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 465
    },
    {
      "epoch": 0.5200892857142857,
      "grad_norm": 6.473444054020356,
      "learning_rate": 2e-05,
      "loss": 1.5938,
      "step": 466
    },
    {
      "epoch": 0.5212053571428571,
      "grad_norm": 7.4946040906703475,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 467
    },
    {
      "epoch": 0.5223214285714286,
      "grad_norm": 7.517101274756477,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 468
    },
    {
      "epoch": 0.5234375,
      "grad_norm": 5.484951120052039,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 469
    },
    {
      "epoch": 0.5245535714285714,
      "grad_norm": 5.403223141452,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 470
    },
    {
      "epoch": 0.5256696428571429,
      "grad_norm": 6.032150457320694,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 471
    },
    {
      "epoch": 0.5267857142857143,
      "grad_norm": 6.908303312738406,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 472
    },
    {
      "epoch": 0.5279017857142857,
      "grad_norm": 6.01720104010625,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 473
    },
    {
      "epoch": 0.5290178571428571,
      "grad_norm": 5.289694474262655,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 474
    },
    {
      "epoch": 0.5301339285714286,
      "grad_norm": 6.054021005714556,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 475
    },
    {
      "epoch": 0.53125,
      "grad_norm": 7.315500183203906,
      "learning_rate": 2e-05,
      "loss": 2.8906,
      "step": 476
    },
    {
      "epoch": 0.5323660714285714,
      "grad_norm": 5.959600683071562,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 477
    },
    {
      "epoch": 0.5334821428571429,
      "grad_norm": 7.471727914337743,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 478
    },
    {
      "epoch": 0.5345982142857143,
      "grad_norm": 5.418746173238664,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 479
    },
    {
      "epoch": 0.5357142857142857,
      "grad_norm": 6.576982561255422,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 480
    },
    {
      "epoch": 0.5368303571428571,
      "grad_norm": 7.05482827603986,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 481
    },
    {
      "epoch": 0.5379464285714286,
      "grad_norm": 6.775847449417505,
      "learning_rate": 2e-05,
      "loss": 1.5781,
      "step": 482
    },
    {
      "epoch": 0.5390625,
      "grad_norm": 6.810206150635688,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 483
    },
    {
      "epoch": 0.5401785714285714,
      "grad_norm": 5.231155553741888,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 484
    },
    {
      "epoch": 0.5412946428571429,
      "grad_norm": 5.5019347897277235,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 485
    },
    {
      "epoch": 0.5424107142857143,
      "grad_norm": 7.681471626862553,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 486
    },
    {
      "epoch": 0.5435267857142857,
      "grad_norm": 7.045405885975734,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 487
    },
    {
      "epoch": 0.5446428571428571,
      "grad_norm": 7.22107619181274,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 488
    },
    {
      "epoch": 0.5457589285714286,
      "grad_norm": 6.758795494377998,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 489
    },
    {
      "epoch": 0.546875,
      "grad_norm": 7.046430042982425,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 490
    },
    {
      "epoch": 0.5479910714285714,
      "grad_norm": 7.555019298129235,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 491
    },
    {
      "epoch": 0.5491071428571429,
      "grad_norm": 21.320133622636387,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 492
    },
    {
      "epoch": 0.5502232142857143,
      "grad_norm": 5.647300531352911,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 493
    },
    {
      "epoch": 0.5513392857142857,
      "grad_norm": 7.457379278094384,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 494
    },
    {
      "epoch": 0.5524553571428571,
      "grad_norm": 6.462934028920638,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 495
    },
    {
      "epoch": 0.5535714285714286,
      "grad_norm": 6.832618514429236,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 496
    },
    {
      "epoch": 0.5546875,
      "grad_norm": 6.924903099903694,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 497
    },
    {
      "epoch": 0.5558035714285714,
      "grad_norm": 7.745826770596821,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 498
    },
    {
      "epoch": 0.5569196428571429,
      "grad_norm": 6.476621917596814,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 499
    },
    {
      "epoch": 0.5580357142857143,
      "grad_norm": 6.37764145462918,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 500
    },
    {
      "epoch": 0.5591517857142857,
      "grad_norm": 7.287321350753652,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 501
    },
    {
      "epoch": 0.5602678571428571,
      "grad_norm": 7.544470055865645,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 502
    },
    {
      "epoch": 0.5613839285714286,
      "grad_norm": 5.433592106725318,
      "learning_rate": 2e-05,
      "loss": 2.9219,
      "step": 503
    },
    {
      "epoch": 0.5625,
      "grad_norm": 7.33444143982492,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 504
    },
    {
      "epoch": 0.5636160714285714,
      "grad_norm": 5.713758572542107,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 505
    },
    {
      "epoch": 0.5647321428571429,
      "grad_norm": 6.707458073683937,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 506
    },
    {
      "epoch": 0.5658482142857143,
      "grad_norm": 6.803334840536561,
      "learning_rate": 2e-05,
      "loss": 1.8047,
      "step": 507
    },
    {
      "epoch": 0.5669642857142857,
      "grad_norm": 6.693282613415061,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 508
    },
    {
      "epoch": 0.5680803571428571,
      "grad_norm": 7.0015250940395095,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 509
    },
    {
      "epoch": 0.5691964285714286,
      "grad_norm": 7.398563117347831,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 510
    },
    {
      "epoch": 0.5703125,
      "grad_norm": 5.926340025577637,
      "learning_rate": 2e-05,
      "loss": 1.6172,
      "step": 511
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 7.858769337353693,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 512
    },
    {
      "epoch": 0.5725446428571429,
      "grad_norm": 9.15600071934991,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 513
    },
    {
      "epoch": 0.5736607142857143,
      "grad_norm": 8.089059350747743,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 514
    },
    {
      "epoch": 0.5747767857142857,
      "grad_norm": 6.603976613525273,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 515
    },
    {
      "epoch": 0.5758928571428571,
      "grad_norm": 6.785829823358593,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 516
    },
    {
      "epoch": 0.5770089285714286,
      "grad_norm": 6.787069524523533,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 517
    },
    {
      "epoch": 0.578125,
      "grad_norm": 7.404594138145085,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 518
    },
    {
      "epoch": 0.5792410714285714,
      "grad_norm": 5.197338671742264,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 519
    },
    {
      "epoch": 0.5803571428571429,
      "grad_norm": 6.503093819456712,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 520
    },
    {
      "epoch": 0.5814732142857143,
      "grad_norm": 7.234193494894187,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 521
    },
    {
      "epoch": 0.5825892857142857,
      "grad_norm": 6.382905598785433,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 522
    },
    {
      "epoch": 0.5837053571428571,
      "grad_norm": 6.259005172333159,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 523
    },
    {
      "epoch": 0.5848214285714286,
      "grad_norm": 7.500613290315144,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 524
    },
    {
      "epoch": 0.5859375,
      "grad_norm": 6.752121141840796,
      "learning_rate": 2e-05,
      "loss": 2.8438,
      "step": 525
    },
    {
      "epoch": 0.5870535714285714,
      "grad_norm": 8.674907180311699,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 526
    },
    {
      "epoch": 0.5881696428571429,
      "grad_norm": 6.230415798326659,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 527
    },
    {
      "epoch": 0.5892857142857143,
      "grad_norm": 7.473396496556749,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 528
    },
    {
      "epoch": 0.5904017857142857,
      "grad_norm": 7.0905655610625,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 529
    },
    {
      "epoch": 0.5915178571428571,
      "grad_norm": 7.547132878917807,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 530
    },
    {
      "epoch": 0.5926339285714286,
      "grad_norm": 7.368677873502134,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 531
    },
    {
      "epoch": 0.59375,
      "grad_norm": 7.629273979369373,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 532
    },
    {
      "epoch": 0.5948660714285714,
      "grad_norm": 6.343202035930694,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 533
    },
    {
      "epoch": 0.5959821428571429,
      "grad_norm": 7.113872410593122,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 534
    },
    {
      "epoch": 0.5970982142857143,
      "grad_norm": 6.861135827815423,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 535
    },
    {
      "epoch": 0.5982142857142857,
      "grad_norm": 8.750357965847202,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 536
    },
    {
      "epoch": 0.5993303571428571,
      "grad_norm": 6.641071597773124,
      "learning_rate": 2e-05,
      "loss": 2.6094,
      "step": 537
    },
    {
      "epoch": 0.6004464285714286,
      "grad_norm": 6.5215686581069745,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 538
    },
    {
      "epoch": 0.6015625,
      "grad_norm": 7.165276449631838,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 539
    },
    {
      "epoch": 0.6026785714285714,
      "grad_norm": 7.737082160252734,
      "learning_rate": 2e-05,
      "loss": 1.6641,
      "step": 540
    },
    {
      "epoch": 0.6037946428571429,
      "grad_norm": 6.885279340195057,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 541
    },
    {
      "epoch": 0.6049107142857143,
      "grad_norm": 7.257508557159585,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 542
    },
    {
      "epoch": 0.6060267857142857,
      "grad_norm": 7.391386065709816,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 543
    },
    {
      "epoch": 0.6071428571428571,
      "grad_norm": 7.137488746959001,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 544
    },
    {
      "epoch": 0.6082589285714286,
      "grad_norm": 7.748661890383331,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 545
    },
    {
      "epoch": 0.609375,
      "grad_norm": 6.582130996381901,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 546
    },
    {
      "epoch": 0.6104910714285714,
      "grad_norm": 7.0629110368915695,
      "learning_rate": 2e-05,
      "loss": 1.5547,
      "step": 547
    },
    {
      "epoch": 0.6116071428571429,
      "grad_norm": 6.734276089021543,
      "learning_rate": 2e-05,
      "loss": 2.7344,
      "step": 548
    },
    {
      "epoch": 0.6127232142857143,
      "grad_norm": 5.535353287322365,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 549
    },
    {
      "epoch": 0.6138392857142857,
      "grad_norm": 6.178656547762734,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 550
    },
    {
      "epoch": 0.6149553571428571,
      "grad_norm": 7.039181863661586,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 551
    },
    {
      "epoch": 0.6160714285714286,
      "grad_norm": 7.165958925195559,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 552
    },
    {
      "epoch": 0.6171875,
      "grad_norm": 7.368164764534582,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 553
    },
    {
      "epoch": 0.6183035714285714,
      "grad_norm": 8.243520899076342,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 554
    },
    {
      "epoch": 0.6194196428571429,
      "grad_norm": 7.486382789499762,
      "learning_rate": 2e-05,
      "loss": 2.7031,
      "step": 555
    },
    {
      "epoch": 0.6205357142857143,
      "grad_norm": 6.640119412702982,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 556
    },
    {
      "epoch": 0.6216517857142857,
      "grad_norm": 7.786718397435241,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 557
    },
    {
      "epoch": 0.6227678571428571,
      "grad_norm": 8.481859862736677,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 558
    },
    {
      "epoch": 0.6238839285714286,
      "grad_norm": 7.27861097670762,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 559
    },
    {
      "epoch": 0.625,
      "grad_norm": 7.574828353868911,
      "learning_rate": 2e-05,
      "loss": 1.6953,
      "step": 560
    },
    {
      "epoch": 0.6261160714285714,
      "grad_norm": 6.6340484097224355,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 561
    },
    {
      "epoch": 0.6272321428571429,
      "grad_norm": 6.500167685041858,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 562
    },
    {
      "epoch": 0.6283482142857143,
      "grad_norm": 6.1237783123238545,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 563
    },
    {
      "epoch": 0.6294642857142857,
      "grad_norm": 5.12880085763775,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 564
    },
    {
      "epoch": 0.6305803571428571,
      "grad_norm": 8.077846784437718,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 565
    },
    {
      "epoch": 0.6316964285714286,
      "grad_norm": 7.850332157185584,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 566
    },
    {
      "epoch": 0.6328125,
      "grad_norm": 6.525401122005646,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 567
    },
    {
      "epoch": 0.6339285714285714,
      "grad_norm": 6.828137853245754,
      "learning_rate": 2e-05,
      "loss": 3.1094,
      "step": 568
    },
    {
      "epoch": 0.6350446428571429,
      "grad_norm": 6.933946469483155,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 569
    },
    {
      "epoch": 0.6361607142857143,
      "grad_norm": 6.4225118553788745,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 570
    },
    {
      "epoch": 0.6372767857142857,
      "grad_norm": 9.055079622552078,
      "learning_rate": 2e-05,
      "loss": 1.4844,
      "step": 571
    },
    {
      "epoch": 0.6383928571428571,
      "grad_norm": 6.9392919932494745,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 572
    },
    {
      "epoch": 0.6395089285714286,
      "grad_norm": 6.714547797727804,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 573
    },
    {
      "epoch": 0.640625,
      "grad_norm": 7.462917953744773,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 574
    },
    {
      "epoch": 0.6417410714285714,
      "grad_norm": 6.019678968935781,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 575
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 5.319139837355859,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 576
    },
    {
      "epoch": 0.6439732142857143,
      "grad_norm": 6.890259971829131,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 577
    },
    {
      "epoch": 0.6450892857142857,
      "grad_norm": 6.343981202862711,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 578
    },
    {
      "epoch": 0.6462053571428571,
      "grad_norm": 6.687299925702872,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 579
    },
    {
      "epoch": 0.6473214285714286,
      "grad_norm": 6.654496776350769,
      "learning_rate": 2e-05,
      "loss": 3.0938,
      "step": 580
    },
    {
      "epoch": 0.6484375,
      "grad_norm": 7.05216497002948,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 581
    },
    {
      "epoch": 0.6495535714285714,
      "grad_norm": 7.185968454340757,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 582
    },
    {
      "epoch": 0.6506696428571429,
      "grad_norm": 6.925185750341311,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 583
    },
    {
      "epoch": 0.6517857142857143,
      "grad_norm": 7.231674884097017,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 584
    },
    {
      "epoch": 0.6529017857142857,
      "grad_norm": 6.345907813514099,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 585
    },
    {
      "epoch": 0.6540178571428571,
      "grad_norm": 7.886918775010117,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 586
    },
    {
      "epoch": 0.6551339285714286,
      "grad_norm": 6.24959111035519,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 587
    },
    {
      "epoch": 0.65625,
      "grad_norm": 6.796481894674375,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 588
    },
    {
      "epoch": 0.6573660714285714,
      "grad_norm": 7.819064229184331,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 589
    },
    {
      "epoch": 0.6584821428571429,
      "grad_norm": 5.8413055916330165,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 590
    },
    {
      "epoch": 0.6595982142857143,
      "grad_norm": 8.268594819742033,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 591
    },
    {
      "epoch": 0.6607142857142857,
      "grad_norm": 7.8517082476443445,
      "learning_rate": 2e-05,
      "loss": 2.375,
      "step": 592
    },
    {
      "epoch": 0.6618303571428571,
      "grad_norm": 7.3243181307788925,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 593
    },
    {
      "epoch": 0.6629464285714286,
      "grad_norm": 7.396669674394314,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 594
    },
    {
      "epoch": 0.6640625,
      "grad_norm": 6.486828848442188,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 595
    },
    {
      "epoch": 0.6651785714285714,
      "grad_norm": 9.034632567352785,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 596
    },
    {
      "epoch": 0.6662946428571429,
      "grad_norm": 8.297827583814112,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 597
    },
    {
      "epoch": 0.6674107142857143,
      "grad_norm": 8.69103226112985,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 598
    },
    {
      "epoch": 0.6685267857142857,
      "grad_norm": 7.847905503198117,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 599
    },
    {
      "epoch": 0.6696428571428571,
      "grad_norm": 11.909023856182442,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 600
    },
    {
      "epoch": 0.6707589285714286,
      "grad_norm": 8.049127314411349,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 601
    },
    {
      "epoch": 0.671875,
      "grad_norm": 6.093627224198067,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 602
    },
    {
      "epoch": 0.6729910714285714,
      "grad_norm": 7.460176050458266,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 603
    },
    {
      "epoch": 0.6741071428571429,
      "grad_norm": 8.680047266521463,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 604
    },
    {
      "epoch": 0.6752232142857143,
      "grad_norm": 6.449522781940608,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 605
    },
    {
      "epoch": 0.6763392857142857,
      "grad_norm": 6.6233250634896566,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 606
    },
    {
      "epoch": 0.6774553571428571,
      "grad_norm": 7.827692141312697,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 607
    },
    {
      "epoch": 0.6785714285714286,
      "grad_norm": 13.597371567260781,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 608
    },
    {
      "epoch": 0.6796875,
      "grad_norm": 7.578447552180426,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 609
    },
    {
      "epoch": 0.6808035714285714,
      "grad_norm": 8.338432520821854,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 610
    },
    {
      "epoch": 0.6819196428571429,
      "grad_norm": 7.317481846904897,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 611
    },
    {
      "epoch": 0.6830357142857143,
      "grad_norm": 7.362421322426254,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 612
    },
    {
      "epoch": 0.6841517857142857,
      "grad_norm": 5.206645099891276,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 613
    },
    {
      "epoch": 0.6852678571428571,
      "grad_norm": 6.530204837885705,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 614
    },
    {
      "epoch": 0.6863839285714286,
      "grad_norm": 8.563800334057776,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 615
    },
    {
      "epoch": 0.6875,
      "grad_norm": 6.337579807856006,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 616
    },
    {
      "epoch": 0.6886160714285714,
      "grad_norm": 7.741881110306949,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 617
    },
    {
      "epoch": 0.6897321428571429,
      "grad_norm": 7.215246397096221,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 618
    },
    {
      "epoch": 0.6908482142857143,
      "grad_norm": 5.124279706126531,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 619
    },
    {
      "epoch": 0.6919642857142857,
      "grad_norm": 8.702251162301915,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 620
    },
    {
      "epoch": 0.6930803571428571,
      "grad_norm": 8.414264255254512,
      "learning_rate": 2e-05,
      "loss": 1.8359,
      "step": 621
    },
    {
      "epoch": 0.6941964285714286,
      "grad_norm": 6.884341711234197,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 622
    },
    {
      "epoch": 0.6953125,
      "grad_norm": 7.441513791789359,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 623
    },
    {
      "epoch": 0.6964285714285714,
      "grad_norm": 7.0293900375822265,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 624
    },
    {
      "epoch": 0.6975446428571429,
      "grad_norm": 7.239744578628313,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 625
    },
    {
      "epoch": 0.6986607142857143,
      "grad_norm": 8.200515413058048,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 626
    },
    {
      "epoch": 0.6997767857142857,
      "grad_norm": 7.54549419799597,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 627
    },
    {
      "epoch": 0.7008928571428571,
      "grad_norm": 7.369938171985584,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 628
    },
    {
      "epoch": 0.7020089285714286,
      "grad_norm": 7.692149336453549,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 629
    },
    {
      "epoch": 0.703125,
      "grad_norm": 8.114366300501242,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 630
    },
    {
      "epoch": 0.7042410714285714,
      "grad_norm": 6.909344860456727,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 631
    },
    {
      "epoch": 0.7053571428571429,
      "grad_norm": 6.626275043118414,
      "learning_rate": 2e-05,
      "loss": 2.4688,
      "step": 632
    },
    {
      "epoch": 0.7064732142857143,
      "grad_norm": 9.22769722379324,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 633
    },
    {
      "epoch": 0.7075892857142857,
      "grad_norm": 7.802051115433907,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 634
    },
    {
      "epoch": 0.7087053571428571,
      "grad_norm": 8.944644090925104,
      "learning_rate": 2e-05,
      "loss": 1.6562,
      "step": 635
    },
    {
      "epoch": 0.7098214285714286,
      "grad_norm": 6.92652515600993,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 636
    },
    {
      "epoch": 0.7109375,
      "grad_norm": 6.812532015268174,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 637
    },
    {
      "epoch": 0.7120535714285714,
      "grad_norm": 6.894567142747653,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 638
    },
    {
      "epoch": 0.7131696428571429,
      "grad_norm": 8.09618501611102,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 639
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 8.48950473722746,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 640
    },
    {
      "epoch": 0.7154017857142857,
      "grad_norm": 6.859688482929468,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 641
    },
    {
      "epoch": 0.7165178571428571,
      "grad_norm": 8.190078940248425,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 642
    },
    {
      "epoch": 0.7176339285714286,
      "grad_norm": 6.43415608795316,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 643
    },
    {
      "epoch": 0.71875,
      "grad_norm": 7.264083390180525,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 644
    },
    {
      "epoch": 0.7198660714285714,
      "grad_norm": 6.197882300103818,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 645
    },
    {
      "epoch": 0.7209821428571429,
      "grad_norm": 8.66468277840732,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 646
    },
    {
      "epoch": 0.7220982142857143,
      "grad_norm": 8.189051036002342,
      "learning_rate": 2e-05,
      "loss": 2.7969,
      "step": 647
    },
    {
      "epoch": 0.7232142857142857,
      "grad_norm": 5.863270120115275,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 648
    },
    {
      "epoch": 0.7243303571428571,
      "grad_norm": 7.5682111696073076,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 649
    },
    {
      "epoch": 0.7254464285714286,
      "grad_norm": 6.75854755331268,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 650
    },
    {
      "epoch": 0.7265625,
      "grad_norm": 6.97848641724375,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 651
    },
    {
      "epoch": 0.7276785714285714,
      "grad_norm": 6.518540836545243,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 652
    },
    {
      "epoch": 0.7287946428571429,
      "grad_norm": 8.39757695297506,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 653
    },
    {
      "epoch": 0.7299107142857143,
      "grad_norm": 7.002154417262316,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 654
    },
    {
      "epoch": 0.7310267857142857,
      "grad_norm": 7.652785148280761,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 655
    },
    {
      "epoch": 0.7321428571428571,
      "grad_norm": 7.58861480616225,
      "learning_rate": 2e-05,
      "loss": 1.8828,
      "step": 656
    },
    {
      "epoch": 0.7332589285714286,
      "grad_norm": 7.014376724110922,
      "learning_rate": 2e-05,
      "loss": 1.6484,
      "step": 657
    },
    {
      "epoch": 0.734375,
      "grad_norm": 6.139948883690198,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 658
    },
    {
      "epoch": 0.7354910714285714,
      "grad_norm": 7.439759946247817,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 659
    },
    {
      "epoch": 0.7366071428571429,
      "grad_norm": 6.8249113237600145,
      "learning_rate": 2e-05,
      "loss": 1.5859,
      "step": 660
    },
    {
      "epoch": 0.7377232142857143,
      "grad_norm": 6.914146794957794,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 661
    },
    {
      "epoch": 0.7388392857142857,
      "grad_norm": 6.837275664608894,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 662
    },
    {
      "epoch": 0.7399553571428571,
      "grad_norm": 7.598402800495714,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 663
    },
    {
      "epoch": 0.7410714285714286,
      "grad_norm": 6.403469630272328,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 664
    },
    {
      "epoch": 0.7421875,
      "grad_norm": 6.00450945960009,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 665
    },
    {
      "epoch": 0.7433035714285714,
      "grad_norm": 8.037821345312222,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 666
    },
    {
      "epoch": 0.7444196428571429,
      "grad_norm": 6.6797290359445745,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 667
    },
    {
      "epoch": 0.7455357142857143,
      "grad_norm": 7.009173429277118,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 668
    },
    {
      "epoch": 0.7466517857142857,
      "grad_norm": 6.524823752456167,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 669
    },
    {
      "epoch": 0.7477678571428571,
      "grad_norm": 7.637059834023824,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 670
    },
    {
      "epoch": 0.7488839285714286,
      "grad_norm": 8.49148476677469,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 671
    },
    {
      "epoch": 0.75,
      "grad_norm": 8.338162586372338,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 672
    },
    {
      "epoch": 0.7511160714285714,
      "grad_norm": 5.69856314965629,
      "learning_rate": 2e-05,
      "loss": 2.5781,
      "step": 673
    },
    {
      "epoch": 0.7522321428571429,
      "grad_norm": 7.631287716869639,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 674
    },
    {
      "epoch": 0.7533482142857143,
      "grad_norm": 6.987960668144088,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 675
    },
    {
      "epoch": 0.7544642857142857,
      "grad_norm": 6.487843927051443,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 676
    },
    {
      "epoch": 0.7555803571428571,
      "grad_norm": 5.618709956810665,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 677
    },
    {
      "epoch": 0.7566964285714286,
      "grad_norm": 7.430354842673078,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 678
    },
    {
      "epoch": 0.7578125,
      "grad_norm": 6.805536374070492,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 679
    },
    {
      "epoch": 0.7589285714285714,
      "grad_norm": 7.402628875469795,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 680
    },
    {
      "epoch": 0.7600446428571429,
      "grad_norm": 6.777933446889452,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 681
    },
    {
      "epoch": 0.7611607142857143,
      "grad_norm": 8.987087718675813,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 682
    },
    {
      "epoch": 0.7622767857142857,
      "grad_norm": 6.735449008398507,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 683
    },
    {
      "epoch": 0.7633928571428571,
      "grad_norm": 6.117891642385611,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 684
    },
    {
      "epoch": 0.7645089285714286,
      "grad_norm": 7.577422617049957,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 685
    },
    {
      "epoch": 0.765625,
      "grad_norm": 7.131983366478902,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 686
    },
    {
      "epoch": 0.7667410714285714,
      "grad_norm": 7.173704669490769,
      "learning_rate": 2e-05,
      "loss": 1.7188,
      "step": 687
    },
    {
      "epoch": 0.7678571428571429,
      "grad_norm": 6.833493701191782,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 688
    },
    {
      "epoch": 0.7689732142857143,
      "grad_norm": 6.862652266855852,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 689
    },
    {
      "epoch": 0.7700892857142857,
      "grad_norm": 7.009071164719957,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 690
    },
    {
      "epoch": 0.7712053571428571,
      "grad_norm": 7.4444652975568255,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 691
    },
    {
      "epoch": 0.7723214285714286,
      "grad_norm": 7.071156172563195,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 692
    },
    {
      "epoch": 0.7734375,
      "grad_norm": 9.076918207109802,
      "learning_rate": 2e-05,
      "loss": 1.5312,
      "step": 693
    },
    {
      "epoch": 0.7745535714285714,
      "grad_norm": 9.049819640386074,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 694
    },
    {
      "epoch": 0.7756696428571429,
      "grad_norm": 7.700182006709742,
      "learning_rate": 2e-05,
      "loss": 1.9141,
      "step": 695
    },
    {
      "epoch": 0.7767857142857143,
      "grad_norm": 8.244202926480916,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 696
    },
    {
      "epoch": 0.7779017857142857,
      "grad_norm": 8.011975509597269,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 697
    },
    {
      "epoch": 0.7790178571428571,
      "grad_norm": 8.235422546185953,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 698
    },
    {
      "epoch": 0.7801339285714286,
      "grad_norm": 8.237198966702296,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 699
    },
    {
      "epoch": 0.78125,
      "grad_norm": 6.958088283859897,
      "learning_rate": 2e-05,
      "loss": 2.875,
      "step": 700
    },
    {
      "epoch": 0.7823660714285714,
      "grad_norm": 8.080095292496676,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 701
    },
    {
      "epoch": 0.7834821428571429,
      "grad_norm": 8.613320075329357,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 702
    },
    {
      "epoch": 0.7845982142857143,
      "grad_norm": 7.267569669648207,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 703
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 7.163436934525759,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 704
    },
    {
      "epoch": 0.7868303571428571,
      "grad_norm": 7.5081256418136455,
      "learning_rate": 2e-05,
      "loss": 1.625,
      "step": 705
    },
    {
      "epoch": 0.7879464285714286,
      "grad_norm": 6.398911623566062,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 706
    },
    {
      "epoch": 0.7890625,
      "grad_norm": 7.357886595221823,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 707
    },
    {
      "epoch": 0.7901785714285714,
      "grad_norm": 8.742769070315433,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 708
    },
    {
      "epoch": 0.7912946428571429,
      "grad_norm": 7.223230135475636,
      "learning_rate": 2e-05,
      "loss": 1.7578,
      "step": 709
    },
    {
      "epoch": 0.7924107142857143,
      "grad_norm": 7.385531215376964,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 710
    },
    {
      "epoch": 0.7935267857142857,
      "grad_norm": 8.494435773668522,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 711
    },
    {
      "epoch": 0.7946428571428571,
      "grad_norm": 7.555709835403667,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 712
    },
    {
      "epoch": 0.7957589285714286,
      "grad_norm": 7.115783696917038,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 713
    },
    {
      "epoch": 0.796875,
      "grad_norm": 8.179043577779913,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 714
    },
    {
      "epoch": 0.7979910714285714,
      "grad_norm": 7.9204184658056755,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 715
    },
    {
      "epoch": 0.7991071428571429,
      "grad_norm": 6.376362290456513,
      "learning_rate": 2e-05,
      "loss": 2.2656,
      "step": 716
    },
    {
      "epoch": 0.8002232142857143,
      "grad_norm": 8.444289579939406,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 717
    },
    {
      "epoch": 0.8013392857142857,
      "grad_norm": 6.625989986426366,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 718
    },
    {
      "epoch": 0.8024553571428571,
      "grad_norm": 8.236021997988285,
      "learning_rate": 2e-05,
      "loss": 2.1562,
      "step": 719
    },
    {
      "epoch": 0.8035714285714286,
      "grad_norm": 8.302409271271902,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 720
    },
    {
      "epoch": 0.8046875,
      "grad_norm": 9.737315346326259,
      "learning_rate": 2e-05,
      "loss": 1.7266,
      "step": 721
    },
    {
      "epoch": 0.8058035714285714,
      "grad_norm": 8.886967551231395,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 722
    },
    {
      "epoch": 0.8069196428571429,
      "grad_norm": 7.21556747069122,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 723
    },
    {
      "epoch": 0.8080357142857143,
      "grad_norm": 7.942171871566063,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 724
    },
    {
      "epoch": 0.8091517857142857,
      "grad_norm": 7.627586842265554,
      "learning_rate": 2e-05,
      "loss": 1.9766,
      "step": 725
    },
    {
      "epoch": 0.8102678571428571,
      "grad_norm": 7.349261648822988,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 726
    },
    {
      "epoch": 0.8113839285714286,
      "grad_norm": 8.407068878246223,
      "learning_rate": 2e-05,
      "loss": 2.5938,
      "step": 727
    },
    {
      "epoch": 0.8125,
      "grad_norm": 7.318734018745202,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 728
    },
    {
      "epoch": 0.8136160714285714,
      "grad_norm": 6.396774982844105,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 729
    },
    {
      "epoch": 0.8147321428571429,
      "grad_norm": 7.660688635890924,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 730
    },
    {
      "epoch": 0.8158482142857143,
      "grad_norm": 7.187627556889666,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 731
    },
    {
      "epoch": 0.8169642857142857,
      "grad_norm": 5.9968303588111675,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 732
    },
    {
      "epoch": 0.8180803571428571,
      "grad_norm": 8.132129445268596,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 733
    },
    {
      "epoch": 0.8191964285714286,
      "grad_norm": 7.133863199586631,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 734
    },
    {
      "epoch": 0.8203125,
      "grad_norm": 7.109822789443284,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 735
    },
    {
      "epoch": 0.8214285714285714,
      "grad_norm": 8.63670134240111,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 736
    },
    {
      "epoch": 0.8225446428571429,
      "grad_norm": 7.3113072525929725,
      "learning_rate": 2e-05,
      "loss": 1.4766,
      "step": 737
    },
    {
      "epoch": 0.8236607142857143,
      "grad_norm": 9.300369055243136,
      "learning_rate": 2e-05,
      "loss": 1.9453,
      "step": 738
    },
    {
      "epoch": 0.8247767857142857,
      "grad_norm": 7.085335281545922,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 739
    },
    {
      "epoch": 0.8258928571428571,
      "grad_norm": 10.069946620389745,
      "learning_rate": 2e-05,
      "loss": 1.8672,
      "step": 740
    },
    {
      "epoch": 0.8270089285714286,
      "grad_norm": 6.852805442574333,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 741
    },
    {
      "epoch": 0.828125,
      "grad_norm": 7.302169996038232,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 742
    },
    {
      "epoch": 0.8292410714285714,
      "grad_norm": 9.255867150465374,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 743
    },
    {
      "epoch": 0.8303571428571429,
      "grad_norm": 7.4413950850050306,
      "learning_rate": 2e-05,
      "loss": 1.8281,
      "step": 744
    },
    {
      "epoch": 0.8314732142857143,
      "grad_norm": 7.7385358273103595,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 745
    },
    {
      "epoch": 0.8325892857142857,
      "grad_norm": 7.340417180177864,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 746
    },
    {
      "epoch": 0.8337053571428571,
      "grad_norm": 5.679604687666759,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 747
    },
    {
      "epoch": 0.8348214285714286,
      "grad_norm": 7.481594455169163,
      "learning_rate": 2e-05,
      "loss": 1.4453,
      "step": 748
    },
    {
      "epoch": 0.8359375,
      "grad_norm": 9.303365562523432,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 749
    },
    {
      "epoch": 0.8370535714285714,
      "grad_norm": 6.869555334216046,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 750
    },
    {
      "epoch": 0.8381696428571429,
      "grad_norm": 7.425644984434222,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 751
    },
    {
      "epoch": 0.8392857142857143,
      "grad_norm": 8.649820336507238,
      "learning_rate": 2e-05,
      "loss": 2.3906,
      "step": 752
    },
    {
      "epoch": 0.8404017857142857,
      "grad_norm": 5.289604832401025,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 753
    },
    {
      "epoch": 0.8415178571428571,
      "grad_norm": 7.277697761309936,
      "learning_rate": 2e-05,
      "loss": 1.8203,
      "step": 754
    },
    {
      "epoch": 0.8426339285714286,
      "grad_norm": 10.090745597411415,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 755
    },
    {
      "epoch": 0.84375,
      "grad_norm": 6.468005829214493,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 756
    },
    {
      "epoch": 0.8448660714285714,
      "grad_norm": 6.035282974460319,
      "learning_rate": 2e-05,
      "loss": 2.5625,
      "step": 757
    },
    {
      "epoch": 0.8459821428571429,
      "grad_norm": 5.581858398047519,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 758
    },
    {
      "epoch": 0.8470982142857143,
      "grad_norm": 7.046060932972032,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 759
    },
    {
      "epoch": 0.8482142857142857,
      "grad_norm": 6.707705784166958,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 760
    },
    {
      "epoch": 0.8493303571428571,
      "grad_norm": 7.1766893492439525,
      "learning_rate": 2e-05,
      "loss": 2.2812,
      "step": 761
    },
    {
      "epoch": 0.8504464285714286,
      "grad_norm": 7.933267883547185,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 762
    },
    {
      "epoch": 0.8515625,
      "grad_norm": 7.499401054942811,
      "learning_rate": 2e-05,
      "loss": 3.0,
      "step": 763
    },
    {
      "epoch": 0.8526785714285714,
      "grad_norm": 8.124846669315502,
      "learning_rate": 2e-05,
      "loss": 2.625,
      "step": 764
    },
    {
      "epoch": 0.8537946428571429,
      "grad_norm": 7.454956785124179,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 765
    },
    {
      "epoch": 0.8549107142857143,
      "grad_norm": 7.7574843361412835,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 766
    },
    {
      "epoch": 0.8560267857142857,
      "grad_norm": 6.671722876057147,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 767
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 9.982878960914976,
      "learning_rate": 2e-05,
      "loss": 1.6094,
      "step": 768
    },
    {
      "epoch": 0.8582589285714286,
      "grad_norm": 7.906663708530348,
      "learning_rate": 2e-05,
      "loss": 1.8594,
      "step": 769
    },
    {
      "epoch": 0.859375,
      "grad_norm": 5.388841252551708,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 770
    },
    {
      "epoch": 0.8604910714285714,
      "grad_norm": 5.778472453045861,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 771
    },
    {
      "epoch": 0.8616071428571429,
      "grad_norm": 7.582251002867786,
      "learning_rate": 2e-05,
      "loss": 1.6016,
      "step": 772
    },
    {
      "epoch": 0.8627232142857143,
      "grad_norm": 8.006312658964319,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 773
    },
    {
      "epoch": 0.8638392857142857,
      "grad_norm": 7.895503372255256,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 774
    },
    {
      "epoch": 0.8649553571428571,
      "grad_norm": 7.679708314806106,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 775
    },
    {
      "epoch": 0.8660714285714286,
      "grad_norm": 7.623966888936106,
      "learning_rate": 2e-05,
      "loss": 1.8125,
      "step": 776
    },
    {
      "epoch": 0.8671875,
      "grad_norm": 8.569040070861268,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 777
    },
    {
      "epoch": 0.8683035714285714,
      "grad_norm": 7.790891892802453,
      "learning_rate": 2e-05,
      "loss": 1.4531,
      "step": 778
    },
    {
      "epoch": 0.8694196428571429,
      "grad_norm": 7.6613463134530235,
      "learning_rate": 2e-05,
      "loss": 1.9922,
      "step": 779
    },
    {
      "epoch": 0.8705357142857143,
      "grad_norm": 7.0592210182544415,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 780
    },
    {
      "epoch": 0.8716517857142857,
      "grad_norm": 7.529115065034707,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 781
    },
    {
      "epoch": 0.8727678571428571,
      "grad_norm": 7.424860194958381,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 782
    },
    {
      "epoch": 0.8738839285714286,
      "grad_norm": 7.626131493352132,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 783
    },
    {
      "epoch": 0.875,
      "grad_norm": 7.876400909117194,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 784
    },
    {
      "epoch": 0.8761160714285714,
      "grad_norm": 7.870317858347874,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 785
    },
    {
      "epoch": 0.8772321428571429,
      "grad_norm": 6.9428170879681455,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 786
    },
    {
      "epoch": 0.8783482142857143,
      "grad_norm": 6.926305000659284,
      "learning_rate": 2e-05,
      "loss": 2.3594,
      "step": 787
    },
    {
      "epoch": 0.8794642857142857,
      "grad_norm": 7.90865231714586,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 788
    },
    {
      "epoch": 0.8805803571428571,
      "grad_norm": 8.089814304439924,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 789
    },
    {
      "epoch": 0.8816964285714286,
      "grad_norm": 6.521415643620119,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 790
    },
    {
      "epoch": 0.8828125,
      "grad_norm": 9.17658570222275,
      "learning_rate": 2e-05,
      "loss": 1.5078,
      "step": 791
    },
    {
      "epoch": 0.8839285714285714,
      "grad_norm": 7.129909145799193,
      "learning_rate": 2e-05,
      "loss": 1.9297,
      "step": 792
    },
    {
      "epoch": 0.8850446428571429,
      "grad_norm": 6.6225561686866365,
      "learning_rate": 2e-05,
      "loss": 2.2031,
      "step": 793
    },
    {
      "epoch": 0.8861607142857143,
      "grad_norm": 10.621948008079693,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 794
    },
    {
      "epoch": 0.8872767857142857,
      "grad_norm": 7.709827609705132,
      "learning_rate": 2e-05,
      "loss": 2.5,
      "step": 795
    },
    {
      "epoch": 0.8883928571428571,
      "grad_norm": 4.4416294940118295,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 796
    },
    {
      "epoch": 0.8895089285714286,
      "grad_norm": 8.362173413105783,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 797
    },
    {
      "epoch": 0.890625,
      "grad_norm": 5.68851529600558,
      "learning_rate": 2e-05,
      "loss": 1.7656,
      "step": 798
    },
    {
      "epoch": 0.8917410714285714,
      "grad_norm": 8.356346198763275,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 799
    },
    {
      "epoch": 0.8928571428571429,
      "grad_norm": 7.762687151391418,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 800
    },
    {
      "epoch": 0.8939732142857143,
      "grad_norm": 6.5805818197757375,
      "learning_rate": 2e-05,
      "loss": 2.7812,
      "step": 801
    },
    {
      "epoch": 0.8950892857142857,
      "grad_norm": 8.25122791610741,
      "learning_rate": 2e-05,
      "loss": 1.5469,
      "step": 802
    },
    {
      "epoch": 0.8962053571428571,
      "grad_norm": 7.466197672511711,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 803
    },
    {
      "epoch": 0.8973214285714286,
      "grad_norm": 6.966245302351483,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 804
    },
    {
      "epoch": 0.8984375,
      "grad_norm": 6.7751623023385426,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 805
    },
    {
      "epoch": 0.8995535714285714,
      "grad_norm": 11.1491256155419,
      "learning_rate": 2e-05,
      "loss": 2.0312,
      "step": 806
    },
    {
      "epoch": 0.9006696428571429,
      "grad_norm": 9.214656191446174,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 807
    },
    {
      "epoch": 0.9017857142857143,
      "grad_norm": 7.3660538769824395,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 808
    },
    {
      "epoch": 0.9029017857142857,
      "grad_norm": 7.429777127164266,
      "learning_rate": 2e-05,
      "loss": 1.9531,
      "step": 809
    },
    {
      "epoch": 0.9040178571428571,
      "grad_norm": 6.883428898836841,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 810
    },
    {
      "epoch": 0.9051339285714286,
      "grad_norm": 6.320497591879356,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 811
    },
    {
      "epoch": 0.90625,
      "grad_norm": 8.43694707550417,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 812
    },
    {
      "epoch": 0.9073660714285714,
      "grad_norm": 7.132273889261657,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 813
    },
    {
      "epoch": 0.9084821428571429,
      "grad_norm": 7.385123139681288,
      "learning_rate": 2e-05,
      "loss": 2.3281,
      "step": 814
    },
    {
      "epoch": 0.9095982142857143,
      "grad_norm": 8.061602711766193,
      "learning_rate": 2e-05,
      "loss": 1.8438,
      "step": 815
    },
    {
      "epoch": 0.9107142857142857,
      "grad_norm": 6.572520918758388,
      "learning_rate": 2e-05,
      "loss": 1.9688,
      "step": 816
    },
    {
      "epoch": 0.9118303571428571,
      "grad_norm": 6.915982889844661,
      "learning_rate": 2e-05,
      "loss": 2.6719,
      "step": 817
    },
    {
      "epoch": 0.9129464285714286,
      "grad_norm": 7.724134960053868,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 818
    },
    {
      "epoch": 0.9140625,
      "grad_norm": 7.907718137289556,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 819
    },
    {
      "epoch": 0.9151785714285714,
      "grad_norm": 7.770451769764148,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 820
    },
    {
      "epoch": 0.9162946428571429,
      "grad_norm": 7.269102966777141,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 821
    },
    {
      "epoch": 0.9174107142857143,
      "grad_norm": 9.66235770318758,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 822
    },
    {
      "epoch": 0.9185267857142857,
      "grad_norm": 7.629217227520317,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 823
    },
    {
      "epoch": 0.9196428571428571,
      "grad_norm": 8.452368469098834,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 824
    },
    {
      "epoch": 0.9207589285714286,
      "grad_norm": 7.57519682426313,
      "learning_rate": 2e-05,
      "loss": 2.8125,
      "step": 825
    },
    {
      "epoch": 0.921875,
      "grad_norm": 6.764097362158105,
      "learning_rate": 2e-05,
      "loss": 1.6328,
      "step": 826
    },
    {
      "epoch": 0.9229910714285714,
      "grad_norm": 7.62189770198356,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 827
    },
    {
      "epoch": 0.9241071428571429,
      "grad_norm": 6.9485382094004615,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 828
    },
    {
      "epoch": 0.9252232142857143,
      "grad_norm": 7.708413330777987,
      "learning_rate": 2e-05,
      "loss": 1.7812,
      "step": 829
    },
    {
      "epoch": 0.9263392857142857,
      "grad_norm": 8.255494452092904,
      "learning_rate": 2e-05,
      "loss": 1.6406,
      "step": 830
    },
    {
      "epoch": 0.9274553571428571,
      "grad_norm": 6.691665341841153,
      "learning_rate": 2e-05,
      "loss": 1.8906,
      "step": 831
    },
    {
      "epoch": 0.9285714285714286,
      "grad_norm": 8.439901138952038,
      "learning_rate": 2e-05,
      "loss": 2.2969,
      "step": 832
    },
    {
      "epoch": 0.9296875,
      "grad_norm": 7.505067871747109,
      "learning_rate": 2e-05,
      "loss": 2.25,
      "step": 833
    },
    {
      "epoch": 0.9308035714285714,
      "grad_norm": 7.957964788608105,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 834
    },
    {
      "epoch": 0.9319196428571429,
      "grad_norm": 7.097183084473005,
      "learning_rate": 2e-05,
      "loss": 1.9062,
      "step": 835
    },
    {
      "epoch": 0.9330357142857143,
      "grad_norm": 8.09717670978944,
      "learning_rate": 2e-05,
      "loss": 1.9219,
      "step": 836
    },
    {
      "epoch": 0.9341517857142857,
      "grad_norm": 8.369563410119511,
      "learning_rate": 2e-05,
      "loss": 2.7188,
      "step": 837
    },
    {
      "epoch": 0.9352678571428571,
      "grad_norm": 6.79453391453038,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 838
    },
    {
      "epoch": 0.9363839285714286,
      "grad_norm": 7.179151748633432,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 839
    },
    {
      "epoch": 0.9375,
      "grad_norm": 11.49101770775062,
      "learning_rate": 2e-05,
      "loss": 1.9844,
      "step": 840
    },
    {
      "epoch": 0.9386160714285714,
      "grad_norm": 7.6322646452696254,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 841
    },
    {
      "epoch": 0.9397321428571429,
      "grad_norm": 8.569679600866866,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 842
    },
    {
      "epoch": 0.9408482142857143,
      "grad_norm": 7.924316767148923,
      "learning_rate": 2e-05,
      "loss": 2.3438,
      "step": 843
    },
    {
      "epoch": 0.9419642857142857,
      "grad_norm": 8.190955496226874,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 844
    },
    {
      "epoch": 0.9430803571428571,
      "grad_norm": 9.056915547089309,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 845
    },
    {
      "epoch": 0.9441964285714286,
      "grad_norm": 6.240819633154437,
      "learning_rate": 2e-05,
      "loss": 2.4844,
      "step": 846
    },
    {
      "epoch": 0.9453125,
      "grad_norm": 9.2630281755415,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 847
    },
    {
      "epoch": 0.9464285714285714,
      "grad_norm": 7.93785259584136,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 848
    },
    {
      "epoch": 0.9475446428571429,
      "grad_norm": 7.703943005271166,
      "learning_rate": 2e-05,
      "loss": 2.6875,
      "step": 849
    },
    {
      "epoch": 0.9486607142857143,
      "grad_norm": 7.709231143205931,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 850
    },
    {
      "epoch": 0.9497767857142857,
      "grad_norm": 5.991929236206334,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 851
    },
    {
      "epoch": 0.9508928571428571,
      "grad_norm": 7.300431800124602,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 852
    },
    {
      "epoch": 0.9520089285714286,
      "grad_norm": 7.359055059295672,
      "learning_rate": 2e-05,
      "loss": 2.6406,
      "step": 853
    },
    {
      "epoch": 0.953125,
      "grad_norm": 9.401226808903882,
      "learning_rate": 2e-05,
      "loss": 2.0938,
      "step": 854
    },
    {
      "epoch": 0.9542410714285714,
      "grad_norm": 8.234120994876932,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 855
    },
    {
      "epoch": 0.9553571428571429,
      "grad_norm": 7.652783252392185,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 856
    },
    {
      "epoch": 0.9564732142857143,
      "grad_norm": 8.387724213253467,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 857
    },
    {
      "epoch": 0.9575892857142857,
      "grad_norm": 8.80450130650201,
      "learning_rate": 2e-05,
      "loss": 1.8984,
      "step": 858
    },
    {
      "epoch": 0.9587053571428571,
      "grad_norm": 7.2414223383416765,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 859
    },
    {
      "epoch": 0.9598214285714286,
      "grad_norm": 6.277292458914491,
      "learning_rate": 2e-05,
      "loss": 2.5312,
      "step": 860
    },
    {
      "epoch": 0.9609375,
      "grad_norm": 8.425193085914424,
      "learning_rate": 2e-05,
      "loss": 1.8516,
      "step": 861
    },
    {
      "epoch": 0.9620535714285714,
      "grad_norm": 8.828768431233788,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 862
    },
    {
      "epoch": 0.9631696428571429,
      "grad_norm": 5.664319232822921,
      "learning_rate": 2e-05,
      "loss": 2.8281,
      "step": 863
    },
    {
      "epoch": 0.9642857142857143,
      "grad_norm": 7.196473451673784,
      "learning_rate": 2e-05,
      "loss": 1.7344,
      "step": 864
    },
    {
      "epoch": 0.9654017857142857,
      "grad_norm": 9.109002661960586,
      "learning_rate": 2e-05,
      "loss": 2.5469,
      "step": 865
    },
    {
      "epoch": 0.9665178571428571,
      "grad_norm": 7.3153509671100325,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 866
    },
    {
      "epoch": 0.9676339285714286,
      "grad_norm": 7.746957519809649,
      "learning_rate": 2e-05,
      "loss": 2.0156,
      "step": 867
    },
    {
      "epoch": 0.96875,
      "grad_norm": 8.71259281907862,
      "learning_rate": 2e-05,
      "loss": 2.3125,
      "step": 868
    },
    {
      "epoch": 0.9698660714285714,
      "grad_norm": 8.406368992644454,
      "learning_rate": 2e-05,
      "loss": 2.0469,
      "step": 869
    },
    {
      "epoch": 0.9709821428571429,
      "grad_norm": 7.8730859516977505,
      "learning_rate": 2e-05,
      "loss": 2.0781,
      "step": 870
    },
    {
      "epoch": 0.9720982142857143,
      "grad_norm": 7.363588014310592,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 871
    },
    {
      "epoch": 0.9732142857142857,
      "grad_norm": 7.384399565939538,
      "learning_rate": 2e-05,
      "loss": 1.7422,
      "step": 872
    },
    {
      "epoch": 0.9743303571428571,
      "grad_norm": 6.48682564127364,
      "learning_rate": 2e-05,
      "loss": 2.4062,
      "step": 873
    },
    {
      "epoch": 0.9754464285714286,
      "grad_norm": 8.175047863768238,
      "learning_rate": 2e-05,
      "loss": 1.9375,
      "step": 874
    },
    {
      "epoch": 0.9765625,
      "grad_norm": 9.490286332023473,
      "learning_rate": 2e-05,
      "loss": 2.2188,
      "step": 875
    },
    {
      "epoch": 0.9776785714285714,
      "grad_norm": 8.296203084604528,
      "learning_rate": 2e-05,
      "loss": 1.5703,
      "step": 876
    },
    {
      "epoch": 0.9787946428571429,
      "grad_norm": 7.562503388027406,
      "learning_rate": 2e-05,
      "loss": 2.4375,
      "step": 877
    },
    {
      "epoch": 0.9799107142857143,
      "grad_norm": 7.363696290211345,
      "learning_rate": 2e-05,
      "loss": 2.4531,
      "step": 878
    },
    {
      "epoch": 0.9810267857142857,
      "grad_norm": 8.548006655887228,
      "learning_rate": 2e-05,
      "loss": 2.1719,
      "step": 879
    },
    {
      "epoch": 0.9821428571428571,
      "grad_norm": 7.3071845684102605,
      "learning_rate": 2e-05,
      "loss": 1.875,
      "step": 880
    },
    {
      "epoch": 0.9832589285714286,
      "grad_norm": 7.967861274278176,
      "learning_rate": 2e-05,
      "loss": 2.0,
      "step": 881
    },
    {
      "epoch": 0.984375,
      "grad_norm": 8.45353461545602,
      "learning_rate": 2e-05,
      "loss": 2.1875,
      "step": 882
    },
    {
      "epoch": 0.9854910714285714,
      "grad_norm": 7.576104875232474,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 883
    },
    {
      "epoch": 0.9866071428571429,
      "grad_norm": 8.840917981292552,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 884
    },
    {
      "epoch": 0.9877232142857143,
      "grad_norm": 7.028927444049435,
      "learning_rate": 2e-05,
      "loss": 1.5625,
      "step": 885
    },
    {
      "epoch": 0.9888392857142857,
      "grad_norm": 8.935505644650485,
      "learning_rate": 2e-05,
      "loss": 1.7969,
      "step": 886
    },
    {
      "epoch": 0.9899553571428571,
      "grad_norm": 6.885889557990625,
      "learning_rate": 2e-05,
      "loss": 1.9609,
      "step": 887
    },
    {
      "epoch": 0.9910714285714286,
      "grad_norm": 7.473054535197915,
      "learning_rate": 2e-05,
      "loss": 2.2344,
      "step": 888
    },
    {
      "epoch": 0.9921875,
      "grad_norm": 7.659368125704597,
      "learning_rate": 2e-05,
      "loss": 1.7734,
      "step": 889
    },
    {
      "epoch": 0.9933035714285714,
      "grad_norm": 7.765536772815751,
      "learning_rate": 2e-05,
      "loss": 2.0625,
      "step": 890
    },
    {
      "epoch": 0.9944196428571429,
      "grad_norm": 6.4654037182918564,
      "learning_rate": 2e-05,
      "loss": 2.1094,
      "step": 891
    },
    {
      "epoch": 0.9955357142857143,
      "grad_norm": 8.559616449987061,
      "learning_rate": 2e-05,
      "loss": 1.7891,
      "step": 892
    },
    {
      "epoch": 0.9966517857142857,
      "grad_norm": 6.7653584756777745,
      "learning_rate": 2e-05,
      "loss": 2.125,
      "step": 893
    },
    {
      "epoch": 0.9977678571428571,
      "grad_norm": 9.255095539737054,
      "learning_rate": 2e-05,
      "loss": 1.7109,
      "step": 894
    },
    {
      "epoch": 0.9988839285714286,
      "grad_norm": 9.094258407653669,
      "learning_rate": 2e-05,
      "loss": 1.75,
      "step": 895
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.574840564248845,
      "learning_rate": 2e-05,
      "loss": 2.1406,
      "step": 896
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 3584,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "total_flos": 17407502450688.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
